<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://abecid.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://abecid.github.io/" rel="alternate" type="text/html"/><updated>2026-02-24T05:49:20+00:00</updated><id>https://abecid.github.io/feed.xml</id><title type="html">Adam Abecid</title><entry><title type="html">Recent Methods Utilizing Diffusion in Graphics</title><link href="https://abecid.github.io/blog/2026/graphics-diffusion/" rel="alternate" type="text/html" title="Recent Methods Utilizing Diffusion in Graphics"/><published>2026-02-22T14:24:00+00:00</published><updated>2026-02-22T14:24:00+00:00</updated><id>https://abecid.github.io/blog/2026/graphics-diffusion</id><content type="html" xml:base="https://abecid.github.io/blog/2026/graphics-diffusion/"><![CDATA[]]></content><author><name></name></author><category term="research-survey"/><category term="diffusion"/><category term="graphics"/><category term="survey"/><summary type="html"><![CDATA[Recent developments of diffusion in graphics methods]]></summary></entry><entry><title type="html">Spatial Memory in Video Diffusion Survey</title><link href="https://abecid.github.io/blog/2026/spatial-video/" rel="alternate" type="text/html" title="Spatial Memory in Video Diffusion Survey"/><published>2026-02-21T14:24:00+00:00</published><updated>2026-02-21T14:24:00+00:00</updated><id>https://abecid.github.io/blog/2026/spatial-video</id><content type="html" xml:base="https://abecid.github.io/blog/2026/spatial-video/"><![CDATA[]]></content><author><name></name></author><category term="research-survey"/><category term="3d"/><category term="diffusion"/><category term="survey"/><category term="video_generation"/><summary type="html"><![CDATA[Recent developments of using spatial memory in video diffusion models]]></summary></entry><entry><title type="html">Recent Flow-Map Distillation Methods</title><link href="https://abecid.github.io/blog/2026/distillation/" rel="alternate" type="text/html" title="Recent Flow-Map Distillation Methods"/><published>2026-02-20T01:24:00+00:00</published><updated>2026-02-20T01:24:00+00:00</updated><id>https://abecid.github.io/blog/2026/distillation</id><content type="html" xml:base="https://abecid.github.io/blog/2026/distillation/"><![CDATA[<p>Meta FlowMap</p> <h1 id="flow-matching-to-flow-maps-to-distillation-a-deep-dive">Flow Matching to Flow Maps to Distillation: A Deep Dive</h1> <p>(MeanFlow, Flow Map Self-Distillation, Stochastic/Meta Flow Maps, TMD, TVM)</p> <h1 id="table-of-content">Table of Content</h1> <ol> <li><a href="#1-foundations-flow-matching-vs-flow-maps">Foundations: Flow Matching and Flow Maps</a></li> <li><a href="#2-meanflow">MeanFlow</a></li> <li><a href="#3-freeflowmap">FreeFlowMap</a></li> <li><a href="#4-meta-flow-map">Meta FlowMap</a></li> <li><a href="#5-transition-matching-distillation">Transition Matching Distillation</a></li> <li><a href="#6-terminal-velocity-matching">Terminal Velocity Matching</a></li> </ol> <h1 id="overview">Overview</h1> <p>Recent generative modeling utilize and develop upon flow maps and jvp based distillation techniques to reduce the number of function evaluations during inference.</p> <h1 id="summary">Summary</h1> <p>The modern progression is:</p> <ol> <li><strong>Flow Matching (FM)</strong> learns the <strong>instantaneous velocity field</strong>.</li> <li><strong>Flow-map methods</strong> learn <strong>time-to-time transport maps</strong> (or average velocities), which are much more compatible with <strong>1-step / few-step generation</strong>.</li> <li><strong>MeanFlow</strong> gives a clean, derivation-first way to train a two-time flow-map model via a <strong>JVP-based identity</strong> (no extra consistency axiom).</li> <li><strong>Flow Map Distillation without Data / self-distillation</strong> (the ‚ÄúFreeFlowMap / How to build a consistency model‚Äù line) points out <strong>teacher-data mismatch</strong> and replaces data-dependent distillation with <strong>prior-only self-generated supervision</strong>, then adds a <strong>correction objective</strong> to fix distribution drift.</li> <li><strong>Stochastic Flow Maps / Meta Flow Maps</strong> generalize flow maps to <strong>stochastic transitions</strong> and derive <strong>diagonal + consistency</strong> objectives from conditional posterior structure; this is the right lens when deterministic flow maps are too restrictive.</li> <li><strong>TMD</strong> ports MeanFlow-style ideas into <strong>video distillation</strong>, using a <strong>transition-matching MeanFlow pretraining stage</strong> + a second-stage distributional distillation objective.</li> <li><strong>TVM</strong> shifts the target from ‚Äúmatch initial/local velocity‚Äù to <strong>match terminal velocity</strong>, and gives a more principled guarantee (explicit <strong>2-Wasserstein upper bound</strong>), while also exposing a key practical systems issue: <strong>JVP through transformer attention</strong>.</li> </ol> <hr/> <h1 id="1-foundations-flow-matching-and-flow-maps">1. Foundations: Flow Matching and Flow Maps</h1> <h3 id="11-flow-matching">1.1 Flow Matching</h3> <p><img src="/assets/img/blogs/1_distillation/flowmatching.png" alt="Flow Matching"/></p> <p><em>Figure 1. Flow Matching. from Sabour, Fidler, and Kreis (2025),</em> Align Your Flow: Scaling Continuous-Time Flow Map Distillation <em>(arXiv:2506.14603).</em></p> <p>Generative models learn to transport probability ditribution from prior, $p_{0}$, to a data distribution, $p_{1}$.</p> <p>Classic flow matching learns a vector field \(u(x,t)\) that acts as an instantaneous velocity at timestep $t$, which collectively defines an ODE trajectory. Sampling involves ODE integration from noise to data.</p> <p>There are several bottlenecks to this approach: if the learned trajectory is curved, a decent solver (Euler, Heun) is required and many number of function evaluations (NFEs) to integrate over the ODE trajectory.</p> <h3 id="12-flow-map">1.2 Flow Map</h3> <p><img src="/assets/img/blogs/1_distillation/flowmap.png" alt="Flow Map"/></p> <p><em>Figure 2. Flow Map. from Sabour, Fidler, and Kreis (2025),</em> Align Your Flow: Scaling Continuous-Time Flow Map Distillation <em>(arXiv:2506.14603).</em></p> <p>Flow map formulation directly targets: \(\phi_u(x_t, t, s)\) which maps a state at time (t) to time (s), instead of learning only the local instantaneous tangent.</p> <p>Recent methods have emerged developing upon this flow map formulation for fewer step, student-teacher, data-free distillation families.</p> <hr/> <h1 id="2-meanflow">2. MeanFlow</h1> <h4 id="21-average-velocity-instead-of-instantaneous-velocity">2.1 Average velocity instead of instantaneous velocity</h4> <p><img src="/assets/img/blogs/1_distillation/meanflow.png" alt="Mean Flow"/></p> <p><em>Figure 3. Mean Flow, with different target timestep $t$. from Geng et al. (2025),</em> Mean Flows for One-step Generative Modeling <em>(arXiv:2505.13447).</em></p> <p>MeanFlow defines an <strong>average velocity</strong></p> \[u(z_t, r, t)\] <p>over the interval $[r,t]$, so the displacement is:</p> \[(t-r)u(z_t,r,t).\] <h4 id="22-the-meanflow-identity-the-central-derivation">2.2 The MeanFlow Identity (the central derivation)</h4> <p>Start from the definition:</p> \[(t-r)u(z_t,r,t) = \int_r^t v(z_\tau,\tau)\, d\tau\] <p>Differentiate both sides with respect to (t) (holding (r) fixed). By product rule + FTC:</p> \[u(z_t,r,t) + (t-r)\frac{d}{dt}u(z_t,r,t) = v(z_t,t)\] <p>Rearranging leads to the <strong>MeanFlow Identity</strong>:</p> \[u(z_t,r,t) = v(z_t,t) - (t-r)\frac{d}{dt}u(z_t,r,t)\] <h4 id="23-main-points">2.3 Main Points</h4> <ul> <li>The authors rewrite an intractable target (the average velocity integral) into a trainable target: <ul> <li>By first taking the derivative of both sides.</li> <li>Instantaneous velocity $v$ is available from FM-style interpolation.</li> <li>Total derivative term computed via <strong>JVP</strong>.</li> </ul> </li> <li>No explicit consistency regularizer is imposed.</li> <li>The consistency-like structure falls out from the definition of average velocity.</li> </ul> <h4 id="24-meanflow-loss">2.4 MeanFlow loss</h4> <p>Parameterize $u_\theta(z_t,r,t)$, and regress to the identity-induced target:</p> \[\mathcal{L}_{\text{MeanFlow}}(\theta) = \mathbb{E}\left[ \left\| u_\theta(z_t,r,t) - \operatorname{sg}(u_{\text{tgt}}) \right\|_2^2 \right]\] <p>with</p> \[u_{\text{tgt}} = v_t - (t-r)\left(v_t \,\partial_z u_\theta + \partial_t u_\theta\right)\] <p>where the total derivative is implemented through a JVP along tangent $(v_t, 0, 1)$.</p> <h4 id="25-sampling">2.5 Sampling</h4> <p>Once you learn the average velocity, sampling is:</p> \[z_r = z_t - (t-r)u(z_t,r,t)\] <p>For 1-step:</p> \[z_0 = z_1 - u(z_1,0,1),\quad z_1 \sim p_{\text{prior}}\] <hr/> <h1 id="3-freeflowmap">3. FreeFlowMap</h1> <p>Flow Map Distillation Without Data</p> <h2 id="31-teacher-data-mismatch-the-hidden-bug-in-many-distillation-pipelines">3.1 Teacher-data mismatch (the hidden bug in many distillation pipelines)</h2> <p>Traditional flow-map distillation often samples intermediate states (x_t) from an <strong>external dataset distribution</strong> and supervises the student using teacher velocities at those states.</p> <p>But the student is supposed to reproduce the teacher‚Äôs <strong>sampling process</strong>, i.e. the trajectory distribution induced by the teacher from the prior.</p> <p>Supervision states coming from a mismatched distribution results in a <strong>teacher-data mismatch</strong>:</p> <ul> <li>the student is trained on states that are not on the teacher‚Äôs true rollout distribution,</li> <li>more augmentation can worsen it,</li> <li>student quality degrades.</li> </ul> <p>This is a deep point because it says the standard ‚Äúdistill on data‚Äù recipe is fundamentally misaligned with the actual objective (imitate the teacher sampler).</p> <h2 id="32-prior-only--self-generated-flow-map-objective">3.2 Prior-only / self-generated flow-map objective</h2> <p>Supervise entirely from the prior and the student‚Äôs own generated states.</p> <p>They derive a sufficient optimality condition leading to a loss of the form \(\mathcal{L}_{\text{pred}} = \mathbb{E}_{z,\delta} \left[ \|F_\theta(z,\delta)-\operatorname{sg}(u_{\text{target}})\|^2 \right]\)</p> <p>with \(u_{\text{target}} = u(f_\theta(z,\delta),1-\delta) - \delta \,\partial_\delta F_\theta(z,\delta)\)</p> <p>The key interpretation:</p> <ul> <li>(f_\theta(z,\delta)) defines the student‚Äôs current trajectory,</li> <li>(\partial_\delta f_\theta) is the <strong>student‚Äôs generating velocity</strong>,</li> <li>the loss is equivalent to aligning the student generating velocity with the teacher field: \(\partial_\delta f_\theta \approx u(f_\theta(z,\delta),1-\delta)\)</li> </ul> <p>So the student learns to <strong>ride the teacher vector field along its own generated path</strong>, starting from pure noise, no dataset needed.</p> <p>This is the right fix for teacher-data mismatch.</p> <h2 id="33-gradient-view">3.3 Gradient view</h2> <p>They explicitly write the optimization gradient in terms of a velocity mismatch \(\Delta v_{G,u} = v_G - u\) which is great because it makes gradient weighting / normalization tricks easier to reason about.</p> <p>This is one of those ‚Äúsmall‚Äù presentation choices that actually matters in practice.</p> <h2 id="34-correction-objective-fixing-distribution-drift-not-just-local-velocity">3.4 Correction objective (fixing distribution drift, not just local velocity)</h2> <p>Here‚Äôs the catch: aligning generating velocity locally is necessary, but in finite-capacity/discrete training the generated distribution can still drift.</p> <p>So they add a <strong>correction objective</strong> motivated from minimizing a KL term over intermediate marginals and then translating score mismatch into <strong>velocity mismatch</strong> (via the score‚Äìvelocity equivalence for linear interpolants).</p> <p>This yields a gradient proportional to \(\nabla_\theta \; \mathbb{E}_{z,n,r} \left[ F_\theta(z,1)^\top \operatorname{sg} \left( v_N(I_r(f_\theta(z,1),n),r) - u(I_r(f_\theta(z,1),n),r) \right) \right]\)</p> <p>where:</p> <ul> <li>(u) is the teacher marginal velocity,</li> <li>(v_N) is the student-induced <strong>noising</strong> marginal velocity,</li> <li>(I_r(\cdot,\cdot)) is the interpolation to intermediate time (r).</li> </ul> <p>Intuition:</p> <ul> <li>the prediction loss aligns the <strong>student‚Äôs forward/generating flow</strong></li> <li>the correction loss aligns the <strong>student-induced noising marginals</strong> with the teacher‚Äôs marginals</li> </ul> <p>That is a nice bidirectional correction mechanism.</p> <hr/> <h1 id="4-meta-flow-maps">4. Meta Flow Maps</h1> <p>Meta flow maps correspond to a <strong>stochastic flow map</strong>, which is important because deterministic flow maps are too rigid.</p> <h3 id="41-why-stochastic-flow-maps">4.1 Why stochastic flow maps?</h3> <p>Deterministic flow-map learning works if the transport map is the right object. But for many diffusion-like processes, especially when you want richer uncertainty handling, a <strong>stochastic transition kernel</strong> \(\kappa_{t,s}(z_t, z_s)\) is the right object.</p> <p>The paper frames this using:</p> <ul> <li><strong>marginal consistency</strong></li> <li><strong>conditional consistency</strong></li> <li> <table> <tbody> <tr> <td>a family of posterior conditionals (p_{1</td> <td>t})</td> </tr> </tbody> </table> </li> <li>and a diagonal supervision view</li> </ul> <p>The important conceptual upgrade is:</p> <ul> <li>instead of only learning deterministic trajectories,</li> <li>learn a transition operator consistent with the stochastic process structure.</li> </ul> <h3 id="42-the-diagonal-condition-same-role-as-fm-diagonal-supervision">4.2 The diagonal condition (same role as FM diagonal supervision)</h3> <p>They derive that on the diagonal: \(\kappa_{t,t}(z_t, z_1) = p_{1|t}(z_1 \mid z_t)\)</p> <p>This is the stochastic analogue of ‚Äúwhen (r=t), your two-time object must match the one-time target.‚Äù</p> <p>So the diagonal again plays the role of anchor supervision.</p> <h3 id="43-pathwiseconsistency-relation-for-stochastic-flow-maps">4.3 Pathwise/consistency relation for stochastic flow maps</h3> <p>They also derive a consistency/composition condition (their Eq. 23 in the snippet): \(\kappa_{t,s}(z_t,z_s) = \mathbb{E}_{z_1 \sim p_{1|t}(\cdot|z_t)} \big[ \kappa_{u,s}(z_u,z_s) \big]\) (with the appropriate latent dependence through (z_1)/paths)</p> <p>The exact notation is heavier, but the key idea is the same as flow-map composition:</p> <ul> <li><strong>two-time transitions must compose correctly through intermediate times</strong>, but now in distributional form.</li> </ul> <h3 id="44-their-training-objective-mfm-loss">4.4 Their training objective (MFM loss)</h3> <p>They build:</p> <ol> <li>a <strong>diagonal supervision loss</strong> (fit the posterior on diagonal time pairs),</li> <li>a <strong>consistency loss</strong> (enforce off-diagonal composition consistency),</li> <li>and combine them into an MFM objective: \(\mathcal{L}_{\text{MFM}} = \mathcal{L}_{\text{diag}} + \lambda \mathcal{L}_{\text{cons}}\)</li> </ol> <p>This is the stochastic counterpart of the deterministic progression:</p> <ul> <li>diagonal target = ‚ÄúFM-like‚Äù anchor</li> <li>off-diagonal consistency = ‚Äúflow-map-like‚Äù propagation</li> </ul> <h3 id="45-why-this-matters-for-the-broader-field">4.5 Why this matters for the broader field</h3> <p>This paper gives a more general lens:</p> <ul> <li>MeanFlow / deterministic flow maps are one branch</li> <li>stochastic transition learning is the broader object when uncertainty matters</li> <li>the ‚Äúdiagonal + consistency‚Äù decomposition is the unifying pattern</li> </ul> <p>This is exactly the kind of conceptual bridge diffusion researchers should care about.</p> <hr/> <h1 id="5-transition-matching-distillation">5. Transition Matching Distillation</h1> <p>Transition Matching Distillation (TMD) bridges engineering and theory based adaptation of MeanFlow to <strong>video distillation</strong>.</p> <h3 id="51-core-problem-setup">5.1 Core problem setup</h3> <p>They want to distill a pretrained video diffusion teacher into a faster student. Direct one-stage distillation is hard in video because:</p> <ul> <li>the space is huge,</li> <li>temporal consistency matters,</li> <li>transformer-based video models make JVP painful (esp. attention kernels/FSDP/context parallelism).</li> </ul> <p>So TMD uses <strong>two stages</strong>.</p> <h3 id="52-stage-1-transition-matching-meanflow-tm-mf">5.2 Stage 1: Transition Matching MeanFlow (TM-MF)</h3> <p>This is the key new idea.</p> <p>Instead of applying MeanFlow directly in the original latent/data space, they define an <strong>inner transition</strong> problem and parameterize a conditional inner flow map via average velocity: \(f_\theta(y_s,s,r;m) = y_s + (s-r)u_\theta(y_s,s,r;m)\)</p> <p>where (m) is a feature extracted from the main backbone.</p> <p>Then they use a MeanFlow-style objective to train this transition head.</p> <p>A very practical (and nontrivial) design choice:</p> <ul> <li>they <strong>reparameterize</strong> the average velocity to stay aligned with the teacher head: \(u_\theta(y_s,s,r;m) = y_1 - \text{head}_\theta(y_s,s,r;m)\)</li> </ul> <p>This is not cosmetic. It keeps the new head close to teacher semantics, which improves stability.</p> <h3 id="53-jvp-issue-and-finite-difference-approximation">5.3 JVP issue and finite-difference approximation</h3> <p>This paper is very realistic about systems constraints:</p> <ul> <li>exact JVP is annoying with large-scale video transformer stacks (FlashAttention, FSDP, context parallelism),</li> <li>so they use a <strong>finite-difference approximation</strong> of the JVP.</li> </ul> <p>That‚Äôs a practical compromise:</p> <ul> <li>theoretically less clean than exact JVP,</li> <li>but massively easier to integrate into production-grade training code.</li> </ul> <h3 id="54-stage-2-distributional-distillation-objective">5.4 Stage 2: Distributional distillation objective</h3> <p>After TM-MF pretraining, they switch to a stronger distillation stage using a VSD/discriminator-style objective (their simplified algorithm shows): \(\mathcal{L} = \text{VSD}(\hat{x}) + \lambda \cdot \text{Discriminator}(\hat{x})\)</p> <p>So the conceptual split is:</p> <ul> <li><strong>Stage 1 (TM-MF):</strong> learn a good transition-aware student parameterization, bootstrap geometry/dynamics</li> <li><strong>Stage 2:</strong> sharpen sample quality and distribution match</li> </ul> <p>This is a strong template for hard domains (video, 3D, multimodal) where pure one-shot distillation is brittle.</p> <hr/> <h1 id="6-terminal-velocity-matching">6. Terminal Velocity Matching</h1> <p>Terminal Velocity Matching (TVM): A More Principled Objective for One/Few-Step Models</p> <h3 id="61-what-it-changes">6.1 What it changes</h3> <p>Prior methods (FM/MeanFlow/FMM-style) mostly match local or initial-time velocity constraints.</p> <p>TVM says: match the <strong>terminal velocity</strong> of the flow trajectory instead.</p> <p>That sounds minor, but it changes the theory:</p> <ul> <li>they derive an explicit <strong>2-Wasserstein upper bound</strong></li> <li>and motivate a more stable training target for one/few-step generation</li> </ul> <h3 id="62-core-theorem-and-loss-structure">6.2 Core theorem and loss structure</h3> <p>TVM introduces a terminal velocity target \(u^*(x_t,t,s) = \mathbb{E}[v_t \mid x_t]\) (at terminal pairing (s), with the paper‚Äôs precise conditioning)</p> <p>Then they define a target involving a time derivative of the learned map: \(u^*_\theta(x_t,t,s) = u^*(x_t,t,s) - (t-s)\partial_s F_\theta(x_t,t,s)\)</p> <p>and train with a matching loss of the form \(\mathbb{E}\|F_\theta - u^*_\theta\|^2\)</p> <p>The crucial thing is <strong>not</strong> just the formula; it‚Äôs the theorem: they show this objective upper-bounds the 2-Wasserstein distance (up to constants / residual terms in their theorem statement).</p> <p>That gives TVM a stronger distributional interpretation than ‚Äújust match a derivative identity.‚Äù</p> <h3 id="63-significance">6.3 Significance</h3> <p>This is the first really clean signal that the field is maturing beyond:</p> <ul> <li>local consistency heuristics,</li> <li>empirical JVP identities,</li> <li>‚Äúit works in 1 step‚Äù</li> </ul> <p>into:</p> <ul> <li><strong>distributional guarantees</strong></li> <li>explicit control over terminal behavior</li> <li>better theory-practice alignment</li> </ul> <h3 id="64-the-systems-contribution">6.4 The systems contribution</h3> <p>TVM also points out a major implementation pain:</p> <ul> <li>JVP of scaled dot-product attention is poorly supported / inefficient in standard autograd stacks.</li> <li>Unlike prior works, TVM also propagates gradient through the JVP term (not just stop-grad around it), which is even harder.</li> </ul> <p>They propose a FlashAttention kernel that fuses JVP with forward pass and supports backward through the JVP result.</p> <p>That matters a lot if you care about scaling this family to modern DiT/transformer stacks.</p> <hr/> <h1 id="7-unifying-view-the-fields-progression-in-one-picture">7. Unifying View: The Field‚Äôs Progression in One Picture</h1> <p>Almost every method in this area can be seen as:</p> <ol> <li><strong>Diagonal anchor</strong> <ul> <li>when time-pair collapses, match a standard object (FM velocity / posterior / terminal target)</li> </ul> </li> <li><strong>Off-diagonal propagation</strong> <ul> <li>via a differential identity (MeanFlow / FMM),</li> <li>or consistency/composition (stochastic/meta flow maps),</li> <li>or terminal-time control (TVM)</li> </ul> </li> </ol> <p>This ‚Äúdiagonal + propagation‚Äù lens is the best mental model for the literature.</p> <hr/> <h1 id="8-practical-research-takeaways-for-top-lab-diffusion-folks">8. Practical Research Takeaways (for top-lab diffusion folks)</h1> <h3 id="81-if-youre-doing-one-stepfew-step-image-generation">8.1 If you‚Äôre doing one-step/few-step image generation</h3> <p>Start by deciding which failure mode you care about:</p> <ul> <li><strong>trajectory geometry / displacement quality</strong> ‚Üí MeanFlow-style</li> <li><strong>distributional mismatch under self-generated rollouts</strong> ‚Üí add FreeFlowMap-style correction</li> <li><strong>theory / Wasserstein control</strong> ‚Üí TVM-style objective</li> </ul> <h3 id="82-if-youre-doing-video--transformers">8.2 If you‚Äôre doing video / transformers</h3> <p>The math is not the main bottleneck; <strong>JVP implementation is</strong>. TMD and TVM both basically scream this:</p> <ul> <li>attention kernels + JVP + distributed training are the real constraint</li> <li>finite-difference JVP is often the ‚Äúactually trains‚Äù solution</li> <li>custom kernels become a differentiator</li> </ul> <h3 id="83-if-youre-doing-3d--world-models--stochastic-simulators">8.3 If you‚Äôre doing 3D / world models / stochastic simulators</h3> <p>The stochastic flow-map lens is probably the most future-proof:</p> <ul> <li>deterministic flow maps are great for one-step generation</li> <li>but world models usually need stochastic transitions</li> <li>the <strong>diagonal posterior + consistency</strong> formulation is much closer to what you actually want</li> </ul> <hr/> <h1 id="9-a-compact-theory-stack-to-remember">9. A compact ‚Äútheory stack‚Äù to remember</h1> <p>The whole area can be compressed into this stack:</p> <ol> <li><strong>FM</strong>: learn local tangent</li> <li><strong>MeanFlow/FMM</strong>: convert local tangent into a trainable two-time displacement rule (via differential identity/JVP)</li> <li><strong>Self-distilled flow maps</strong>: train on the student‚Äôs own rollout distribution (fix teacher-data mismatch)</li> <li><strong>Correction terms</strong>: explicitly align marginals / noising velocities</li> <li><strong>Stochastic flow maps</strong>: move from deterministic maps to transition kernels (posterior-diagonal + consistency)</li> <li><strong>TVM</strong>: choose a target (terminal velocity) that gives stronger distributional guarantees</li> </ol> <p>That‚Äôs the real progression.</p> <hr/> <h1 id="10-where-the-field-is-likely-going-next">10. Where the field is likely going next</h1> <p>The next wave is probably a merge of these threads:</p> <ul> <li><strong>TVM-style distributional guarantees</strong></li> <li><strong>self-distilled prior-only training</strong></li> <li><strong>stochastic transition operators</strong></li> <li><strong>kernel-aware JVP training for transformers/video/world models</strong></li> </ul> <p>In other words: the future is not just ‚Äúbetter one-step image generation,‚Äù it‚Äôs <strong>learned transition operators</strong> for high-dimensional structured dynamics (video, 3D, simulators, world models), with both:</p> <ul> <li>strong numerical behavior (few NFEs)</li> <li>and actual distributional control (Wasserstein/KL-style guarantees)</li> </ul> <p>That‚Äôs exactly where flow maps stop being a distillation trick and become a real modeling primitive.</p>]]></content><author><name></name></author><category term="research-survey"/><category term="diffusion"/><category term="survey"/><summary type="html"><![CDATA[Recent developments of diffusion distillation techniques]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://abecid.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://abecid.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://abecid.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>Learn more:Learn more:Learn more:Learn more:Learn more:Learn more:May 14, 2024 We‚Äôre introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we‚Äôre introducing Gemini 1.5 Flash: a model that‚Äôs lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We‚Äôre also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5‚Äôs 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It‚Äôs optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it‚Äôs a lighter weight model than 1.5 Pro, it‚Äôs highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it‚Äôs been trained by 1.5 Pro through a process called ‚Äúdistillation,‚Äù where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash‚Äôs availability and pricing.Over the last few months, we‚Äôve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we‚Äôve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We‚Äôve improved control over the model‚Äôs responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we‚Äôve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we‚Äôre now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do ‚Äî not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we‚Äôre also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We‚Äôre announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we‚Äôve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind‚Äôs mission to build AI responsibly to benefit humanity, we‚Äôve always wanted to develop universal AI agents that can be helpful in everyday life. That‚Äôs why today, we‚Äôre sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do ‚Äî and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we‚Äôve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we‚Äôve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we‚Äôve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they‚Äôre being used in, and respond quickly, in conversation.With technology like this, it‚Äôs easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We‚Äôve made incredible progress so far with our family of Gemini models, and we‚Äôre always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we‚Äôre able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google‚Äôs privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let‚Äôs stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><category term="external-posts"/><category term="google"/><summary type="html"><![CDATA[We‚Äôre sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://abecid.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://abecid.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://abecid.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio¬†Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website!¬†üéâüéâ</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as¬†sources.</p> <p>Any questions or suggestions? üëâ Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on¬†GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry></feed>