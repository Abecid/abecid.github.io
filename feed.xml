<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://abecid.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://abecid.github.io/" rel="alternate" type="text/html"/><updated>2026-02-24T11:50:29+00:00</updated><id>https://abecid.github.io/feed.xml</id><title type="html">Adam Abecid</title><entry><title type="html">Flow Maps and Distillation</title><link href="https://abecid.github.io/blog/2026/flowmap/" rel="alternate" type="text/html" title="Flow Maps and Distillation"/><published>2026-02-24T03:40:00+00:00</published><updated>2026-02-24T03:40:00+00:00</updated><id>https://abecid.github.io/blog/2026/flowmap</id><content type="html" xml:base="https://abecid.github.io/blog/2026/flowmap/"><![CDATA[<h1 id="flow-matching-to-flow-maps-to-distillation-a-deep-dive">Flow Matching to Flow Maps to Distillation: A Deep Dive</h1> <h1 id="table-of-contents">Table of Contents</h1> <ol> <li><a href="#1-foundations">Foundations</a> <ol> <li><a href="#11-diffusion">Diffusion</a></li> <li><a href="#12-flow-matching">Flow Matching</a></li> <li><a href="#13-rectified-flow">Rectified Flow</a></li> <li><a href="#14-flow-map">Flow Map</a></li> <li><a href="#15-consistency-models">Consistency Models</a></li> </ol> </li> <li><a href="#2-meanflow-family">MeanFlow Family</a> <ol> <li><a href="#21-meanflow">MeanFlow</a></li> <li><a href="#22-improved-meanflow-imf">Improved MeanFlow (iMF)</a></li> <li><a href="#23-alphaflow">AlphaFlow</a></li> <li><a href="#24-accelerating-and-improving-meanflow">Accelerating and Improving MeanFlow</a></li> <li><a href="#25-decoupled-meanflow">Decoupled MeanFlow</a></li> </ol> </li> <li><a href="#3-flowmap">FlowMap</a> <ol> <li><a href="#31-free-flowmap">Free FlowMap</a></li> <li><a href="#32-meta-flow-maps">Meta Flow Maps</a></li> <li><a href="#33-terminal-velocity-matching-tvm">Terminal Velocity Matching (TVM)</a></li> <li><a href="#34-transition-matching-distillation">Transition Matching Distillation (TMD)</a></li> </ol> </li> </ol> <h1 id="overview">Overview</h1> <p>Recent generative modeling utilize and develop upon flow maps and jvp based distillation techniques to reduce the number of function evaluations during inference. We focus on the Meanflow family, score distillation methods, and its applications in video generation.</p> <hr/> <h1 id="1-foundations">1. Foundations</h1> <h2 id="11-diffusion">1.1 Diffusion</h2> <p>Diffusion models define a <strong>forward noising process</strong> that gradually corrupts data into noise, and a <strong>reverse process</strong> that learns to reconstruct data from noise. The main reason diffusion models became dominant is that they are stable and high-quality, but the tradeoff is <strong>slow iterative sampling</strong>.</p> <h3 id="111-forward-process-discrete-ddpm-view">1.1.1 Forward process (discrete DDPM view)</h3> <p>In DDPM, the forward process is a Markov chain:</p> \[q(x_t \mid x_{t-1}) = \mathcal{N}\!\left(\sqrt{1-\beta_t}\,x_{t-1}, \beta_t I\right),\] <p>where $\beta_t \in (0,1)$ is a variance schedule.</p> <p>A key closed form is:</p> \[q(x_t \mid x_0) = \mathcal{N}\!\left(\sqrt{\bar\alpha_t}\,x_0,\,(1-\bar\alpha_t)I\right),\] <p>with</p> \[\alpha_t = 1-\beta_t,\qquad \bar\alpha_t=\prod_{s=1}^t \alpha_s.\] <p>So we can sample $x_t$ directly as:</p> \[x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon,\qquad \epsilon \sim \mathcal{N}(0,I).\] <p>This is the standard “data + Gaussian noise” interpolation used in many diffusion derivations.</p> <h3 id="112-reverse-process-and-denoising-objective">1.1.2 Reverse process and denoising objective</h3> <p>The generative model learns the reverse transitions</p> \[p_\theta(x_{t-1}\mid x_t),\] <p>which are parameterized via a neural network (predicting noise, $x_0$, or velocity depending on parameterization).</p> <p>The most common training objective is noise prediction:</p> \[\mathcal{L}_{\text{simple}}(\theta) = \mathbb{E}_{t,x_0,\epsilon} \left[ \left\|\epsilon - \epsilon_\theta(x_t,t)\right\|^2 \right].\] <p>This objective is simple and works extremely well, but inference still requires many reverse denoising steps.</p> <h3 id="113-continuous-time-diffusion-sde-view">1.1.3 Continuous-time diffusion (SDE view)</h3> <p>A continuous-time diffusion can be written as an SDE:</p> \[dx = f(x,t)\,dt + g(t)\,dW_t,\] <p>where $f$ is drift, $g$ is diffusion scale, and $W_t$ is a Wiener process.</p> <p>The reverse-time generative dynamics also form an SDE involving the score:</p> \[\nabla_x \log p_t(x).\] <p>This is the bridge to <strong>score-based generative modeling</strong> and continuous-time transport formulations.</p> <h3 id="114-probability-flow-ode-deterministic-counterpart">1.1.4 Probability flow ODE (deterministic counterpart)</h3> <p>Every diffusion SDE has an associated deterministic <strong>probability flow ODE</strong> that shares the same marginals $p_t$:</p> \[\frac{dx}{dt} = f(x,t) - \frac{1}{2}g(t)^2 \nabla_x \log p_t(x).\] <p>This is huge conceptually because it turns diffusion sampling into solving an ODE, which directly connects to:</p> <ul> <li>continuous normalizing flows,</li> <li>flow matching,</li> <li>rectified flow,</li> <li>and later flow-map distillation methods.</li> </ul> <p>So diffusion is not just “denoising noise,” it is also a <strong>continuous transport process</strong> in disguise.</p> <h3 id="115-why-diffusion-motivates-distillation">1.1.5 Why diffusion motivates distillation</h3> <p>Diffusion teachers are strong but slow because generation requires many function evaluations (NFEs). Distillation methods aim to compress this long trajectory into:</p> <ul> <li><strong>few-step samplers</strong> (e.g. 2–8 steps),</li> <li>or even <strong>one-step generators</strong>,</li> </ul> <p>while preserving the teacher’s learned transport geometry.</p> <p>This is exactly why Chapter 1 naturally progresses from <strong>Diffusion $\to$ Flow Matching $\to$ Flow Maps / Consistency / Distillation</strong>.</p> <h2 id="12-flow-matching">1.2 Flow Matching</h2> <p><img src="/assets/img/blogs/1_distillation/flowmatching.png" alt="Flow Matching"/></p> <p><em>Figure 1. Flow Matching. from Sabour, Fidler, and Kreis (2025),</em> Align Your Flow: Scaling Continuous-Time Flow Map Distillation <em>(arXiv:2506.14603).</em></p> <p>Flow Matching (FM) reframes generative modeling as directly learning a <strong>time-dependent velocity field</strong> that transports a simple source distribution (usually Gaussian noise) to the data distribution.</p> <h3 id="121-core-idea-learn-the-instantaneous-velocity">1.2.1 Core idea: learn the instantaneous velocity</h3> <p>Instead of learning reverse denoising conditionals, FM learns a vector field</p> \[u_\theta(x,t)\] <p>such that samples evolve via the ODE</p> \[\frac{dx}{dt} = u_\theta(x,t),\qquad t\in[0,1].\] <p>If this ODE is integrated from source noise at $t=0$ to $t=1$, the final samples should follow the data distribution.</p> <h3 id="122-conditional-path-and-target-velocity">1.2.2 Conditional path and target velocity</h3> <p>FM is usually trained by defining a conditional interpolation path between paired endpoints $(x_0,x_1)$:</p> \[x_t = \psi_t(x_0,x_1).\] <p>For simple linear interpolation:</p> \[x_t = (1-t)x_0 + tx_1.\] <p>The target velocity along this path is:</p> \[\dot{x}_t = \frac{d}{dt}\psi_t(x_0,x_1).\] <p>For the linear path, this becomes:</p> \[\dot{x}_t = x_1 - x_0.\] <p>The model is trained to match this conditional velocity in expectation:</p> \[\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t,x_0,x_1} \left[ \left\|u_\theta(x_t,t)-\dot{x}_t\right\|^2 \right].\] <p>So FM is “supervised vector field learning” on a chosen path family.</p> <h3 id="123-why-fm-is-attractive">1.2.3 Why FM is attractive</h3> <p>Compared to score/diffusion training, FM gives a very clean ODE-learning objective and avoids explicit score estimation. It is especially natural when you want to reason about:</p> <ul> <li>transport geometry,</li> <li>ODE trajectories,</li> <li>and later finite-time maps (flow maps).</li> </ul> <h3 id="124-limitation-local-field-expensive-sampling">1.2.4 Limitation: local field, expensive sampling</h3> <p>FM learns a <strong>local tangent</strong> $u_\theta(x,t)$, not a finite-time jump. That means sampling still requires numerical integration:</p> \[x_{t+\Delta t} \approx x_t + \Delta t\,u_\theta(x_t,t)\] <p>(or higher-order solvers like Heun).</p> <p>If trajectories are curved, discretization error accumulates, so many NFEs are needed. This is the main bottleneck that motivates:</p> <ul> <li>Rectified Flow (straighter trajectories),</li> <li>Flow Maps (direct time-to-time transport),</li> <li>and distillation methods (few-step or one-step generation).</li> </ul> <h2 id="13-rectified-flow">1.3 Rectified Flow</h2> <p>Rectified Flow (RF) keeps the ODE/transport framing of FM, but explicitly pushes the learned trajectories to become <strong>straighter</strong>, which makes them much easier to sample with few steps.</p> <h3 id="131-motivation-straight-trajectories-are-cheap">1.3.1 Motivation: straight trajectories are cheap</h3> <p>If a trajectory is highly curved, Euler updates need many small steps. If a trajectory is nearly straight, even a coarse solver can track it accurately.</p> <p>So RF is basically a geometry-aware fix to the FM sampling bottleneck.</p> <h3 id="132-linear-interpolation-path-and-velocity-target">1.3.2 Linear interpolation path and velocity target</h3> <p>A standard RF path is the same linear interpolation:</p> \[x_t = (1-t)x_0 + tx_1,\] <p>with instantaneous derivative</p> \[\frac{dx_t}{dt} = x_1 - x_0.\] <p>RF trains a velocity field to match this transport direction along the path:</p> \[v_\theta(x_t,t) \approx x_1 - x_0.\] <p>A common training objective is:</p> \[\mathcal{L}_{\text{RF}}(\theta) = \mathbb{E}_{t,x_0,x_1} \left[ \left\|v_\theta(x_t,t)-(x_1-x_0)\right\|^2 \right].\] <p>This looks similar to FM, but the interpretation is sharper: RF cares about learning a transport field whose induced trajectories are easy to discretize.</p> <h3 id="133-reflow-iterative-rectification">1.3.3 Reflow (iterative rectification)</h3> <p>A major practical idea in RF is <strong>reflow</strong>:</p> <ol> <li>Train an initial transport field.</li> <li>Sample trajectories from the model.</li> <li>Use those trajectories (or endpoint couplings) to retrain a straighter field.</li> <li>Repeat.</li> </ol> <p>Each round reduces curvature and improves few-step generation. This is why RF is often viewed as a bridge between classical diffusion/FM and modern one-step distillation.</p> <h3 id="134-why-rf-matters-for-the-rest-of-this-blog">1.3.4 Why RF matters for the rest of this blog</h3> <p>RF is the clean conceptual bridge to flow-map methods because it shifts the focus from “match local field” to “shape trajectories for fast transport.” Once you think this way, the next obvious step is:</p> <blockquote> <p>Why learn only the local tangent at all?<br/> Why not learn the <strong>finite-time map</strong> directly?</p> </blockquote> <p>That is exactly the flow-map perspective.</p> <h2 id="14-flow-map">1.4 Flow Map</h2> <p><img src="/assets/img/blogs/1_distillation/flowmap.png" alt="Flow Map"/></p> <p><em>Figure 2. Flow Map. from Sabour, Fidler, and Kreis (2025),</em> Align Your Flow: Scaling Continuous-Time Flow Map Distillation <em>(arXiv:2506.14603).</em></p> <p>Flow-map methods move beyond local vector fields and directly learn a <strong>time-to-time transport operator</strong>.</p> <h3 id="141-from-vector-field-to-finite-time-map">1.4.1 From vector field to finite-time map</h3> <p>Given a velocity field $u(x,t)$ and its ODE</p> \[\frac{dx}{dt}=u(x,t),\] <p>the associated flow map $\phi_u$ sends a state from time $t$ to time $s$:</p> \[\phi_u(x_t,t,s)=x_s.\] <p>So instead of learning only the local tangent $u(x,t)$, we learn the finite-time update:</p> \[x_s \approx \phi_\theta(x_t,t,s).\] <p>This is much more aligned with few-step sampling, because a single model evaluation can move across a large time interval.</p> <h3 id="142-why-flow-maps-help-distillation">1.4.2 Why flow maps help distillation</h3> <p>A flow field gives infinitesimal updates; a flow map gives finite jumps.</p> <p>That means flow maps are naturally suited for:</p> <ul> <li><strong>few-step sampling</strong> (large $t\to s$ jumps),</li> <li><strong>teacher-student distillation</strong> (student imitates teacher transitions),</li> <li><strong>self-distillation</strong> (model supervises its own multistep consistency),</li> <li>and <strong>data-free distillation</strong> variants (matching dynamics without original data).</li> </ul> <p>This is the key conceptual move behind MeanFlow-family and FreeFlowMap-family methods.</p> <h3 id="143-semigroup--composition-structure">1.4.3 Semigroup / composition structure</h3> <p>Exact flow maps satisfy a composition rule (semigroup property):</p> \[\phi(x_t,t,r) = \phi\!\left(\phi(x_t,t,s),\,s,\,r\right) \qquad \text{for } t \le s \le r.\] <p>This property is incredibly important because it gives a built-in consistency constraint across time triples. Many modern distillation methods exploit some version of this:</p> <ul> <li>explicit composition matching,</li> <li>consistency losses,</li> <li>JVP-based local constraints that imply finite-time consistency.</li> </ul> <h3 id="144-relation-to-fm-and-rf">1.4.4 Relation to FM and RF</h3> <ul> <li><strong>FM</strong> learns $u(x,t)$ (local instantaneous velocity)</li> <li><strong>RF</strong> improves trajectory geometry (straighter ODE paths)</li> <li><strong>Flow Map</strong> learns $\phi(x_t,t,s)$ (finite-time transport)</li> </ul> <p>So flow maps are not a totally different universe; they are the natural next abstraction after FM/RF if your goal is <strong>fast generation</strong>.</p> <h2 id="15-consistency-models">1.5 Consistency Models</h2> <p>Consistency Models (CMs) attack the same bottleneck from another angle: instead of learning a vector field or even an explicit flow map, they learn a <strong>cross-time consistent predictor</strong> that maps noisy states to a shared target representation (often an estimate of clean data).</p> <p>This makes them one of the foundational one-step / few-step distillation paradigms.</p> <h3 id="151-core-consistency-idea">1.5.1 Core consistency idea</h3> <p>Suppose $x_t$ and $x_s$ lie on the same teacher trajectory (or same underlying denoising path). A consistency model $f_\theta$ is trained so that: \(f_\theta(x_t,t) \approx f_\theta(x_s,s),\) after the appropriate scaling/parameterization.</p> <p>In words: different noise levels along the same trajectory should produce the same final prediction.</p> <p>This is a <strong>cross-time agreement constraint</strong>, not just a local derivative-matching objective.</p> <h3 id="152-why-this-enables-one-step-generation">1.5.2 Why this enables one-step generation</h3> <p>Because the model is trained to collapse trajectory points to a common target, we can often sample by evaluating the model once (or very few times) from a noisy input.</p> <p>This directly targets inference speed, unlike standard diffusion training which optimizes denoising accuracy at every step but does not inherently optimize for low-NFE sampling.</p> <h3 id="153-teacher-student-consistency-distillation">1.5.3 Teacher-student consistency distillation</h3> <p>A common setup is:</p> <ol> <li>Start with a strong diffusion/score teacher.</li> <li>Generate paired states $(x_t, x_s)$ on teacher trajectories.</li> <li>Train the student consistency model to agree across those states.</li> </ol> <p>This makes consistency models a very important predecessor to later:</p> <ul> <li>flow-map distillation,</li> <li>self-distillation,</li> <li>and JVP-based transport-map objectives.</li> </ul> <h3 id="154-conceptual-relation-to-flow-maps">1.5.4 Conceptual relation to flow maps</h3> <p>Consistency models and flow-map methods are closely related in spirit:</p> <ul> <li><strong>Flow map view:</strong> learn explicit transport $\phi(x_t,t,s)$</li> <li><strong>Consistency view:</strong> learn a representation/prediction that is invariant (or aligned) across times on the same trajectory</li> </ul> <p>Both replace purely local supervision with <strong>cross-time structure</strong>, which is exactly what you need for few-step and one-step generation.</p> <hr/> <h1 id="2-meanflow-family">2. MeanFlow Family</h1> <h2 id="21-meanflow">2.1 MeanFlow</h2> <h4 id="211-average-velocity-instead-of-instantaneous-velocity">2.1.1 Average velocity instead of instantaneous velocity</h4> <p><img src="/assets/img/blogs/1_distillation/meanflow.png" alt="Mean Flow"/></p> <p><em>Figure 3. Mean Flow, with different target timestep $t$. from Geng et al. (2025),</em> Mean Flows for One-step Generative Modeling <em>(arXiv:2505.13447).</em></p> <p>MeanFlow defines an <strong>average velocity</strong></p> \[u(z_t, r, t)\] <p>over the interval $[r,t]$, so the displacement is:</p> \[(t-r)u(z_t,r,t).\] <h4 id="212-the-meanflow-identity-the-central-derivation">2.1.2 The MeanFlow Identity (the central derivation)</h4> <p>Start from the definition:</p> \[(t-r)u(z_t,r,t) = \int_r^t v(z_\tau,\tau)\, d\tau\] <p>Differentiate both sides with respect to (t) (holding (r) fixed). By product rule + FTC:</p> \[u(z_t,r,t) + (t-r)\frac{d}{dt}u(z_t,r,t) = v(z_t,t)\] <p>Rearranging leads to the <strong>MeanFlow Identity</strong>:</p> \[u(z_t,r,t) = v(z_t,t) - (t-r)\frac{d}{dt}u(z_t,r,t)\] <h4 id="213-main-points">2.1.3 Main Points</h4> <ul> <li>The authors rewrite an intractable target (the average velocity integral) into a trainable target: <ul> <li>By first taking the derivative of both sides.</li> <li>Instantaneous velocity $v$ is available from FM-style interpolation.</li> <li>Total derivative term computed via <strong>JVP</strong>.</li> </ul> </li> <li>No explicit consistency regularizer is imposed.</li> <li>The consistency-like structure falls out from the definition of average velocity.</li> </ul> <h4 id="214-meanflow-loss">2.1.4 MeanFlow loss</h4> <p>Parameterize $u_\theta(z_t,r,t)$, and regress to the identity-induced target:</p> \[\mathcal{L}_{\text{MeanFlow}}(\theta) = \mathbb{E}\left[ \left\| u_\theta(z_t,r,t) - \operatorname{sg}(u_{\text{tgt}}) \right\|_2^2 \right]\] <p>with</p> \[u_{\text{tgt}} = v_t - (t-r)\left(v_t \,\partial_z u_\theta + \partial_t u_\theta\right)\] <p>where the total derivative is implemented through a JVP along tangent $(v_t, 0, 1)$.</p> <h4 id="215-sampling">2.1.5 Sampling</h4> <p>Once you learn the average velocity, sampling is:</p> \[z_r = z_t - (t-r)u(z_t,r,t)\] <p>For 1-step:</p> \[z_0 = z_1 - u(z_1,0,1),\quad z_1 \sim p_{\text{prior}}\] <hr/> <h2 id="22-improved-meanflow-imf">2.2 Improved MeanFlow (iMF)</h2> <p>iMF addresses two practical issues in MeanFlow:</p> <ol> <li>the self-referential target,</li> <li>fixed-CFG training (bad for inference-time flexibility).</li> </ol> <h4 id="221-meanflow-as-a-v-loss">2.2.1 MeanFlow as a v-loss</h4> <p>iMF rewrites the MeanFlow identity into a <strong>velocity regression form</strong>:</p> \[v(z_t) = u(z_t,r,t) + (t-r)\frac{d}{dt}u(z_t,r,t).\] <p>Then parameterize the RHS with $u_\theta$:</p> \[V_\theta = u_\theta(z_t,r,t) + (t-r)\,\mathrm{JVP}_{\mathrm{sg}}(u_\theta; v_\theta),\] <p>and train with a Flow-Matching-like loss</p> \[\mathcal{L}_{\mathrm{iMF}} = \mathbb{E}\left[\left\|V_\theta - (\epsilon-x)\right\|_2^2\right].\] <p>This is cleaner because the regression target is now the standard FM target $(\epsilon-x)$ rather than an apparent target constructed from $u_\theta$.</p> <h3 id="222-flexible-cfg-as-conditioning">2.2.2 Flexible CFG as conditioning</h3> <p>Original MeanFlow supports CFG in 1-NFE, but with a <strong>fixed guidance scale</strong> chosen at training time.</p> <p>iMF fixes this by making the guidance scale part of the conditioning:</p> <ul> <li>guidance scale $\omega$ becomes an input condition,</li> <li>the same model can be sampled with different CFG scales at inference.</li> </ul> <p>That is a big deal because the optimal CFG scale shifts with model size, training progress, and NFE.</p> <h3 id="223-in-context-conditioning">2.2.3 In-context conditioning</h3> <p>iMF also upgrades conditioning architecture:</p> <ul> <li>conditions include $(r,t)$, class label $c$, and guidance-related variables $\Omega$,</li> <li>each condition is represented with multiple learnable tokens,</li> <li>all condition tokens are concatenated with image latent tokens and processed by the Transformer.</li> </ul> <p>This allows:</p> <ul> <li>support richer heterogeneous conditioning more naturally,</li> <li>remove <strong>adaLN-zero</strong>,</li> <li>cut params significantly (they report about <strong>1/3 reduction</strong> in a base model setting).</li> </ul> <h2 id="23-alphaflow">2.3 AlphaFlow</h2> <p>AlphaFlow is the paper that gives the most useful conceptual interpretation of MeanFlow training.</p> <h4 id="231-core-insight-meanflow-decomposes-into-two-losses">2.3.1 Core insight: MeanFlow decomposes into two losses</h4> <p>AlphaFlow shows the MeanFlow objective can be algebraically decomposed into:</p> <ol> <li><strong>Trajectory Flow Matching (TFM)</strong></li> <li><strong>Trajectory Consistency (TC)</strong></li> </ol> <p>The decomposition (up to a constant) is:</p> \[\mathcal{L}_{\mathrm{MF}} = \underbrace{\mathbb{E}\left[\|u_\theta(z_t,r,t)-v_t\|_2^2\right]}_{\mathcal{L}_{\mathrm{TFM}}} + \underbrace{\mathbb{E}\left[2(t-r)\,u_\theta^\top \frac{d u_\theta^-}{dt}\right]}_{\mathcal{L}_{\mathrm{TC}}} + C.\] <p>Interpretation:</p> <ul> <li><strong>TFM</strong> says “fit the trajectory-local velocity target.”</li> <li><strong>TC</strong> says “be self-consistent along the trajectory.”</li> <li>MeanFlow is effectively a <strong>consistency-like model with extra trajectory FM supervision</strong>.</li> </ul> <h4 id="232-why-meanflow-often-needs-lots-of-border-case-fm-samples">2.3.2 Why MeanFlow often needs lots of border-case FM samples</h4> <p>AlphaFlow also explains a weird empirical fact from MeanFlow:</p> <ul> <li>MeanFlow works best when many samples use the border case $r=t$ (which looks like vanilla FM).</li> </ul> <p>Their analysis shows this is not just a hack:</p> <ul> <li>the gradients of TFM and trajectory consistency are often <strong>negatively correlated</strong>,</li> <li>so the extra FM-style supervision helps stabilize and speed up training.</li> </ul> <h4 id="233-α-flow-loss-one-objective-that-unifies-tfm-shortcut-meanflow">2.3.3 α-Flow loss: one objective that unifies TFM, Shortcut, MeanFlow</h4> <p>AlphaFlow defines a family of losses parameterized by $\alpha$:</p> \[\mathcal{L}_\alpha(\theta) = \mathbb{E}_{t,r,z_t} \left[ \alpha^{-1} \left\| u_\theta(z_t,r,t) - \left(\alpha \,\tilde v_{s,t} + (1-\alpha)\,u_{\theta^-}(z_s,r,s)\right) \right\|_2^2 \right],\] <p>where</p> \[s = \alpha r + (1-\alpha)t\] <p>is an intermediate time.</p> <p>This unifies several training objectives:</p> <ul> <li>$\alpha=1$ gives <strong>trajectory flow matching</strong> (with suitable $\tilde v_{s,t}$),</li> <li>$\alpha=\tfrac{1}{2}$ recovers a <strong>Shortcut-style</strong> objective,</li> <li>$\alpha \to 0$ recovers the <strong>MeanFlow gradient</strong>.</li> </ul> <p>That is the key conceptual win: AlphaFlow puts FM, Shortcut, and MeanFlow on one continuum.</p> <h4 id="234-curriculum">2.3.4 Curriculum</h4> <p>Because TFM and TC conflict early in training, AlphaFlow uses a curriculum:</p> <ul> <li>start more FM-like (larger $\alpha$),</li> <li>gradually anneal toward MeanFlow-like behavior (smaller $\alpha$).</li> </ul> <p>This disentangles optimization and improves convergence.</p> <h4 id="235-takeaways">2.3.5 Takeaways</h4> <p>AlphaFlow is best understood as:</p> <ul> <li>a <strong>theory paper for MeanFlow optimization</strong> (decomposition + gradient conflict),</li> <li>plus a <strong>practical training recipe</strong> (curriculum over $\alpha$) that improves one-step/few-step quality.</li> </ul> <h2 id="24-accelerating-and-improving-meanflow">2.4 Accelerating and Improving MeanFlow</h2> <p>Understanding and Improving Mean Flow’s (UAIMF) main point is simple and very practical: MeanFlow training is bottlenecked by <strong>slow velocity formation</strong> and <strong>bad temporal-gap scheduling</strong>, so they speed up both. They propose two complementary components:</p> <ol> <li>accelerate the velocity-learning part with standard diffusion training tricks (they use <strong>MinSNR</strong> or <strong>DTD</strong>), and</li> <li>add a <strong>progressive weighting</strong> over the MeanFlow loss so the model learns small-gap average velocities first, then gradually expands to larger gaps.</li> </ol> <h3 id="241-why-meanflow-trains-slowly">2.4.1 Why MeanFlow trains slowly</h3> <p>UAIMF analyzes MeanFlow training through two coupled subproblems:</p> <ul> <li>learning instantaneous velocity (the easier FM-like part),</li> <li>learning average velocity over larger timestep gaps (the harder MeanFlow part).</li> </ul> <p>Their empirical claim is that <strong>rapid velocity formation helps MeanFlow converge much faster</strong>, and that the temporal gap matters a lot: large-gap average-velocity learning is harder and should be delayed. This is why they combine velocity acceleration + progressive gap weighting.</p> <h3 id="242-component-1-accelerate-the-velocity-part-minsnr--dtd">2.4.2 Component 1: Accelerate the velocity part (MinSNR / DTD)</h3> <p>UAIMF tests one method from each category:</p> <ul> <li><strong>MinSNR</strong> as a loss-weighting acceleration method</li> <li><strong>DTD</strong> as a timestep-sampling acceleration method</li> </ul> <p>and plugs them into MeanFlow training. They report both help, but emphasize that <strong>DTD is more robust across model scales</strong> because it changes the sampling distribution instead of interfering with MeanFlow’s own adaptive loss normalization.</p> <h3 id="243-component-2-progressive-weighting-on-the-meanflow-loss">2.4.3 Component 2: Progressive weighting on the MeanFlow loss</h3> <p>UAIMF progressively reweights the MeanFlow term so training starts by emphasizing <strong>small temporal gaps</strong> (easy) and gradually transitions to <strong>uniform weighting</strong> (full MeanFlow objective). Their weighting is:</p> \[\beta(\Delta t, s) = 1 - s + \lambda s (1 - \Delta t)\] <p>where:</p> <ul> <li>\(\Delta t\) is the temporal gap,</li> <li>\(s \in [0,1]\) is training progress,</li> <li>\(\lambda\) normalizes the expectation at initialization.</li> </ul> <p>At initialization, the weighting prioritizes small gaps; by the end, it becomes uniform. They use a linear schedule by default:</p> \[s = 1 - \frac{i}{T}\] <p>and also discuss the generalized schedule:</p> \[s = 1 - \left(\frac{i}{T}\right)^k\] <p>with \(k=1\) (linear) working best in their ablations.</p> <h3 id="244-why-the-two-components-work-together">2.4.4 Why the two components work together</h3> <p>Their ablation is clean:</p> <ul> <li>velocity acceleration alone improves MeanFlow,</li> <li>progressive \(L_u\) weighting alone improves MeanFlow,</li> <li>combining both works best.</li> </ul> <p>They explicitly interpret this as:</p> <ul> <li>acceleration methods quickly establish the <strong>instantaneous velocity foundation</strong></li> <li>progressive weighting improves <strong>average velocity learning</strong> over time</li> </ul> <p>which is exactly the right mental model for MeanFlow optimization.</p> <h2 id="25-decoupled-meanflow">2.5 Decoupled MeanFlow</h2> <p>Decoupled MeanFlow is the strongest architectural update in this line. The core idea is: <strong>the encoder should care about the current timestep, and the decoder should care about the target timestep</strong>. They decouple timestep conditioning and turn a pretrained flow model into a flow-map model with almost no architectural surgery.</p> <h3 id="251-core-architectural-idea-decouple-encoder-vs-decoder-conditioning">2.5.1 Core architectural idea: decouple encoder vs decoder conditioning</h3> <p>They reinterpret a flow model as:</p> \[v_\theta = g_\theta \circ f_\theta\] <p>with an encoder \(f_\theta\) and decoder \(g_\theta\). Then they argue the standard MeanFlow design is redundant because it feeds the next timestep \(r\) everywhere. Their fix:</p> <ul> <li>encoder gets current timestep \(t\)</li> <li>decoder gets next timestep \(r\)</li> </ul> <p>and the flow map becomes:</p> \[u_\theta(x_t, t, r) = g_\theta(f_\theta(x_t, t), r)\] <p>This is the defining DMF equation.</p> <h3 id="252-why-this-matters-pretrained-flow-models-already-contain-flow-map-structure">2.5.2 Why this matters: pretrained flow models already contain flow-map structure</h3> <p>DMF shows that a pretrained flow model can be <strong>converted into a flow map without fine-tuning</strong>, just by choosing an encoder/decoder split and decoding the representation with \(r\). They report the converted DMF can even outperform the original flow model in some settings, which supports the claim that good flow-model representations are already enough for flow-map prediction.</p> <p>This is a major conceptual shift: instead of training flow maps from scratch, you can <strong>reuse pretrained flow-model representations</strong> and repurpose the decoder.</p> <h3 id="253-representation-first-view">2.5.3 Representation-first view</h3> <p>DMF explicitly argues that <strong>representation quality matters</strong> for flow maps. They show:</p> <ul> <li>stronger pretrained encoders transfer better to flow-map fine-tuning</li> <li>freezing encoder + tuning decoder already gives a large speed/quality gain</li> <li>but true 1-step performance needs joint optimization (encoder cannot stay frozen forever)</li> </ul> <p>This gives a practical recipe: pretrain a strong flow model first, then convert/fine-tune as DMF.</p> <h3 id="254-training-recipe-fm-warm-up--mf-fine-tuning">2.5.4 Training recipe: FM warm-up + MF fine-tuning</h3> <p>DMF also proposes a better training pipeline:</p> <ol> <li>train a flow model with FM loss</li> <li>convert to DMF</li> <li>fine-tune with MeanFlow loss</li> </ol> <p>They justify this on compute grounds (MF/JVP is expensive) and show it scales better than training a flow map from scratch. This is one of the most important practical contributions in the paper.</p> <h3 id="255-enhanced-training-techniques">2.5.5 Enhanced training techniques</h3> <h4 id="a-adaptive-weighted-cauchy-loss">(a) Adaptive weighted Cauchy loss</h4> <p>They note MF loss has high variance, then replace the raw MSE-style MF loss with a <strong>Cauchy (Lorentzian) robust loss</strong> and an adaptive weighting term over timestep pairs. Their DMF objective is written as an adaptive weighted Cauchy form over the MeanFlow residual.</p> <h4 id="b-time-proposal-tailored-to-flow-maps">(b) Time proposal tailored to flow maps</h4> <p>They adapt timestep-pair sampling for flow maps (since you need ordered pairs with \(t&gt;r\)). They sample two logit-normal values and sort them, and then bias the proposal toward larger gaps / smaller \(r\) for better 1-step behavior, because converted DMF models are already strong near the diagonal \(r \approx t\).</p> <h4 id="c-model-guidance-mg">(c) Model Guidance (MG)</h4> <p>They use <strong>Model Guidance (MG)</strong> to avoid the full compute cost of CFG during training, and note MG is especially effective for training high-quality few-step flow maps. This is part of why their 1-step/4-step results are strong.</p> <h3 id="256-takeaways">2.5.6 Takeaways</h3> <p>DMF is not just another MeanFlow variant. It reframes the problem:</p> <ul> <li><strong>MeanFlow</strong> gives the right objective (average velocity via JVP)</li> <li><strong>UAIMF / AlphaFlow</strong> improve optimization dynamics</li> <li><strong>DMF</strong> improves the <strong>architecture + training pipeline</strong>, and shows pretrained flow models are the best starting point</li> </ul> <p>They report SOTA-level few-step results and show 1-step / 4-step generation approaching much more expensive flow-model sampling with large inference-speed gains.</p> <hr/> <h1 id="3-flowmap">3. FlowMap</h1> <h2 id="31-free-flowmap">3.1 Free FlowMap</h2> <p>Flow Map Distillation Without Data bias</p> <h4 id="311-teacher-data-mismatch-the-hidden-bug-in-many-distillation-pipelines">3.1.1 Teacher-data mismatch (the hidden bug in many distillation pipelines)</h4> <p>Traditional flow-map distillation often samples intermediate states $x_t$ from an <strong>external dataset distribution</strong> and supervises the student using teacher velocities at those states.</p> <p>But the student is supposed to reproduce the teacher’s <strong>sampling process</strong>, i.e. the trajectory distribution induced by the teacher from the prior.</p> <p>Supervision states coming from a mismatched distribution results in a <strong>teacher-data mismatch</strong>:</p> <ul> <li>the student is trained on states that are not on the teacher’s true rollout distribution,</li> <li>more augmentation can worsen it,</li> <li>student quality degrades.</li> </ul> <p>This is a deep point because it says the standard “distill on data” recipe is fundamentally misaligned with the actual objective (imitate the teacher sampler).</p> <h4 id="312-prior-only--self-generated-flow-map-objective">3.1.2 Prior-only / self-generated flow-map objective</h4> <p>Supervise entirely from the prior and the student’s own generated states.</p> <p>They derive a sufficient optimality condition leading to a loss of the form</p> \[\mathcal{L}_{\text{pred}} = \mathbb{E}_{z,\delta} \left[ \|F_\theta(z,\delta)-\operatorname{sg}(u_{\text{target}})\|^2 \right]\] <p>with</p> \[u_{\text{target}} = u(f_\theta(z,\delta),1-\delta) - \delta \,\partial_\delta F_\theta(z,\delta)\] <p>The key interpretation:</p> <ul> <li>$f_\theta(z,\delta)$ defines the student’s current trajectory,</li> <li>$\partial_\delta f_\theta$ is the <strong>student’s generating velocity</strong>,</li> <li>the loss is equivalent to aligning the student generating velocity with the teacher field:</li> </ul> \[\partial_\delta f_\theta \approx u(f_\theta(z,\delta),1-\delta)\] <p>So the student learns to <strong>ride the teacher vector field along its own generated path</strong>, starting from pure noise, no dataset needed.</p> <p>This is the right fix for teacher-data mismatch.</p> <h4 id="313-gradient-view">3.1.3 Gradient view</h4> <p>They explicitly write the optimization gradient in terms of a velocity mismatch</p> \[\Delta v_{G,u} = v_G - u\] <p>which is great because it makes gradient weighting / normalization tricks easier to reason about.</p> <p>This is one of those “small” presentation choices that actually matters in practice.</p> <h4 id="314-correction-objective-fixing-distribution-drift-not-just-local-velocity">3.1.4 Correction objective (fixing distribution drift, not just local velocity)</h4> <p>Here’s the catch: aligning generating velocity locally is necessary, but in finite-capacity/discrete training the generated distribution can still drift.</p> <p>So they add a <strong>correction objective</strong> motivated from minimizing a KL term over intermediate marginals and then translating score mismatch into <strong>velocity mismatch</strong> (via the score–velocity equivalence for linear interpolants).</p> <p>This yields a gradient proportional to</p> \[\nabla_\theta \; \mathbb{E}_{z,n,r} \left[ F_\theta(z,1)^\top \operatorname{sg} \left( v_N(I_r(f_\theta(z,1),n),r) - u(I_r(f_\theta(z,1),n),r) \right) \right]\] <p>where:</p> <ul> <li>$u$ is the teacher marginal velocity,</li> <li>$v_N$ is the student-induced <strong>noising</strong> marginal velocity,</li> <li>$I_r(\cdot,\cdot)$ is the interpolation to intermediate time $r$.</li> </ul> <p>Intuition:</p> <ul> <li>the prediction loss aligns the <strong>student’s forward/generating flow</strong></li> <li>the correction loss aligns the <strong>student-induced noising marginals</strong> with the teacher’s marginals</li> </ul> <p>That is a nice bidirectional correction mechanism.</p> <hr/> <h2 id="32-meta-flow-maps">3.2 Meta Flow Maps</h2> <p>Meta flow maps correspond to a <strong>stochastic flow map</strong>, which is important because deterministic flow maps are too rigid.</p> <h4 id="321-why-stochastic-flow-maps">3.2.1 Why stochastic flow maps?</h4> <p>Deterministic flow-map learning works if the transport map is the right object. But for many diffusion-like processes, especially when you want richer uncertainty handling, a <strong>stochastic transition kernel</strong></p> <p>\(\kappa_{t,s}(z_t, z_s)\) is the right object.</p> <p>The paper frames this using:</p> <ul> <li><strong>marginal consistency</strong></li> <li><strong>conditional consistency</strong></li> <li> <table> <tbody> <tr> <td>a family of posterior conditionals $p_{1</td> <td>t}$</td> </tr> </tbody> </table> </li> <li>and a diagonal supervision view</li> </ul> <p>The important conceptual upgrade is:</p> <ul> <li>instead of only learning deterministic trajectories,</li> <li>learn a transition operator consistent with the stochastic process structure.</li> </ul> <h4 id="322-the-diagonal-condition-same-role-as-fm-diagonal-supervision">3.2.2 The diagonal condition (same role as FM diagonal supervision)</h4> <p>They derive that on the diagonal:</p> \[\kappa_{t,t}(z_t, z_1) = p_{1|t}(z_1 \mid z_t)\] <p>This is the stochastic analogue of “when $r=t$, your two-time object must match the one-time target.”</p> <p>So the diagonal again plays the role of anchor supervision.</p> <h4 id="323-pathwiseconsistency-relation-for-stochastic-flow-maps">3.2.3 Pathwise/consistency relation for stochastic flow maps</h4> <p>They also derive a consistency/composition condition (their Eq. 23 in the snippet):</p> \[\kappa_{t,s}(z_t,z_s) = \mathbb{E}_{z_1 \sim p_{1|t}(\cdot|z_t)} \big[ \kappa_{u,s}(z_u,z_s) \big]\] <p>(with the appropriate latent dependence through $z_1$/paths)</p> <p>The exact notation is heavier, but the key idea is the same as flow-map composition:</p> <ul> <li><strong>two-time transitions must compose correctly through intermediate times</strong>, but now in distributional form.</li> </ul> <h4 id="324-their-training-objective-mfm-loss">3.2.4 Their training objective (MFM loss)</h4> <p>They build:</p> <ol> <li>a <strong>diagonal supervision loss</strong> (fit the posterior on diagonal time pairs),</li> <li>a <strong>consistency loss</strong> (enforce off-diagonal composition consistency),</li> <li>and combine them into an MFM objective:</li> </ol> \[\mathcal{L}_{\text{MFM}} = \mathcal{L}_{\text{diag}} + \lambda \mathcal{L}_{\text{cons}}\] <p>This is the stochastic counterpart of the deterministic progression:</p> <ul> <li>diagonal target = “FM-like” anchor</li> <li>off-diagonal consistency = “flow-map-like” propagation</li> </ul> <h4 id="325-takeaways">3.2.5 Takeaways</h4> <p>This paper gives a more general lens:</p> <ul> <li>MeanFlow / deterministic flow maps are one branch</li> <li>stochastic transition learning is the broader object when uncertainty matters</li> <li>the “diagonal + consistency” decomposition is the unifying pattern</li> </ul> <p>This is exactly the kind of conceptual bridge diffusion researchers should care about.</p> <hr/> <h2 id="33-terminal-velocity-matching-tvm">3.3 Terminal Velocity Matching (TVM)</h2> <h4 id="331-core-idea-match-the-terminal-velocity-of-a-learned-flow-map">3.3.1 Core idea: match the <strong>terminal</strong> velocity of a learned flow map</h4> <p>TVM parameterizes a <strong>two-time displacement map</strong> (flow map increment) directly, instead of only learning the instantaneous velocity field.</p> <p>Let the ground-truth displacement from time $t$ to $s$ be \(f(x_t,t,s) := \psi(x_t,t,s) - x_t.\)</p> <p>TVM uses a model \(f_\theta(x_t,t,s) = (s-t)F_\theta(x_t,t,s),\) and defines the model’s instantaneous velocity as the boundary derivative \(u_\theta(x_t,t) := \left.\frac{d}{ds}f_\theta(x_t,t,s)\right|_{s=t} = F_\theta(x_t,t,t).\)</p> <p>This is the key unification:</p> <ul> <li>the <strong>same network</strong> learns both <ol> <li>a large-step displacement map $f_\theta$, and</li> <li>an infinitesimal velocity field $u_\theta$.</li> </ol> </li> </ul> <h4 id="332-why-terminal-velocity">3.3.2 Why “terminal” velocity?</h4> <p>The ground-truth displacement satisfies \(f(x_t,t,s)=\int_t^s u(x_r,r)\,dr.\)</p> <p>Differentiate w.r.t. the <strong>terminal time</strong> $s$: \(\frac{d}{ds}f(x_t,t,s)=u(\psi(x_t,t,s),s).\)</p> <p>This is the terminal velocity condition.</p> <p>The nice part is: if the terminal-velocity condition is satisfied along the trajectory, then the displacement map error is controlled (TVM shows an upper bound of displacement error by integrated terminal-velocity error). So instead of directly supervising the full ODE integral, TVM supervises the <strong>derivative at the terminal endpoint</strong>.</p> <p>This is the conceptual contrast with MeanFlow:</p> <ul> <li><strong>MeanFlow</strong> differentiates w.r.t. the <strong>start time</strong> $t$,</li> <li><strong>TVM</strong> differentiates w.r.t. the <strong>end time</strong> $s$.</li> </ul> <h4 id="333-proxy-trick-how-they-make-it-trainable">3.3.3 Proxy trick (how they make it trainable)</h4> <p>The terminal condition depends on unknown ground-truth objects:</p> <ul> <li>$\psi(x_t,t,s)$ (true flow map)</li> <li>$u(\cdot,s)$ (true velocity field)</li> </ul> <p>TVM replaces them with model proxies: \(u(\psi(x_t,t,s),s) \;\approx\; u_\theta\big(x_t + f_\theta(x_t,t,s),\, s\big).\)</p> <p>So the model predicts a displacement to a new point \(x_s^{(\theta)} = x_t + f_\theta(x_t,t,s),\) then evaluates its own velocity field at that terminal point.</p> <p>This makes the loss <strong>self-consistent</strong> and trainable in one stage.</p> <h4 id="334-tvm-objective-the-actual-loss">3.3.4 TVM objective (the actual loss)</h4> <p>TVM jointly optimizes:</p> <ol> <li><strong>Terminal-velocity matching term</strong> (general $t \ge s$)</li> <li><strong>Flow Matching boundary term</strong> (special case / anchor)</li> </ol> <p>Per-time objective: \(\mathcal{L}_{\mathrm{TVM}}^{t,s}(\theta) = \mathbb{E} \Big[ \underbrace{ \left\| \frac{d}{ds}f_\theta(x_t,t,s) - u_\theta\big(x_t + f_\theta(x_t,t,s), s\big) \right\|_2^2 }_{\text{terminal velocity matching}} + \underbrace{ \|u_\theta(x_s,s)-v_s\|_2^2 }_{\text{Flow Matching anchor}} \Big].\)</p> <p>Where:</p> <ul> <li>$x_s$ is sampled from the interpolation path (as in Flow Matching),</li> <li>$v_s$ is the standard FM target velocity (e.g. for linear interpolation, $v_s = x_1 - x_0$).</li> </ul> <p>Then the practical training objective is just expectation over sampled time pairs: \(\mathcal{L}_{\mathrm{TVM}}(\theta) = \mathbb{E}_{t,s}\left[\mathcal{L}_{\mathrm{TVM}}^{t,s}(\theta)\right].\)</p> <h4 id="335-ema--stop-gradient-version-important-in-practice">3.3.5 EMA + stop-gradient version (important in practice)</h4> <p>Like consistency/distillation-style methods, TVM stabilizes training using:</p> <ul> <li><strong>EMA target network</strong></li> <li><strong>stop-gradient</strong> on proxy paths</li> </ul> <p>The practical version uses:</p> <ul> <li>stop-grad copy for displacement branch,</li> <li>stop-grad EMA copy for terminal velocity target.</li> </ul> <p>Conceptually: \(u_\theta\big(x_t + f_\theta(x_t,t,s),s\big) \;\to\; u_{\theta_{\text{EMA}}}^{\text{sg}} \Big(x_t + f_{\theta}^{\text{sg}}(x_t,t,s), s\Big).\)</p> <p>This avoids collapse / target chasing and makes the proxy supervision much more stable.</p> <h4 id="336-why-tvm-is-strong-theoretically-the-important-claim">3.3.6 Why TVM is strong theoretically (the important claim)</h4> <p>TVM proves a <strong>distribution-level guarantee</strong>:</p> <p>Under a Lipschitz assumption on $u_\theta(\cdot,s)$, a weighted time integral of the TVM objective upper-bounds the squared Wasserstein-2 distance between:</p> <ul> <li>the model pushforward distribution (via the learned map), and</li> <li>the true data distribution.</li> </ul> <p>In spirit: $$ W_2^2(\text{model pushforward}, p_0) \;\lesssim\; \int_0^t \lambda(s)\,\mathcal{L}_{\mathrm{TVM}}^{t,s}(\theta)\,ds</p> <ul> <li>C. $$</li> </ul> <p>This is a big deal because many one/few-step distillation methods work well empirically but do <strong>not</strong> cleanly tie their loss to a distribution divergence.</p> <h4 id="337-classifier-free-guidance-cfg-version">3.3.7 Classifier-Free Guidance (CFG) version</h4> <p>TVM extends naturally to conditional generation with CFG.</p> <p>They define a CFG-conditioned displacement map \(f_\theta(x_t,t,s,c,w),\) where:</p> <ul> <li>$c$ = class condition</li> <li>$w$ = guidance scale</li> </ul> <p>The practical CFG objective adds two important ideas:</p> <ol> <li><strong>Condition on $w$ directly</strong> <ul> <li>The network sees the guidance scale at training time.</li> <li>This lets one model support multiple CFG scales.</li> </ul> </li> <li><strong>Use a $1/w^2$ weighting</strong> <ul> <li>Because target velocity magnitude scales roughly linearly with $w$,</li> <li>the loss can explode for large $w$ without correction.</li> </ul> </li> </ol> <p>So the conditional TVM loss is roughly: \(\frac{1}{w^2} \left\| \frac{d}{ds}f_\theta(\cdot,c,w) - u_{\theta_{\text{EMA}}}^{\text{sg}}(\cdot,c,w) \right\|_2^2 + \mathcal{L}_{\mathrm{FM}}^{\text{CFG}}.\)</p> <p>This is one of the most practical contributions in the paper because it supports <strong>stable training under varying CFG</strong>.</p> <h4 id="338-sampling-algorithm">3.3.8 Sampling algorithm</h4> <p>Once trained, sampling is dead simple and supports both 1-step and few-step generation <strong>without retraining</strong>.</p> <p>For a sequence of times \(1=t_0 &gt; t_1 &gt; \cdots &gt; t_n=0,\) iterate: \(x_{t_{k+1}} = x_{t_k} + f_\theta(x_{t_k}, t_k, t_{k+1}) = x_{t_k} + (t_{k+1}-t_k)F_\theta(x_{t_k}, t_k, t_{k+1}).\)</p> <p>So:</p> <ul> <li><strong>1-NFE</strong>: one direct jump $t=1 \to 0$</li> <li><strong>few-NFE</strong>: chain multiple learned jumps</li> <li>no ODE solver needed (the network itself is the integrator)</li> </ul> <p>This is exactly why flow-map-style methods are so appealing for distillation and fast sampling.</p> <h4 id="339-jvp-term-and-implementation-detail-important-for-algorithm-understanding">3.3.9 JVP term and implementation detail (important for “algorithm” understanding)</h4> <p>Because \(f_\theta(x_t,t,s)=(s-t)F_\theta(x_t,t,s),\) the terminal derivative expands as \(\frac{d}{ds}f_\theta(x_t,t,s) = F_\theta(x_t,t,s) + (s-t)\,\partial_s F_\theta(x_t,t,s).\)</p> <p>That second term is a <strong>Jacobian-vector product (JVP)</strong> through the network.</p> <p>TVM’s practical novelty is not just using JVP, but supporting:</p> <ul> <li><strong>JVP through FlashAttention</strong></li> <li><strong>backprop through the JVP result</strong></li> </ul> <p>That matters a lot for DiT-scale transformers, because naive PyTorch attention + JVP is too slow / memory-heavy.</p> <h4 id="3310-practical-tricks-that-actually-matter-from-the-paper">3.3.10 Practical tricks that actually matter (from the paper)</h4> <p>TVM adds several engineering choices that are unusually important:</p> <ol> <li><strong>Semi-Lipschitz control in DiT</strong> <ul> <li>The theory needs Lipschitzness-ish behavior.</li> <li>Vanilla DiT components (LayerNorm/SDPA) are problematic.</li> <li>They replace key normalizations with RMSNorm-style variants and normalize AdaLN modulation terms.</li> </ul> </li> <li><strong>AdamW $\beta_2 = 0.95$</strong> <ul> <li>Higher-order gradients from JVP make training noisy.</li> <li>Lower $\beta_2$ stabilizes second-moment tracking.</li> </ul> </li> <li><strong>Scaled parameterization for CFG</strong> <ul> <li>Make the model output scale with $w$ by construction: \(f_\theta(x_t,t,s,c,w)=(s-t)\,w\,F_\theta(x_t,t,s,c,w).\)</li> <li>This improves optimization under large guidance.</li> </ul> </li> <li><strong>Time sampling matters</strong> <ul> <li>They ablate multiple $(t,s)$ sampling schemes.</li> <li>Biasing toward larger jumps (larger $t$, smaller $s$) helps, but too aggressive hurts.</li> <li>They also find using a separate time distribution for the FM anchor term can help.</li> </ul> </li> </ol> <h4 id="3311-tvm-vs-meanflow-the-clean-comparison">3.3.11 TVM vs MeanFlow (the clean comparison)</h4> <p><strong>MeanFlow</strong></p> <ul> <li>matches a derivative condition w.r.t. <strong>start time</strong> $t$</li> <li>propagates $u(x_t,t)$ through the JVP path</li> <li>more sensitive to random CFG because the velocity magnitude directly enters the JVP branch</li> </ul> <p><strong>TVM</strong></p> <ul> <li>matches a derivative condition w.r.t. <strong>terminal time</strong> $s$</li> <li>JVP is w.r.t. $s$ (cleaner and more stable under random CFG)</li> <li>has a clearer Wasserstein-style distribution guarantee</li> <li>naturally supports one-step and few-step jumps with the same network</li> </ul> <p>This is why TVM is a strong “algorithmic” evolution of flow-map distillation rather than just another loss tweak.</p> <hr/> <h2 id="34-transition-matching-distillation">3.4 Transition Matching Distillation</h2> <p>Transition Matching Distillation (TMD) bridges engineering and theory based adaptation of MeanFlow to <strong>video distillation</strong>.</p> <h4 id="341-core-problem-setup">3.4.1 Core problem setup</h4> <p>They want to distill a pretrained video diffusion teacher into a faster student. Direct one-stage distillation is hard in video because:</p> <ul> <li>the space is huge,</li> <li>temporal consistency matters,</li> <li>transformer-based video models make JVP painful (esp. attention kernels/FSDP/context parallelism).</li> </ul> <p>So TMD uses <strong>two stages</strong>.</p> <h4 id="342-stage-1-transition-matching-meanflow-tm-mf">3.4.2 Stage 1: Transition Matching MeanFlow (TM-MF)</h4> <p>This is the key new idea.</p> <p>Instead of applying MeanFlow directly in the original latent/data space, they define an <strong>inner transition</strong> problem and parameterize a conditional inner flow map via average velocity:</p> \[f_\theta(y_s,s,r;m) = y_s + (s-r)u_\theta(y_s,s,r;m)\] <p>where $m$ is a feature extracted from the main backbone.</p> <p>Then they use a MeanFlow-style objective to train this transition head.</p> <p>A very practical (and nontrivial) design choice:</p> <ul> <li>they <strong>reparameterize</strong> the average velocity to stay aligned with the teacher head:</li> </ul> \[u_\theta(y_s,s,r;m) = y_1 - \text{head}_\theta(y_s,s,r;m)\] <p>This is not cosmetic. It keeps the new head close to teacher semantics, which improves stability.</p> <h4 id="343-jvp-issue-and-finite-difference-approximation">3.4.3 JVP issue and finite-difference approximation</h4> <p>This paper is very realistic about systems constraints:</p> <ul> <li>exact JVP is annoying with large-scale video transformer stacks (FlashAttention, FSDP, context parallelism),</li> <li>so they use a <strong>finite-difference approximation</strong> of the JVP.</li> </ul> <p>That’s a practical compromise:</p> <ul> <li>theoretically less clean than exact JVP,</li> <li>but massively easier to integrate into production-grade training code.</li> </ul> <h4 id="344-stage-2-distributional-distillation-objective">3.4.4 Stage 2: Distributional distillation objective</h4> <p>After TM-MF pretraining, they switch to a stronger distillation stage using a VSD/discriminator-style objective (their simplified algorithm shows):</p> \[\mathcal{L} = \text{VSD}(\hat{x}) + \lambda \cdot \text{Discriminator}(\hat{x})\] <p>So the conceptual split is:</p> <ul> <li><strong>Stage 1 (TM-MF):</strong> learn a good transition-aware student parameterization, bootstrap geometry/dynamics</li> <li><strong>Stage 2:</strong> sharpen sample quality and distribution match</li> </ul> <p>This is a strong template for hard domains (video, 3D, multimodal) where pure one-shot distillation is brittle.</p> <h4 id="345-why-tmd-is-strong">3.4.5 Why TMD is strong</h4> <p>TMD wins because it combines both worlds:</p> <ul> <li><strong>trajectory-based structure</strong> (TM-MF / flow-map adaptation)</li> <li><strong>distribution-based distillation</strong> (DMD2-v)</li> </ul> <p>And it does so with an architecture that respects the hierarchical structure of big video diffusion Transformers.</p> <p>That’s why it gets a better speed-quality tradeoff than plain one-step/few-step distillation baselines in video.</p>]]></content><author><name></name></author><category term="research-survey"/><category term="diffusion"/><category term="survey"/><summary type="html"><![CDATA[Recent developments of flow map distillation techniques]]></summary></entry><entry><title type="html">Modern Diffusion Distillation Overview</title><link href="https://abecid.github.io/blog/2026/diffusion/" rel="alternate" type="text/html" title="Modern Diffusion Distillation Overview"/><published>2026-02-24T03:24:00+00:00</published><updated>2026-02-24T03:24:00+00:00</updated><id>https://abecid.github.io/blog/2026/diffusion</id><content type="html" xml:base="https://abecid.github.io/blog/2026/diffusion/"><![CDATA[<h1 id="flow-matching-to-flow-maps-to-distillation-a-deep-dive">Flow Matching to Flow Maps to Distillation: A Deep Dive</h1> <h1 id="table-of-contents">Table of Contents</h1> <ol> <li><a href="#1-foundations">Foundations</a> <ol> <li><a href="#11-diffusion">Diffusion</a></li> <li><a href="#12-flow-matching">Flow Matching</a></li> <li><a href="#13-rectified-flow">Rectified Flow</a></li> <li><a href="#14-flow-map">Flow Map</a></li> <li><a href="#15-consistency-models">Consistency Models</a></li> </ol> </li> <li><a href="#2-meanflow-family">MeanFlow Family</a> <ol> <li><a href="#21-meanflow">MeanFlow</a></li> <li><a href="#22-improved-meanflow-imf">Improved MeanFlow (iMF)</a></li> <li><a href="#23-alphaflow">AlphaFlow</a></li> <li><a href="#24-accelerating-and-improving-meanflow">Accelerating and Improving MeanFlow</a></li> <li><a href="#25-decoupled-meanflow">Decoupled MeanFlow</a></li> </ol> </li> <li><a href="#3-flowmap">FlowMap</a> <ol> <li><a href="#31-free-flowmap">Free FlowMap</a></li> <li><a href="#32-meta-flow-maps">Meta Flow Maps</a></li> <li><a href="#33-terminal-velocity-matching-tvm">Terminal Velocity Matching (TVM)</a></li> </ol> </li> <li><a href="#4-score-distillation">Score Distillation</a> <ol> <li><a href="#41-variational-score-distillation-vsd">Variational Score Distillation (VSD)</a></li> <li><a href="#42-dmd-distribution-matching-distillation">Distribution Matching Distillation (DMD)</a></li> <li><a href="#43-score-identity-distillation-sid">Score Identity Distillation (SiD)</a></li> </ol> </li> <li><a href="#5-adversarial-distillation">Adversarial Distillation</a> <ol> <li><a href="#51-adversarial-diffusion-distillation-add">Adversarial Diffusion Distillation (ADD)</a></li> <li><a href="#52-ladd-latent-adversarial-diffusion-distillation">LADD</a></li> <li><a href="#53-diffratio">DiffRatio</a></li> <li><a href="#54-apt-and-the-modern-aapt-extension">APT (and AAPT)</a></li> <li><a href="#55-comparisons">Comparisons</a></li> </ol> </li> <li><a href="#6-video-generation">Video Generation</a> <ol> <li><a href="#61-causvid">CausVid</a></li> <li><a href="#62-self-forcing">Self-Forcing</a></li> <li><a href="#63-transition-matching-distillation">Transition Matching Distillation (TMD)</a></li> </ol> </li> <li><a href="#7-new-domains">New Domains</a> <ol> <li><a href="#71-jit-just-image-transformers">JiT</a></li> <li><a href="#72-drifting">Drifting</a></li> <li><a href="#73-pixel-meanflow-pmf">Pixel MeanFlow (pMF)</a></li> <li><a href="#74-repa-representation-alignment-for-generation">REPA</a></li> <li><a href="#75-latent-forcing">Latent Forcing</a></li> <li><a href="#76-unified-latents-ul">Unified Latents (UL)</a></li> <li><a href="#77-unifying-pattern-across-these-new-domain-methods">Unifying Pattern</a></li> </ol> </li> <li><a href="#8-manifold">Manifold</a> <ol> <li><a href="#81-riemannian-manifold">Riemannian Manifold</a></li> <li><a href="#82-optimal-transport">Optimal Transport</a></li> </ol> </li> </ol> <h1 id="overview">Overview</h1> <p>Recent generative modeling utilize and develop upon flow maps and jvp based distillation techniques to reduce the number of function evaluations during inference. We focus on the Meanflow family, score distillation methods, and its applications in video generation.</p> <hr/> <h1 id="1-foundations">1. Foundations</h1> <h2 id="11-diffusion">1.1 Diffusion</h2> <p>Diffusion models define a <strong>forward noising process</strong> that gradually corrupts data into noise, and a <strong>reverse process</strong> that learns to reconstruct data from noise. The main reason diffusion models became dominant is that they are stable and high-quality, but the tradeoff is <strong>slow iterative sampling</strong>.</p> <h3 id="111-forward-process-discrete-ddpm-view">1.1.1 Forward process (discrete DDPM view)</h3> <p>In DDPM, the forward process is a Markov chain:</p> \[q(x_t \mid x_{t-1}) = \mathcal{N}\!\left(\sqrt{1-\beta_t}\,x_{t-1}, \beta_t I\right),\] <p>where $\beta_t \in (0,1)$ is a variance schedule.</p> <p>A key closed form is:</p> \[q(x_t \mid x_0) = \mathcal{N}\!\left(\sqrt{\bar\alpha_t}\,x_0,\,(1-\bar\alpha_t)I\right),\] <p>with</p> \[\alpha_t = 1-\beta_t,\qquad \bar\alpha_t=\prod_{s=1}^t \alpha_s.\] <p>So we can sample $x_t$ directly as:</p> \[x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon,\qquad \epsilon \sim \mathcal{N}(0,I).\] <p>This is the standard “data + Gaussian noise” interpolation used in many diffusion derivations.</p> <h3 id="112-reverse-process-and-denoising-objective">1.1.2 Reverse process and denoising objective</h3> <p>The generative model learns the reverse transitions</p> \[p_\theta(x_{t-1}\mid x_t),\] <p>which are parameterized via a neural network (predicting noise, $x_0$, or velocity depending on parameterization).</p> <p>The most common training objective is noise prediction:</p> \[\mathcal{L}_{\text{simple}}(\theta) = \mathbb{E}_{t,x_0,\epsilon} \left[ \left\|\epsilon - \epsilon_\theta(x_t,t)\right\|^2 \right].\] <p>This objective is simple and works extremely well, but inference still requires many reverse denoising steps.</p> <h3 id="113-continuous-time-diffusion-sde-view">1.1.3 Continuous-time diffusion (SDE view)</h3> <p>A continuous-time diffusion can be written as an SDE:</p> \[dx = f(x,t)\,dt + g(t)\,dW_t,\] <p>where $f$ is drift, $g$ is diffusion scale, and $W_t$ is a Wiener process.</p> <p>The reverse-time generative dynamics also form an SDE involving the score:</p> \[\nabla_x \log p_t(x).\] <p>This is the bridge to <strong>score-based generative modeling</strong> and continuous-time transport formulations.</p> <h3 id="114-probability-flow-ode-deterministic-counterpart">1.1.4 Probability flow ODE (deterministic counterpart)</h3> <p>Every diffusion SDE has an associated deterministic <strong>probability flow ODE</strong> that shares the same marginals $p_t$:</p> \[\frac{dx}{dt} = f(x,t) - \frac{1}{2}g(t)^2 \nabla_x \log p_t(x).\] <p>This is huge conceptually because it turns diffusion sampling into solving an ODE, which directly connects to:</p> <ul> <li>continuous normalizing flows,</li> <li>flow matching,</li> <li>rectified flow,</li> <li>and later flow-map distillation methods.</li> </ul> <p>So diffusion is not just “denoising noise,” it is also a <strong>continuous transport process</strong> in disguise.</p> <h3 id="115-why-diffusion-motivates-distillation">1.1.5 Why diffusion motivates distillation</h3> <p>Diffusion teachers are strong but slow because generation requires many function evaluations (NFEs). Distillation methods aim to compress this long trajectory into:</p> <ul> <li><strong>few-step samplers</strong> (e.g. 2–8 steps),</li> <li>or even <strong>one-step generators</strong>,</li> </ul> <p>while preserving the teacher’s learned transport geometry.</p> <p>This is exactly why Chapter 1 naturally progresses from <strong>Diffusion $\to$ Flow Matching $\to$ Flow Maps / Consistency / Distillation</strong>.</p> <h2 id="12-flow-matching">1.2 Flow Matching</h2> <p><img src="/assets/img/blogs/1_distillation/flowmatching.png" alt="Flow Matching"/></p> <p><em>Figure 1. Flow Matching. from Sabour, Fidler, and Kreis (2025),</em> Align Your Flow: Scaling Continuous-Time Flow Map Distillation <em>(arXiv:2506.14603).</em></p> <p>Flow Matching (FM) reframes generative modeling as directly learning a <strong>time-dependent velocity field</strong> that transports a simple source distribution (usually Gaussian noise) to the data distribution.</p> <h3 id="121-core-idea-learn-the-instantaneous-velocity">1.2.1 Core idea: learn the instantaneous velocity</h3> <p>Instead of learning reverse denoising conditionals, FM learns a vector field</p> \[u_\theta(x,t)\] <p>such that samples evolve via the ODE</p> \[\frac{dx}{dt} = u_\theta(x,t),\qquad t\in[0,1].\] <p>If this ODE is integrated from source noise at $t=0$ to $t=1$, the final samples should follow the data distribution.</p> <h3 id="122-conditional-path-and-target-velocity">1.2.2 Conditional path and target velocity</h3> <p>FM is usually trained by defining a conditional interpolation path between paired endpoints $(x_0,x_1)$:</p> \[x_t = \psi_t(x_0,x_1).\] <p>For simple linear interpolation:</p> \[x_t = (1-t)x_0 + tx_1.\] <p>The target velocity along this path is:</p> \[\dot{x}_t = \frac{d}{dt}\psi_t(x_0,x_1).\] <p>For the linear path, this becomes:</p> \[\dot{x}_t = x_1 - x_0.\] <p>The model is trained to match this conditional velocity in expectation:</p> \[\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t,x_0,x_1} \left[ \left\|u_\theta(x_t,t)-\dot{x}_t\right\|^2 \right].\] <p>So FM is “supervised vector field learning” on a chosen path family.</p> <h3 id="123-why-fm-is-attractive">1.2.3 Why FM is attractive</h3> <p>Compared to score/diffusion training, FM gives a very clean ODE-learning objective and avoids explicit score estimation. It is especially natural when you want to reason about:</p> <ul> <li>transport geometry,</li> <li>ODE trajectories,</li> <li>and later finite-time maps (flow maps).</li> </ul> <h3 id="124-limitation-local-field-expensive-sampling">1.2.4 Limitation: local field, expensive sampling</h3> <p>FM learns a <strong>local tangent</strong> $u_\theta(x,t)$, not a finite-time jump. That means sampling still requires numerical integration:</p> \[x_{t+\Delta t} \approx x_t + \Delta t\,u_\theta(x_t,t)\] <p>(or higher-order solvers like Heun).</p> <p>If trajectories are curved, discretization error accumulates, so many NFEs are needed. This is the main bottleneck that motivates:</p> <ul> <li>Rectified Flow (straighter trajectories),</li> <li>Flow Maps (direct time-to-time transport),</li> <li>and distillation methods (few-step or one-step generation).</li> </ul> <h2 id="13-rectified-flow">1.3 Rectified Flow</h2> <p>Rectified Flow (RF) keeps the ODE/transport framing of FM, but explicitly pushes the learned trajectories to become <strong>straighter</strong>, which makes them much easier to sample with few steps.</p> <h3 id="131-motivation-straight-trajectories-are-cheap">1.3.1 Motivation: straight trajectories are cheap</h3> <p>If a trajectory is highly curved, Euler updates need many small steps. If a trajectory is nearly straight, even a coarse solver can track it accurately.</p> <p>So RF is basically a geometry-aware fix to the FM sampling bottleneck.</p> <h3 id="132-linear-interpolation-path-and-velocity-target">1.3.2 Linear interpolation path and velocity target</h3> <p>A standard RF path is the same linear interpolation:</p> \[x_t = (1-t)x_0 + tx_1,\] <p>with instantaneous derivative</p> \[\frac{dx_t}{dt} = x_1 - x_0.\] <p>RF trains a velocity field to match this transport direction along the path:</p> \[v_\theta(x_t,t) \approx x_1 - x_0.\] <p>A common training objective is:</p> \[\mathcal{L}_{\text{RF}}(\theta) = \mathbb{E}_{t,x_0,x_1} \left[ \left\|v_\theta(x_t,t)-(x_1-x_0)\right\|^2 \right].\] <p>This looks similar to FM, but the interpretation is sharper: RF cares about learning a transport field whose induced trajectories are easy to discretize.</p> <h3 id="133-reflow-iterative-rectification">1.3.3 Reflow (iterative rectification)</h3> <p>A major practical idea in RF is <strong>reflow</strong>:</p> <ol> <li>Train an initial transport field.</li> <li>Sample trajectories from the model.</li> <li>Use those trajectories (or endpoint couplings) to retrain a straighter field.</li> <li>Repeat.</li> </ol> <p>Each round reduces curvature and improves few-step generation. This is why RF is often viewed as a bridge between classical diffusion/FM and modern one-step distillation.</p> <h3 id="134-why-rf-matters-for-the-rest-of-this-blog">1.3.4 Why RF matters for the rest of this blog</h3> <p>RF is the clean conceptual bridge to flow-map methods because it shifts the focus from “match local field” to “shape trajectories for fast transport.” Once you think this way, the next obvious step is:</p> <blockquote> <p>Why learn only the local tangent at all?<br/> Why not learn the <strong>finite-time map</strong> directly?</p> </blockquote> <p>That is exactly the flow-map perspective.</p> <h2 id="14-flow-map">1.4 Flow Map</h2> <p><img src="/assets/img/blogs/1_distillation/flowmap.png" alt="Flow Map"/></p> <p><em>Figure 2. Flow Map. from Sabour, Fidler, and Kreis (2025),</em> Align Your Flow: Scaling Continuous-Time Flow Map Distillation <em>(arXiv:2506.14603).</em></p> <p>Flow-map methods move beyond local vector fields and directly learn a <strong>time-to-time transport operator</strong>.</p> <h3 id="141-from-vector-field-to-finite-time-map">1.4.1 From vector field to finite-time map</h3> <p>Given a velocity field $u(x,t)$ and its ODE</p> \[\frac{dx}{dt}=u(x,t),\] <p>the associated flow map $\phi_u$ sends a state from time $t$ to time $s$:</p> \[\phi_u(x_t,t,s)=x_s.\] <p>So instead of learning only the local tangent $u(x,t)$, we learn the finite-time update:</p> \[x_s \approx \phi_\theta(x_t,t,s).\] <p>This is much more aligned with few-step sampling, because a single model evaluation can move across a large time interval.</p> <h3 id="142-why-flow-maps-help-distillation">1.4.2 Why flow maps help distillation</h3> <p>A flow field gives infinitesimal updates; a flow map gives finite jumps.</p> <p>That means flow maps are naturally suited for:</p> <ul> <li><strong>few-step sampling</strong> (large $t\to s$ jumps),</li> <li><strong>teacher-student distillation</strong> (student imitates teacher transitions),</li> <li><strong>self-distillation</strong> (model supervises its own multistep consistency),</li> <li>and <strong>data-free distillation</strong> variants (matching dynamics without original data).</li> </ul> <p>This is the key conceptual move behind MeanFlow-family and FreeFlowMap-family methods.</p> <h3 id="143-semigroup--composition-structure">1.4.3 Semigroup / composition structure</h3> <p>Exact flow maps satisfy a composition rule (semigroup property):</p> \[\phi(x_t,t,r) = \phi\!\left(\phi(x_t,t,s),\,s,\,r\right) \qquad \text{for } t \le s \le r.\] <p>This property is incredibly important because it gives a built-in consistency constraint across time triples. Many modern distillation methods exploit some version of this:</p> <ul> <li>explicit composition matching,</li> <li>consistency losses,</li> <li>JVP-based local constraints that imply finite-time consistency.</li> </ul> <h3 id="144-relation-to-fm-and-rf">1.4.4 Relation to FM and RF</h3> <ul> <li><strong>FM</strong> learns $u(x,t)$ (local instantaneous velocity)</li> <li><strong>RF</strong> improves trajectory geometry (straighter ODE paths)</li> <li><strong>Flow Map</strong> learns $\phi(x_t,t,s)$ (finite-time transport)</li> </ul> <p>So flow maps are not a totally different universe; they are the natural next abstraction after FM/RF if your goal is <strong>fast generation</strong>.</p> <h2 id="15-consistency-models">1.5 Consistency Models</h2> <p>Consistency Models (CMs) attack the same bottleneck from another angle: instead of learning a vector field or even an explicit flow map, they learn a <strong>cross-time consistent predictor</strong> that maps noisy states to a shared target representation (often an estimate of clean data).</p> <p>This makes them one of the foundational one-step / few-step distillation paradigms.</p> <h3 id="151-core-consistency-idea">1.5.1 Core consistency idea</h3> <p>Suppose $x_t$ and $x_s$ lie on the same teacher trajectory (or same underlying denoising path). A consistency model $f_\theta$ is trained so that: \(f_\theta(x_t,t) \approx f_\theta(x_s,s),\) after the appropriate scaling/parameterization.</p> <p>In words: different noise levels along the same trajectory should produce the same final prediction.</p> <p>This is a <strong>cross-time agreement constraint</strong>, not just a local derivative-matching objective.</p> <h3 id="152-why-this-enables-one-step-generation">1.5.2 Why this enables one-step generation</h3> <p>Because the model is trained to collapse trajectory points to a common target, we can often sample by evaluating the model once (or very few times) from a noisy input.</p> <p>This directly targets inference speed, unlike standard diffusion training which optimizes denoising accuracy at every step but does not inherently optimize for low-NFE sampling.</p> <h3 id="153-teacher-student-consistency-distillation">1.5.3 Teacher-student consistency distillation</h3> <p>A common setup is:</p> <ol> <li>Start with a strong diffusion/score teacher.</li> <li>Generate paired states $(x_t, x_s)$ on teacher trajectories.</li> <li>Train the student consistency model to agree across those states.</li> </ol> <p>This makes consistency models a very important predecessor to later:</p> <ul> <li>flow-map distillation,</li> <li>self-distillation,</li> <li>and JVP-based transport-map objectives.</li> </ul> <h3 id="154-conceptual-relation-to-flow-maps">1.5.4 Conceptual relation to flow maps</h3> <p>Consistency models and flow-map methods are closely related in spirit:</p> <ul> <li><strong>Flow map view:</strong> learn explicit transport $\phi(x_t,t,s)$</li> <li><strong>Consistency view:</strong> learn a representation/prediction that is invariant (or aligned) across times on the same trajectory</li> </ul> <p>Both replace purely local supervision with <strong>cross-time structure</strong>, which is exactly what you need for few-step and one-step generation.</p> <hr/> <h1 id="2-meanflow-family">2. MeanFlow Family</h1> <h2 id="21-meanflow">2.1 MeanFlow</h2> <h4 id="211-average-velocity-instead-of-instantaneous-velocity">2.1.1 Average velocity instead of instantaneous velocity</h4> <p><img src="/assets/img/blogs/1_distillation/meanflow.png" alt="Mean Flow"/></p> <p><em>Figure 3. Mean Flow, with different target timestep $t$. from Geng et al. (2025),</em> Mean Flows for One-step Generative Modeling <em>(arXiv:2505.13447).</em></p> <p>MeanFlow defines an <strong>average velocity</strong></p> \[u(z_t, r, t)\] <p>over the interval $[r,t]$, so the displacement is:</p> \[(t-r)u(z_t,r,t).\] <h4 id="212-the-meanflow-identity-the-central-derivation">2.1.2 The MeanFlow Identity (the central derivation)</h4> <p>Start from the definition:</p> \[(t-r)u(z_t,r,t) = \int_r^t v(z_\tau,\tau)\, d\tau\] <p>Differentiate both sides with respect to (t) (holding (r) fixed). By product rule + FTC:</p> \[u(z_t,r,t) + (t-r)\frac{d}{dt}u(z_t,r,t) = v(z_t,t)\] <p>Rearranging leads to the <strong>MeanFlow Identity</strong>:</p> \[u(z_t,r,t) = v(z_t,t) - (t-r)\frac{d}{dt}u(z_t,r,t)\] <h4 id="213-main-points">2.1.3 Main Points</h4> <ul> <li>The authors rewrite an intractable target (the average velocity integral) into a trainable target: <ul> <li>By first taking the derivative of both sides.</li> <li>Instantaneous velocity $v$ is available from FM-style interpolation.</li> <li>Total derivative term computed via <strong>JVP</strong>.</li> </ul> </li> <li>No explicit consistency regularizer is imposed.</li> <li>The consistency-like structure falls out from the definition of average velocity.</li> </ul> <h4 id="214-meanflow-loss">2.1.4 MeanFlow loss</h4> <p>Parameterize $u_\theta(z_t,r,t)$, and regress to the identity-induced target:</p> \[\mathcal{L}_{\text{MeanFlow}}(\theta) = \mathbb{E}\left[ \left\| u_\theta(z_t,r,t) - \operatorname{sg}(u_{\text{tgt}}) \right\|_2^2 \right]\] <p>with</p> \[u_{\text{tgt}} = v_t - (t-r)\left(v_t \,\partial_z u_\theta + \partial_t u_\theta\right)\] <p>where the total derivative is implemented through a JVP along tangent $(v_t, 0, 1)$.</p> <h4 id="215-sampling">2.1.5 Sampling</h4> <p>Once you learn the average velocity, sampling is:</p> \[z_r = z_t - (t-r)u(z_t,r,t)\] <p>For 1-step:</p> \[z_0 = z_1 - u(z_1,0,1),\quad z_1 \sim p_{\text{prior}}\] <hr/> <h2 id="22-improved-meanflow-imf">2.2 Improved MeanFlow (iMF)</h2> <p>iMF addresses two practical issues in MeanFlow:</p> <ol> <li>the self-referential target,</li> <li>fixed-CFG training (bad for inference-time flexibility).</li> </ol> <h4 id="221-meanflow-as-a-v-loss">2.2.1 MeanFlow as a v-loss</h4> <p>iMF rewrites the MeanFlow identity into a <strong>velocity regression form</strong>:</p> \[v(z_t) = u(z_t,r,t) + (t-r)\frac{d}{dt}u(z_t,r,t).\] <p>Then parameterize the RHS with $u_\theta$:</p> \[V_\theta = u_\theta(z_t,r,t) + (t-r)\,\mathrm{JVP}_{\mathrm{sg}}(u_\theta; v_\theta),\] <p>and train with a Flow-Matching-like loss</p> \[\mathcal{L}_{\mathrm{iMF}} = \mathbb{E}\left[\left\|V_\theta - (\epsilon-x)\right\|_2^2\right].\] <p>This is cleaner because the regression target is now the standard FM target $(\epsilon-x)$ rather than an apparent target constructed from $u_\theta$.</p> <h3 id="222-flexible-cfg-as-conditioning">2.2.2 Flexible CFG as conditioning</h3> <p>Original MeanFlow supports CFG in 1-NFE, but with a <strong>fixed guidance scale</strong> chosen at training time.</p> <p>iMF fixes this by making the guidance scale part of the conditioning:</p> <ul> <li>guidance scale $\omega$ becomes an input condition,</li> <li>the same model can be sampled with different CFG scales at inference.</li> </ul> <p>That is a big deal because the optimal CFG scale shifts with model size, training progress, and NFE.</p> <h3 id="223-in-context-conditioning">2.2.3 In-context conditioning</h3> <p>iMF also upgrades conditioning architecture:</p> <ul> <li>conditions include $(r,t)$, class label $c$, and guidance-related variables $\Omega$,</li> <li>each condition is represented with multiple learnable tokens,</li> <li>all condition tokens are concatenated with image latent tokens and processed by the Transformer.</li> </ul> <p>This allows:</p> <ul> <li>support richer heterogeneous conditioning more naturally,</li> <li>remove <strong>adaLN-zero</strong>,</li> <li>cut params significantly (they report about <strong>1/3 reduction</strong> in a base model setting).</li> </ul> <h2 id="23-alphaflow">2.3 AlphaFlow</h2> <p>AlphaFlow is the paper that gives the most useful conceptual interpretation of MeanFlow training.</p> <h4 id="231-core-insight-meanflow-decomposes-into-two-losses">2.3.1 Core insight: MeanFlow decomposes into two losses</h4> <p>AlphaFlow shows the MeanFlow objective can be algebraically decomposed into:</p> <ol> <li><strong>Trajectory Flow Matching (TFM)</strong></li> <li><strong>Trajectory Consistency (TC)</strong></li> </ol> <p>The decomposition (up to a constant) is:</p> \[\mathcal{L}_{\mathrm{MF}} = \underbrace{\mathbb{E}\left[\|u_\theta(z_t,r,t)-v_t\|_2^2\right]}_{\mathcal{L}_{\mathrm{TFM}}} + \underbrace{\mathbb{E}\left[2(t-r)\,u_\theta^\top \frac{d u_\theta^-}{dt}\right]}_{\mathcal{L}_{\mathrm{TC}}} + C.\] <p>Interpretation:</p> <ul> <li><strong>TFM</strong> says “fit the trajectory-local velocity target.”</li> <li><strong>TC</strong> says “be self-consistent along the trajectory.”</li> <li>MeanFlow is effectively a <strong>consistency-like model with extra trajectory FM supervision</strong>.</li> </ul> <h4 id="232-why-meanflow-often-needs-lots-of-border-case-fm-samples">2.3.2 Why MeanFlow often needs lots of border-case FM samples</h4> <p>AlphaFlow also explains a weird empirical fact from MeanFlow:</p> <ul> <li>MeanFlow works best when many samples use the border case $r=t$ (which looks like vanilla FM).</li> </ul> <p>Their analysis shows this is not just a hack:</p> <ul> <li>the gradients of TFM and trajectory consistency are often <strong>negatively correlated</strong>,</li> <li>so the extra FM-style supervision helps stabilize and speed up training.</li> </ul> <h4 id="233-α-flow-loss-one-objective-that-unifies-tfm-shortcut-meanflow">2.3.3 α-Flow loss: one objective that unifies TFM, Shortcut, MeanFlow</h4> <p>AlphaFlow defines a family of losses parameterized by $\alpha$:</p> \[\mathcal{L}_\alpha(\theta) = \mathbb{E}_{t,r,z_t} \left[ \alpha^{-1} \left\| u_\theta(z_t,r,t) - \left(\alpha \,\tilde v_{s,t} + (1-\alpha)\,u_{\theta^-}(z_s,r,s)\right) \right\|_2^2 \right],\] <p>where</p> \[s = \alpha r + (1-\alpha)t\] <p>is an intermediate time.</p> <p>This unifies several training objectives:</p> <ul> <li>$\alpha=1$ gives <strong>trajectory flow matching</strong> (with suitable $\tilde v_{s,t}$),</li> <li>$\alpha=\tfrac{1}{2}$ recovers a <strong>Shortcut-style</strong> objective,</li> <li>$\alpha \to 0$ recovers the <strong>MeanFlow gradient</strong>.</li> </ul> <p>That is the key conceptual win: AlphaFlow puts FM, Shortcut, and MeanFlow on one continuum.</p> <h4 id="234-curriculum">2.3.4 Curriculum</h4> <p>Because TFM and TC conflict early in training, AlphaFlow uses a curriculum:</p> <ul> <li>start more FM-like (larger $\alpha$),</li> <li>gradually anneal toward MeanFlow-like behavior (smaller $\alpha$).</li> </ul> <p>This disentangles optimization and improves convergence.</p> <h4 id="235-takeaways">2.3.5 Takeaways</h4> <p>AlphaFlow is best understood as:</p> <ul> <li>a <strong>theory paper for MeanFlow optimization</strong> (decomposition + gradient conflict),</li> <li>plus a <strong>practical training recipe</strong> (curriculum over $\alpha$) that improves one-step/few-step quality.</li> </ul> <h2 id="24-accelerating-and-improving-meanflow">2.4 Accelerating and Improving MeanFlow</h2> <p>Understanding and Improving Mean Flow’s (UAIMF) main point is simple and very practical: MeanFlow training is bottlenecked by <strong>slow velocity formation</strong> and <strong>bad temporal-gap scheduling</strong>, so they speed up both. They propose two complementary components:</p> <ol> <li>accelerate the velocity-learning part with standard diffusion training tricks (they use <strong>MinSNR</strong> or <strong>DTD</strong>), and</li> <li>add a <strong>progressive weighting</strong> over the MeanFlow loss so the model learns small-gap average velocities first, then gradually expands to larger gaps.</li> </ol> <h3 id="241-why-meanflow-trains-slowly">2.4.1 Why MeanFlow trains slowly</h3> <p>UAIMF analyzes MeanFlow training through two coupled subproblems:</p> <ul> <li>learning instantaneous velocity (the easier FM-like part),</li> <li>learning average velocity over larger timestep gaps (the harder MeanFlow part).</li> </ul> <p>Their empirical claim is that <strong>rapid velocity formation helps MeanFlow converge much faster</strong>, and that the temporal gap matters a lot: large-gap average-velocity learning is harder and should be delayed. This is why they combine velocity acceleration + progressive gap weighting.</p> <h3 id="242-component-1-accelerate-the-velocity-part-minsnr--dtd">2.4.2 Component 1: Accelerate the velocity part (MinSNR / DTD)</h3> <p>UAIMF tests one method from each category:</p> <ul> <li><strong>MinSNR</strong> as a loss-weighting acceleration method</li> <li><strong>DTD</strong> as a timestep-sampling acceleration method</li> </ul> <p>and plugs them into MeanFlow training. They report both help, but emphasize that <strong>DTD is more robust across model scales</strong> because it changes the sampling distribution instead of interfering with MeanFlow’s own adaptive loss normalization.</p> <h3 id="243-component-2-progressive-weighting-on-the-meanflow-loss">2.4.3 Component 2: Progressive weighting on the MeanFlow loss</h3> <p>UAIMF progressively reweights the MeanFlow term so training starts by emphasizing <strong>small temporal gaps</strong> (easy) and gradually transitions to <strong>uniform weighting</strong> (full MeanFlow objective). Their weighting is:</p> \[\beta(\Delta t, s) = 1 - s + \lambda s (1 - \Delta t)\] <p>where:</p> <ul> <li>\(\Delta t\) is the temporal gap,</li> <li>\(s \in [0,1]\) is training progress,</li> <li>\(\lambda\) normalizes the expectation at initialization.</li> </ul> <p>At initialization, the weighting prioritizes small gaps; by the end, it becomes uniform. They use a linear schedule by default:</p> \[s = 1 - \frac{i}{T}\] <p>and also discuss the generalized schedule:</p> \[s = 1 - \left(\frac{i}{T}\right)^k\] <p>with \(k=1\) (linear) working best in their ablations.</p> <h3 id="244-why-the-two-components-work-together">2.4.4 Why the two components work together</h3> <p>Their ablation is clean:</p> <ul> <li>velocity acceleration alone improves MeanFlow,</li> <li>progressive \(L_u\) weighting alone improves MeanFlow,</li> <li>combining both works best.</li> </ul> <p>They explicitly interpret this as:</p> <ul> <li>acceleration methods quickly establish the <strong>instantaneous velocity foundation</strong></li> <li>progressive weighting improves <strong>average velocity learning</strong> over time</li> </ul> <p>which is exactly the right mental model for MeanFlow optimization.</p> <h2 id="25-decoupled-meanflow">2.5 Decoupled MeanFlow</h2> <p>Decoupled MeanFlow is the strongest architectural update in this line. The core idea is: <strong>the encoder should care about the current timestep, and the decoder should care about the target timestep</strong>. They decouple timestep conditioning and turn a pretrained flow model into a flow-map model with almost no architectural surgery.</p> <h3 id="251-core-architectural-idea-decouple-encoder-vs-decoder-conditioning">2.5.1 Core architectural idea: decouple encoder vs decoder conditioning</h3> <p>They reinterpret a flow model as:</p> \[v_\theta = g_\theta \circ f_\theta\] <p>with an encoder \(f_\theta\) and decoder \(g_\theta\). Then they argue the standard MeanFlow design is redundant because it feeds the next timestep \(r\) everywhere. Their fix:</p> <ul> <li>encoder gets current timestep \(t\)</li> <li>decoder gets next timestep \(r\)</li> </ul> <p>and the flow map becomes:</p> \[u_\theta(x_t, t, r) = g_\theta(f_\theta(x_t, t), r)\] <p>This is the defining DMF equation.</p> <h3 id="252-why-this-matters-pretrained-flow-models-already-contain-flow-map-structure">2.5.2 Why this matters: pretrained flow models already contain flow-map structure</h3> <p>DMF shows that a pretrained flow model can be <strong>converted into a flow map without fine-tuning</strong>, just by choosing an encoder/decoder split and decoding the representation with \(r\). They report the converted DMF can even outperform the original flow model in some settings, which supports the claim that good flow-model representations are already enough for flow-map prediction.</p> <p>This is a major conceptual shift: instead of training flow maps from scratch, you can <strong>reuse pretrained flow-model representations</strong> and repurpose the decoder.</p> <h3 id="253-representation-first-view">2.5.3 Representation-first view</h3> <p>DMF explicitly argues that <strong>representation quality matters</strong> for flow maps. They show:</p> <ul> <li>stronger pretrained encoders transfer better to flow-map fine-tuning</li> <li>freezing encoder + tuning decoder already gives a large speed/quality gain</li> <li>but true 1-step performance needs joint optimization (encoder cannot stay frozen forever)</li> </ul> <p>This gives a practical recipe: pretrain a strong flow model first, then convert/fine-tune as DMF.</p> <h3 id="254-training-recipe-fm-warm-up--mf-fine-tuning">2.5.4 Training recipe: FM warm-up + MF fine-tuning</h3> <p>DMF also proposes a better training pipeline:</p> <ol> <li>train a flow model with FM loss</li> <li>convert to DMF</li> <li>fine-tune with MeanFlow loss</li> </ol> <p>They justify this on compute grounds (MF/JVP is expensive) and show it scales better than training a flow map from scratch. This is one of the most important practical contributions in the paper.</p> <h3 id="255-enhanced-training-techniques">2.5.5 Enhanced training techniques</h3> <h4 id="a-adaptive-weighted-cauchy-loss">(a) Adaptive weighted Cauchy loss</h4> <p>They note MF loss has high variance, then replace the raw MSE-style MF loss with a <strong>Cauchy (Lorentzian) robust loss</strong> and an adaptive weighting term over timestep pairs. Their DMF objective is written as an adaptive weighted Cauchy form over the MeanFlow residual.</p> <h4 id="b-time-proposal-tailored-to-flow-maps">(b) Time proposal tailored to flow maps</h4> <p>They adapt timestep-pair sampling for flow maps (since you need ordered pairs with \(t&gt;r\)). They sample two logit-normal values and sort them, and then bias the proposal toward larger gaps / smaller \(r\) for better 1-step behavior, because converted DMF models are already strong near the diagonal \(r \approx t\).</p> <h4 id="c-model-guidance-mg">(c) Model Guidance (MG)</h4> <p>They use <strong>Model Guidance (MG)</strong> to avoid the full compute cost of CFG during training, and note MG is especially effective for training high-quality few-step flow maps. This is part of why their 1-step/4-step results are strong.</p> <h3 id="256-takeaways">2.5.6 Takeaways</h3> <p>DMF is not just another MeanFlow variant. It reframes the problem:</p> <ul> <li><strong>MeanFlow</strong> gives the right objective (average velocity via JVP)</li> <li><strong>UAIMF / AlphaFlow</strong> improve optimization dynamics</li> <li><strong>DMF</strong> improves the <strong>architecture + training pipeline</strong>, and shows pretrained flow models are the best starting point</li> </ul> <p>They report SOTA-level few-step results and show 1-step / 4-step generation approaching much more expensive flow-model sampling with large inference-speed gains.</p> <hr/> <h1 id="3-flowmap">3. FlowMap</h1> <h2 id="31-free-flowmap">3.1 Free FlowMap</h2> <p>Flow Map Distillation Without Data bias</p> <h4 id="311-teacher-data-mismatch-the-hidden-bug-in-many-distillation-pipelines">3.1.1 Teacher-data mismatch (the hidden bug in many distillation pipelines)</h4> <p>Traditional flow-map distillation often samples intermediate states $x_t$ from an <strong>external dataset distribution</strong> and supervises the student using teacher velocities at those states.</p> <p>But the student is supposed to reproduce the teacher’s <strong>sampling process</strong>, i.e. the trajectory distribution induced by the teacher from the prior.</p> <p>Supervision states coming from a mismatched distribution results in a <strong>teacher-data mismatch</strong>:</p> <ul> <li>the student is trained on states that are not on the teacher’s true rollout distribution,</li> <li>more augmentation can worsen it,</li> <li>student quality degrades.</li> </ul> <p>This is a deep point because it says the standard “distill on data” recipe is fundamentally misaligned with the actual objective (imitate the teacher sampler).</p> <h4 id="312-prior-only--self-generated-flow-map-objective">3.1.2 Prior-only / self-generated flow-map objective</h4> <p>Supervise entirely from the prior and the student’s own generated states.</p> <p>They derive a sufficient optimality condition leading to a loss of the form</p> \[\mathcal{L}_{\text{pred}} = \mathbb{E}_{z,\delta} \left[ \|F_\theta(z,\delta)-\operatorname{sg}(u_{\text{target}})\|^2 \right]\] <p>with</p> \[u_{\text{target}} = u(f_\theta(z,\delta),1-\delta) - \delta \,\partial_\delta F_\theta(z,\delta)\] <p>The key interpretation:</p> <ul> <li>$f_\theta(z,\delta)$ defines the student’s current trajectory,</li> <li>$\partial_\delta f_\theta$ is the <strong>student’s generating velocity</strong>,</li> <li>the loss is equivalent to aligning the student generating velocity with the teacher field:</li> </ul> \[\partial_\delta f_\theta \approx u(f_\theta(z,\delta),1-\delta)\] <p>So the student learns to <strong>ride the teacher vector field along its own generated path</strong>, starting from pure noise, no dataset needed.</p> <p>This is the right fix for teacher-data mismatch.</p> <h4 id="313-gradient-view">3.1.3 Gradient view</h4> <p>They explicitly write the optimization gradient in terms of a velocity mismatch</p> \[\Delta v_{G,u} = v_G - u\] <p>which is great because it makes gradient weighting / normalization tricks easier to reason about.</p> <p>This is one of those “small” presentation choices that actually matters in practice.</p> <h4 id="314-correction-objective-fixing-distribution-drift-not-just-local-velocity">3.1.4 Correction objective (fixing distribution drift, not just local velocity)</h4> <p>Here’s the catch: aligning generating velocity locally is necessary, but in finite-capacity/discrete training the generated distribution can still drift.</p> <p>So they add a <strong>correction objective</strong> motivated from minimizing a KL term over intermediate marginals and then translating score mismatch into <strong>velocity mismatch</strong> (via the score–velocity equivalence for linear interpolants).</p> <p>This yields a gradient proportional to</p> \[\nabla_\theta \; \mathbb{E}_{z,n,r} \left[ F_\theta(z,1)^\top \operatorname{sg} \left( v_N(I_r(f_\theta(z,1),n),r) - u(I_r(f_\theta(z,1),n),r) \right) \right]\] <p>where:</p> <ul> <li>$u$ is the teacher marginal velocity,</li> <li>$v_N$ is the student-induced <strong>noising</strong> marginal velocity,</li> <li>$I_r(\cdot,\cdot)$ is the interpolation to intermediate time $r$.</li> </ul> <p>Intuition:</p> <ul> <li>the prediction loss aligns the <strong>student’s forward/generating flow</strong></li> <li>the correction loss aligns the <strong>student-induced noising marginals</strong> with the teacher’s marginals</li> </ul> <p>That is a nice bidirectional correction mechanism.</p> <hr/> <h2 id="32-meta-flow-maps">3.2 Meta Flow Maps</h2> <p>Meta flow maps correspond to a <strong>stochastic flow map</strong>, which is important because deterministic flow maps are too rigid.</p> <h4 id="321-why-stochastic-flow-maps">3.2.1 Why stochastic flow maps?</h4> <p>Deterministic flow-map learning works if the transport map is the right object. But for many diffusion-like processes, especially when you want richer uncertainty handling, a <strong>stochastic transition kernel</strong></p> <p>\(\kappa_{t,s}(z_t, z_s)\) is the right object.</p> <p>The paper frames this using:</p> <ul> <li><strong>marginal consistency</strong></li> <li><strong>conditional consistency</strong></li> <li> <table> <tbody> <tr> <td>a family of posterior conditionals $p_{1</td> <td>t}$</td> </tr> </tbody> </table> </li> <li>and a diagonal supervision view</li> </ul> <p>The important conceptual upgrade is:</p> <ul> <li>instead of only learning deterministic trajectories,</li> <li>learn a transition operator consistent with the stochastic process structure.</li> </ul> <h4 id="322-the-diagonal-condition-same-role-as-fm-diagonal-supervision">3.2.2 The diagonal condition (same role as FM diagonal supervision)</h4> <p>They derive that on the diagonal:</p> \[\kappa_{t,t}(z_t, z_1) = p_{1|t}(z_1 \mid z_t)\] <p>This is the stochastic analogue of “when $r=t$, your two-time object must match the one-time target.”</p> <p>So the diagonal again plays the role of anchor supervision.</p> <h4 id="323-pathwiseconsistency-relation-for-stochastic-flow-maps">3.2.3 Pathwise/consistency relation for stochastic flow maps</h4> <p>They also derive a consistency/composition condition (their Eq. 23 in the snippet):</p> \[\kappa_{t,s}(z_t,z_s) = \mathbb{E}_{z_1 \sim p_{1|t}(\cdot|z_t)} \big[ \kappa_{u,s}(z_u,z_s) \big]\] <p>(with the appropriate latent dependence through $z_1$/paths)</p> <p>The exact notation is heavier, but the key idea is the same as flow-map composition:</p> <ul> <li><strong>two-time transitions must compose correctly through intermediate times</strong>, but now in distributional form.</li> </ul> <h4 id="324-their-training-objective-mfm-loss">3.2.4 Their training objective (MFM loss)</h4> <p>They build:</p> <ol> <li>a <strong>diagonal supervision loss</strong> (fit the posterior on diagonal time pairs),</li> <li>a <strong>consistency loss</strong> (enforce off-diagonal composition consistency),</li> <li>and combine them into an MFM objective:</li> </ol> \[\mathcal{L}_{\text{MFM}} = \mathcal{L}_{\text{diag}} + \lambda \mathcal{L}_{\text{cons}}\] <p>This is the stochastic counterpart of the deterministic progression:</p> <ul> <li>diagonal target = “FM-like” anchor</li> <li>off-diagonal consistency = “flow-map-like” propagation</li> </ul> <h4 id="325-takeaways">3.2.5 Takeaways</h4> <p>This paper gives a more general lens:</p> <ul> <li>MeanFlow / deterministic flow maps are one branch</li> <li>stochastic transition learning is the broader object when uncertainty matters</li> <li>the “diagonal + consistency” decomposition is the unifying pattern</li> </ul> <p>This is exactly the kind of conceptual bridge diffusion researchers should care about.</p> <hr/> <h2 id="33-terminal-velocity-matching-tvm">3.3 Terminal Velocity Matching (TVM)</h2> <h4 id="331-core-idea-match-the-terminal-velocity-of-a-learned-flow-map">3.3.1 Core idea: match the <strong>terminal</strong> velocity of a learned flow map</h4> <p>TVM parameterizes a <strong>two-time displacement map</strong> (flow map increment) directly, instead of only learning the instantaneous velocity field.</p> <p>Let the ground-truth displacement from time $t$ to $s$ be \(f(x_t,t,s) := \psi(x_t,t,s) - x_t.\)</p> <p>TVM uses a model \(f_\theta(x_t,t,s) = (s-t)F_\theta(x_t,t,s),\) and defines the model’s instantaneous velocity as the boundary derivative \(u_\theta(x_t,t) := \left.\frac{d}{ds}f_\theta(x_t,t,s)\right|_{s=t} = F_\theta(x_t,t,t).\)</p> <p>This is the key unification:</p> <ul> <li>the <strong>same network</strong> learns both <ol> <li>a large-step displacement map $f_\theta$, and</li> <li>an infinitesimal velocity field $u_\theta$.</li> </ol> </li> </ul> <h4 id="332-why-terminal-velocity">3.3.2 Why “terminal” velocity?</h4> <p>The ground-truth displacement satisfies \(f(x_t,t,s)=\int_t^s u(x_r,r)\,dr.\)</p> <p>Differentiate w.r.t. the <strong>terminal time</strong> $s$: \(\frac{d}{ds}f(x_t,t,s)=u(\psi(x_t,t,s),s).\)</p> <p>This is the terminal velocity condition.</p> <p>The nice part is: if the terminal-velocity condition is satisfied along the trajectory, then the displacement map error is controlled (TVM shows an upper bound of displacement error by integrated terminal-velocity error). So instead of directly supervising the full ODE integral, TVM supervises the <strong>derivative at the terminal endpoint</strong>.</p> <p>This is the conceptual contrast with MeanFlow:</p> <ul> <li><strong>MeanFlow</strong> differentiates w.r.t. the <strong>start time</strong> $t$,</li> <li><strong>TVM</strong> differentiates w.r.t. the <strong>end time</strong> $s$.</li> </ul> <h4 id="333-proxy-trick-how-they-make-it-trainable">3.3.3 Proxy trick (how they make it trainable)</h4> <p>The terminal condition depends on unknown ground-truth objects:</p> <ul> <li>$\psi(x_t,t,s)$ (true flow map)</li> <li>$u(\cdot,s)$ (true velocity field)</li> </ul> <p>TVM replaces them with model proxies: \(u(\psi(x_t,t,s),s) \;\approx\; u_\theta\big(x_t + f_\theta(x_t,t,s),\, s\big).\)</p> <p>So the model predicts a displacement to a new point \(x_s^{(\theta)} = x_t + f_\theta(x_t,t,s),\) then evaluates its own velocity field at that terminal point.</p> <p>This makes the loss <strong>self-consistent</strong> and trainable in one stage.</p> <h4 id="334-tvm-objective-the-actual-loss">3.3.4 TVM objective (the actual loss)</h4> <p>TVM jointly optimizes:</p> <ol> <li><strong>Terminal-velocity matching term</strong> (general $t \ge s$)</li> <li><strong>Flow Matching boundary term</strong> (special case / anchor)</li> </ol> <p>Per-time objective: \(\mathcal{L}_{\mathrm{TVM}}^{t,s}(\theta) = \mathbb{E} \Big[ \underbrace{ \left\| \frac{d}{ds}f_\theta(x_t,t,s) - u_\theta\big(x_t + f_\theta(x_t,t,s), s\big) \right\|_2^2 }_{\text{terminal velocity matching}} + \underbrace{ \|u_\theta(x_s,s)-v_s\|_2^2 }_{\text{Flow Matching anchor}} \Big].\)</p> <p>Where:</p> <ul> <li>$x_s$ is sampled from the interpolation path (as in Flow Matching),</li> <li>$v_s$ is the standard FM target velocity (e.g. for linear interpolation, $v_s = x_1 - x_0$).</li> </ul> <p>Then the practical training objective is just expectation over sampled time pairs: \(\mathcal{L}_{\mathrm{TVM}}(\theta) = \mathbb{E}_{t,s}\left[\mathcal{L}_{\mathrm{TVM}}^{t,s}(\theta)\right].\)</p> <h4 id="335-ema--stop-gradient-version-important-in-practice">3.3.5 EMA + stop-gradient version (important in practice)</h4> <p>Like consistency/distillation-style methods, TVM stabilizes training using:</p> <ul> <li><strong>EMA target network</strong></li> <li><strong>stop-gradient</strong> on proxy paths</li> </ul> <p>The practical version uses:</p> <ul> <li>stop-grad copy for displacement branch,</li> <li>stop-grad EMA copy for terminal velocity target.</li> </ul> <p>Conceptually: \(u_\theta\big(x_t + f_\theta(x_t,t,s),s\big) \;\to\; u_{\theta_{\text{EMA}}}^{\text{sg}} \Big(x_t + f_{\theta}^{\text{sg}}(x_t,t,s), s\Big).\)</p> <p>This avoids collapse / target chasing and makes the proxy supervision much more stable.</p> <h4 id="336-why-tvm-is-strong-theoretically-the-important-claim">3.3.6 Why TVM is strong theoretically (the important claim)</h4> <p>TVM proves a <strong>distribution-level guarantee</strong>:</p> <p>Under a Lipschitz assumption on $u_\theta(\cdot,s)$, a weighted time integral of the TVM objective upper-bounds the squared Wasserstein-2 distance between:</p> <ul> <li>the model pushforward distribution (via the learned map), and</li> <li>the true data distribution.</li> </ul> <p>In spirit: $$ W_2^2(\text{model pushforward}, p_0) \;\lesssim\; \int_0^t \lambda(s)\,\mathcal{L}_{\mathrm{TVM}}^{t,s}(\theta)\,ds</p> <ul> <li>C. $$</li> </ul> <p>This is a big deal because many one/few-step distillation methods work well empirically but do <strong>not</strong> cleanly tie their loss to a distribution divergence.</p> <h4 id="337-classifier-free-guidance-cfg-version">3.3.7 Classifier-Free Guidance (CFG) version</h4> <p>TVM extends naturally to conditional generation with CFG.</p> <p>They define a CFG-conditioned displacement map \(f_\theta(x_t,t,s,c,w),\) where:</p> <ul> <li>$c$ = class condition</li> <li>$w$ = guidance scale</li> </ul> <p>The practical CFG objective adds two important ideas:</p> <ol> <li><strong>Condition on $w$ directly</strong> <ul> <li>The network sees the guidance scale at training time.</li> <li>This lets one model support multiple CFG scales.</li> </ul> </li> <li><strong>Use a $1/w^2$ weighting</strong> <ul> <li>Because target velocity magnitude scales roughly linearly with $w$,</li> <li>the loss can explode for large $w$ without correction.</li> </ul> </li> </ol> <p>So the conditional TVM loss is roughly: \(\frac{1}{w^2} \left\| \frac{d}{ds}f_\theta(\cdot,c,w) - u_{\theta_{\text{EMA}}}^{\text{sg}}(\cdot,c,w) \right\|_2^2 + \mathcal{L}_{\mathrm{FM}}^{\text{CFG}}.\)</p> <p>This is one of the most practical contributions in the paper because it supports <strong>stable training under varying CFG</strong>.</p> <h4 id="338-sampling-algorithm">3.3.8 Sampling algorithm</h4> <p>Once trained, sampling is dead simple and supports both 1-step and few-step generation <strong>without retraining</strong>.</p> <p>For a sequence of times \(1=t_0 &gt; t_1 &gt; \cdots &gt; t_n=0,\) iterate: \(x_{t_{k+1}} = x_{t_k} + f_\theta(x_{t_k}, t_k, t_{k+1}) = x_{t_k} + (t_{k+1}-t_k)F_\theta(x_{t_k}, t_k, t_{k+1}).\)</p> <p>So:</p> <ul> <li><strong>1-NFE</strong>: one direct jump $t=1 \to 0$</li> <li><strong>few-NFE</strong>: chain multiple learned jumps</li> <li>no ODE solver needed (the network itself is the integrator)</li> </ul> <p>This is exactly why flow-map-style methods are so appealing for distillation and fast sampling.</p> <h4 id="339-jvp-term-and-implementation-detail-important-for-algorithm-understanding">3.3.9 JVP term and implementation detail (important for “algorithm” understanding)</h4> <p>Because \(f_\theta(x_t,t,s)=(s-t)F_\theta(x_t,t,s),\) the terminal derivative expands as \(\frac{d}{ds}f_\theta(x_t,t,s) = F_\theta(x_t,t,s) + (s-t)\,\partial_s F_\theta(x_t,t,s).\)</p> <p>That second term is a <strong>Jacobian-vector product (JVP)</strong> through the network.</p> <p>TVM’s practical novelty is not just using JVP, but supporting:</p> <ul> <li><strong>JVP through FlashAttention</strong></li> <li><strong>backprop through the JVP result</strong></li> </ul> <p>That matters a lot for DiT-scale transformers, because naive PyTorch attention + JVP is too slow / memory-heavy.</p> <h4 id="3310-practical-tricks-that-actually-matter-from-the-paper">3.3.10 Practical tricks that actually matter (from the paper)</h4> <p>TVM adds several engineering choices that are unusually important:</p> <ol> <li><strong>Semi-Lipschitz control in DiT</strong> <ul> <li>The theory needs Lipschitzness-ish behavior.</li> <li>Vanilla DiT components (LayerNorm/SDPA) are problematic.</li> <li>They replace key normalizations with RMSNorm-style variants and normalize AdaLN modulation terms.</li> </ul> </li> <li><strong>AdamW $\beta_2 = 0.95$</strong> <ul> <li>Higher-order gradients from JVP make training noisy.</li> <li>Lower $\beta_2$ stabilizes second-moment tracking.</li> </ul> </li> <li><strong>Scaled parameterization for CFG</strong> <ul> <li>Make the model output scale with $w$ by construction: \(f_\theta(x_t,t,s,c,w)=(s-t)\,w\,F_\theta(x_t,t,s,c,w).\)</li> <li>This improves optimization under large guidance.</li> </ul> </li> <li><strong>Time sampling matters</strong> <ul> <li>They ablate multiple $(t,s)$ sampling schemes.</li> <li>Biasing toward larger jumps (larger $t$, smaller $s$) helps, but too aggressive hurts.</li> <li>They also find using a separate time distribution for the FM anchor term can help.</li> </ul> </li> </ol> <h4 id="3311-tvm-vs-meanflow-the-clean-comparison">3.3.11 TVM vs MeanFlow (the clean comparison)</h4> <p><strong>MeanFlow</strong></p> <ul> <li>matches a derivative condition w.r.t. <strong>start time</strong> $t$</li> <li>propagates $u(x_t,t)$ through the JVP path</li> <li>more sensitive to random CFG because the velocity magnitude directly enters the JVP branch</li> </ul> <p><strong>TVM</strong></p> <ul> <li>matches a derivative condition w.r.t. <strong>terminal time</strong> $s$</li> <li>JVP is w.r.t. $s$ (cleaner and more stable under random CFG)</li> <li>has a clearer Wasserstein-style distribution guarantee</li> <li>naturally supports one-step and few-step jumps with the same network</li> </ul> <p>This is why TVM is a strong “algorithmic” evolution of flow-map distillation rather than just another loss tweak.</p> <hr/> <h1 id="4-score-distillation">4. Score Distillation</h1> <h2 id="41-variational-score-distillation-vsd">4.1 Variational Score Distillation (VSD)</h2> <h3 id="411-score-distillation-sampling-sds">4.1.1 Score Distillation Sampling (SDS)</h3> <p>VSD explicitly frames <strong>SDS</strong> as the starting point for text-to-3D optimization.<br/> Given a differentiable renderer</p> \[g(\theta, c)\] <p>and a pretrained text-to-image diffusion model, SDS optimizes a <strong>single 3D parameter</strong> $\theta$ by minimizing a KL objective over noisy rendered images:</p> \[L_{\mathrm{SDS}}(\theta) := \mathbb{E}_{t,c} \left[ \frac{\sigma_t}{\alpha_t}\,\omega(t)\, D_{\mathrm{KL}} \big(q_t^\theta(x_t \mid c)\,\|\,p_t(x_t \mid y_c)\big) \right].\] <p>The practical gradient (the one everyone uses) is approximated as:</p> \[\nabla_\theta L_{\mathrm{SDS}}(\theta) \approx \mathbb{E}_{t,\epsilon,c} \left[ \omega(t)\big(\epsilon_{\mathrm{pretrain}}(x_t,t,y_c)-\epsilon\big) \frac{\partial g(\theta,c)}{\partial \theta} \right],\] <p>with</p> \[x_t = \alpha_t g(\theta,c) + \sigma_t \epsilon.\] <h4 id="412-why-sds-is-limited">4.1.2 Why SDS is limited</h4> <p>SDS uses a <strong>single parameter</strong> $\theta$ and directly optimizes it, which makes it a <strong>mode-seeking</strong> procedure in practice.<br/> VSD points out this is one reason SDS often gives over-smoothed / over-saturated / artifacted 3D results (especially in low-density regions of image space).</p> <hr/> <h3 id="413-vsd-move-from-optimizing-one-3d-sample-to-learning-a-distribution-over-3d-parameters">4.1.3 VSD: move from optimizing one 3D sample to learning a distribution over 3D parameters</h3> <p>The key VSD move is to define a <strong>distribution over 3D parameters</strong></p> \[\mu(\theta \mid y)\] <p>instead of optimizing one fixed $\theta$.</p> <p>Then VSD evolves particles by a <strong>Wasserstein gradient flow / particle ODE</strong>. The ODE uses a score difference between:</p> <ol> <li>the score of noisy real images (from the pretrained diffusion model), and</li> <li>the score of noisy rendered images (estimated by a learnable network).</li> </ol> <p>The VSD particle dynamics are:</p> \[\frac{d\theta_\tau}{d\tau} = -\mathbb{E}_{t,\epsilon,c} \left[ \omega(t) \left( -\sigma_t \nabla_{x_t}\log p_t(x_t \mid y_c) - \big(-\sigma_t \nabla_{x_t}\log q_t^{\mu_\tau}(x_t \mid c,y)\big) \right) \frac{\partial g(\theta_\tau,c)}{\partial \theta_\tau} \right].\] <p>This is the conceptual heart of VSD:</p> <ul> <li><strong>teacher score</strong> pulls rendered images toward the text-conditioned image manifold,</li> <li><strong>rendered-image score</strong> corrects for the current particle distribution,</li> <li>the difference gives a much better update than raw SDS.</li> </ul> <hr/> <h3 id="414-the-vsd-score-function-estimator-this-is-the-diffusion-distillation-part">4.1.4 The VSD score function estimator (this is the diffusion-distillation part)</h3> <p>VSD introduces a second noise predictor</p> \[\epsilon_\phi(x_t,t,c,y)\] <p>to estimate the score of noisy rendered images. It is trained with the <strong>standard diffusion noise-prediction objective</strong> on rendered images from the current particles:</p> \[\min_\phi \sum_{i=1}^n \mathbb{E}_{t,\epsilon,c} \left[ \left\| \epsilon_\phi\big(\alpha_t g(\theta^{(i)},c)+\sigma_t\epsilon,\ t,\ c,\ y\big)-\epsilon \right\|_2^2 \right].\] <p>This is the crucial diffusion-distillation component in VSD:</p> <ul> <li>VSD <strong>distills a score model for the rendered-image distribution</strong> (not just the teacher),</li> <li>then uses the <strong>difference of two scores</strong> for particle updates.</li> </ul> <p>In practice, VSD parameterizes $\epsilon_\phi$ as either:</p> <ul> <li>a small U-Net, or</li> <li>a <strong>LoRA</strong> adaptation of the pretrained diffusion model (usually better fidelity).</li> </ul> <hr/> <h3 id="415-vsd-gradient-used-for-updating-the-3d-particles">4.1.5 VSD gradient used for updating the 3D particles</h3> <p>Once $\epsilon_\phi$ is trained, the per-particle VSD gradient becomes:</p> \[\nabla_\theta L_{\mathrm{VSD}}(\theta) = \mathbb{E}_{t,\epsilon,c} \left[ \omega(t)\, \big( \epsilon_{\mathrm{pretrain}}(x_t,t,y_c) - \epsilon_\phi(x_t,t,c,y) \big)\, \frac{\partial g(\theta,c)}{\partial \theta} \right],\] <p>with</p> \[x_t = \alpha_t g(\theta,c)+\sigma_t\epsilon.\] <p>This is the clean algorithmic form:</p> <ul> <li><strong>SDS</strong> uses $\epsilon_{\mathrm{pretrain}} - \epsilon$</li> <li><strong>VSD</strong> uses $\epsilon_{\mathrm{pretrain}} - \epsilon_\phi$</li> </ul> <p>That replacement is the whole game.</p> <hr/> <h3 id="416-vsd-vs-sds">4.1.6 VSD vs SDS</h3> <p>VSD shows SDS is a <strong>special case</strong>:</p> <ul> <li>if you approximate the parameter distribution $\mu(\theta \mid y)$ by a single empirical point mass (one particle),</li> <li>then the rendered-image score term degenerates to the sampled noise $\epsilon$,</li> <li>and you recover vanilla SDS / SJC.</li> </ul> <p>So the practical interpretation is:</p> <ul> <li><strong>SDS</strong> = single-point / no-generalization approximation</li> <li><strong>VSD</strong> = distributional score-distillation with a learned rendered-image score model</li> </ul> <p>That is why VSD usually gives better diversity and better fidelity in text-to-3D than plain SDS.</p> <hr/> <h2 id="42-dmd-distribution-matching-distillation">4.2 DMD (Distribution Matching Distillation)</h2> <h3 id="421-core-idea">4.2.1 Core idea</h3> <p>DMD trains a <strong>one-step generator</strong> by taking a <strong>distribution-matching gradient</strong> (KL-driven, score-difference style). The distribution-matching improves realism and fixes pure regression collapse.</p> <hr/> <h3 id="422-distribution-matching-gradient">4.2.2 Distribution matching gradient</h3> <p>DMD computes a gradient by comparing two denoisers on the same noisy fake sample:</p> <ul> <li>$\mu_{\text{real}}$: pretrained denoiser trained on real data</li> <li>$\mu_{\text{fake}}$: denoiser trained on fake/generated data</li> </ul> <p>For a fake image $x = G_\theta(z)$:</p> <ol> <li>add random diffusion noise to get $x_t$</li> <li>denoise with both networks</li> <li>use the difference as the realism direction</li> </ol> <p>The implementation-level gradient proxy is essentially:</p> \[\mathrm{grad} \propto \frac{ \mu_{\text{fake}}(x_t,t)-\mu_{\text{real}}(x_t,t) }{ \text{weighting\_factor} }\] <p>and they realize this as a stop-grad MSE objective on $x$ (so autograd yields the desired gradient direction).</p> <p>This is the key DMD trick:</p> <ul> <li>avoid backprop through a long teacher trajectory,</li> <li>still get a <strong>distribution-level</strong> correction signal.</li> </ul> <h4 id="423-practical-read-on-dmd">4.2.3 Practical read on DMD</h4> <p>DMD is strong because it is <strong>not just teacher regression</strong>:</p> <ul> <li>the KL / distribution-matching gradient gives a realism signal beyond memorizing teacher trajectories,</li> <li>the regression term keeps training stable.</li> </ul> <p>That combo is why DMD was a big step for one-step image generation.</p> <hr/> <h2 id="43-sid-score-identity-distillation">4.3 SiD (Score identity Distillation)</h2> <h3 id="431-core-framing-explicit-score-matching-on-fake-data-diffusion">4.3.1 Core framing: explicit score matching on fake-data diffusion</h3> <p>SiD reframes one-step diffusion distillation as minimizing a <strong>score-difference loss</strong> on the diffused fake-data distribution.</p> <p>Define the score difference:</p> \[\delta_{\phi,\theta}(x_t) := S_\phi(x_t) - \nabla_{x_t}\log p_\theta(x_t),\] <p>where:</p> <ul> <li>$S_\phi(x_t)$ is the pretrained teacher score,</li> <li>$p_\theta(x_t)$ is the diffused generator distribution.</li> </ul> <p>Then SiD defines the theoretical objective (MESM / Fisher divergence):</p> \[L_\theta = \mathbb{E}_{x_t \sim p_\theta(x_t)} \left[ \|\delta_{\phi,\theta}(x_t)\|_2^2 \right].\] <p>This is the cleanest formulation in this chapter conceptually:</p> <ul> <li>make the <strong>fake-data score</strong> match the <strong>teacher score</strong>.</li> </ul> <hr/> <h3 id="432-why-sid-needs-an-auxiliary-score-network">4.3.2 Why SiD needs an auxiliary score network</h3> <p>The hard part is the generator score</p> \[\nabla_{x_t}\log p_\theta(x_t),\] <p>which is intractable directly.</p> <p>SiD introduces a second network</p> \[f_\psi(x_t,t)\] <p>to approximate the conditional denoising mean / score-related quantity:</p> \[f_{\psi^*(\theta)}(x_t,t) = \mathbb{E}[x_g \mid x_t] = x_t + \sigma_t^2 \nabla_{x_t}\log p_\theta(x_t).\] <p>This gives a score-difference estimator:</p> \[\delta_{\phi,\psi^*(\theta)}(x_t) = \sigma_t^{-2}\big(f_\phi(x_t,t)-f_{\psi^*(\theta)}(x_t,t)\big).\] <p>So SiD turns score estimation into a diffusion-style denoising regression problem for $f_\psi$.</p> <hr/> <h3 id="433-the-naive-approximation-fails-important">4.3.3 The naive approximation fails (important)</h3> <p>A very natural idea is to replace $\psi^*(\theta)$ with the current $\psi$ and use</p> \[\delta_{\phi,\psi}(x_t) = \sigma_t^{-2}\big(f_\phi(x_t,t)-f_\psi(x_t,t)\big),\] <p>then optimize the naive approximated loss</p> \[L_\theta^{(1)} = \mathbb{E}\left[\|\delta_{\phi,\psi}(x_t)\|_2^2\right].\] <p>SiD explicitly shows this can fail badly because:</p> <ul> <li>the approximation error in $f_\psi$ enters the objective in a destabilizing way,</li> <li>the loss depends on both score-estimation error and the true score difference.</li> </ul> <p>This is the main reason SiD is not just “DMD but with a different notation.”</p> <hr/> <h3 id="434-projected-score-matching-sids-key-fix">4.3.4 Projected score matching (SiD’s key fix)</h3> <p>SiD derives an alternative MESM form (Projected Score Matching) and then approximates it to get a much more stable objective:</p> \[L_\theta^{(2)} = \mathbb{E} \left[ \sigma_t^{-2}\, \delta_{\phi,\psi}(x_t)^\top \big(f_\phi(x_t,t)-x_g\big) \right].\] <p>This version is much more stable because it depends on the denoising residual</p> \[f_\phi(x_t,t)-x_g\] <p>instead of directly squaring the noisy score-difference estimate.</p> <p>That is the algorithmic insight behind SiD:</p> <ul> <li>use score identities to derive a loss that still targets MESM,</li> <li>but behaves better under imperfect generator-score estimation.</li> </ul> <hr/> <h3 id="435-fused-loss-and-alternating-updates-the-actual-algorithm">4.3.5 Fused loss and alternating updates (the actual algorithm)</h3> <p>SiD trains with <strong>alternating updates</strong>:</p> <ol> <li><strong>Update</strong> $f_\psi$ (the generator-score estimator) with a diffusion / denoising loss.</li> <li><strong>Update</strong> $G_\theta$ using the SiD generator loss (built from the projected score-matching approximation / fused loss).</li> </ol> <p>It also uses score gradients (backprop through both score networks), which distinguishes it from methods like Diff-Instruct and DMD.</p> <p>In practice this is more memory/computation heavy, but it is a major reason SiD gets very strong one-step results.</p> <hr/> <h1 id="5-adversarial-distillation">5. Adversarial Distillation</h1> <h2 id="51-adversarial-diffusion-distillation-add">5.1 Adversarial Diffusion Distillation (ADD)</h2> <p>ADD is the clean “diffusion-distill + GAN refine” recipe for turning a pre-trained diffusion teacher into a fast student (often 1-step or few-step), while explicitly preserving the teacher’s denoising behavior.</p> <h3 id="511-core-idea-combine-adversarial-learning-with-diffusion-distillation">5.1.1 Core idea: combine adversarial learning with diffusion distillation</h3> <p>The student is trained with two losses:</p> <ol> <li><strong>Adversarial loss</strong> for photorealistic high-frequency detail</li> <li><strong>Diffusion distillation loss</strong> to stay aligned with the teacher’s denoising trajectory</li> </ol> <p>The total generator objective is:</p> \[\mathcal{L}_G = \lambda_{\mathrm{adv}}\mathcal{L}_{\mathrm{adv}} + \lambda_{\mathrm{distill}}\mathcal{L}_{\mathrm{distill}}\] <p>This is the key point: ADD is not “just a GAN on top of diffusion outputs.” It is explicitly a <strong>teacher-constrained adversarial distillation</strong> method.</p> <h3 id="512-adversarial-part-generator--discriminator">5.1.2 Adversarial part (generator + discriminator)</h3> <p>ADD uses a DINOv2-feature-based discriminator setup (projected / feature-space discrimination style), which is more stable and semantically informed than a plain pixel discriminator.</p> <p>Generator adversarial loss (hinge-form style, on discriminator outputs) is:</p> \[\mathcal{L}_{\mathrm{adv}} = \mathbb{E}_{x_t, t, c}\left[D_\phi(\hat{x}_{\theta,t}, t, c)\right]\] <p>The discriminator is trained with a real/fake hinge objective plus regularization:</p> \[\mathcal{L}_D = \mathbb{E}_{x_t, t, c}\left[\max(0, 1 - D_\phi(\hat{x}_{\psi,t}, t, c))\right] + \mathbb{E}_{x_t, t, c}\left[\max(0, 1 + D_\phi(\hat{x}_{\theta,t}, t, c))\right] + \lambda_{R1}\mathcal{L}_{R1}\] <p>where:</p> <ul> <li>$\hat{x}_{\psi,t}$ is the <strong>teacher-predicted denoised image</strong></li> <li>$\hat{x}_{\theta,t}$ is the <strong>student-predicted denoised image</strong></li> <li>the discriminator sees timestep and conditioning too, so the adversarial game is timestep-aware</li> </ul> <h3 id="513-diffusion-distillation-part-the-important-anchor">5.1.3 Diffusion distillation part (the important anchor)</h3> <p>The distillation target is the teacher’s denoised estimate. ADD uses an SDS-style weighted regression to the teacher prediction:</p> \[\mathcal{L}_{\mathrm{distill}} = \mathbb{E}_{x_t,t,c}\left[ \omega(\lambda(t)) \left\| \hat{x}_{\theta,t} - \hat{x}_{\psi,t} \right\|_2^2 \right]\] <p>This is the “don’t drift too far from the teacher” term. Without it, pure adversarial training tends to hallucinate detail but break semantic / structural fidelity.</p> <h3 id="514-why-add-matters">5.1.4 Why ADD matters</h3> <p>ADD became a strong template because it fixes the classic one-step problem:</p> <ul> <li>pure distillation gives blurry or over-smoothed outputs</li> <li>pure GAN gives sharp but unstable / off-manifold outputs</li> </ul> <p>ADD gets both:</p> <ul> <li><strong>teacher alignment</strong> from diffusion distillation</li> <li><strong>sharpness and realism</strong> from adversarial training</li> </ul> <h3 id="515-practical-algorithm-high-level">5.1.5 Practical algorithm (high-level)</h3> <ol> <li>Sample $(x,c)$ and a timestep $t$</li> <li>Corrupt to get $x_t$</li> <li>Get teacher denoised prediction $\hat{x}_{\psi,t}$</li> <li>Get student denoised prediction $\hat{x}_{\theta,t}$</li> <li>Update discriminator with real = teacher prediction, fake = student prediction</li> <li>Update student with: <ul> <li>adversarial loss against discriminator</li> <li>weighted distillation loss to teacher prediction</li> </ul> </li> </ol> <hr/> <h2 id="52-ladd-latent-adversarial-diffusion-distillation">5.2 LADD (Latent Adversarial Diffusion Distillation)</h2> <p>LADD is the latent-video extension of the adversarial distillation idea. The big shift is: <strong>do the adversarial game in latent/video space and train for long-horizon generation without requiring paired long targets</strong>.</p> <h3 id="521-motivation-one-step-video-gets-killed-by-exposure-error">5.2.1 Motivation: one-step video gets killed by exposure error</h3> <p>Teacher-forced diffusion distillation works okay for short clips, but for long autoregressive rollout:</p> <ul> <li>errors accumulate</li> <li>small distortions compound</li> <li>teacher-forced supervision mismatches inference-time behavior</li> </ul> <p>LADD’s fix is to lean harder into adversarial training and use the discriminator as the long-horizon supervision signal.</p> <h3 id="522-key-move-adversarial-supervision-replaces-explicit-per-step-paired-targets">5.2.2 Key move: adversarial supervision replaces explicit per-step paired targets</h3> <p>Instead of requiring exact paired long-video targets (which are scarce and awkward), LADD trains the generator so each generated segment looks real to a discriminator.</p> <p>That is the crucial algorithmic advantage:</p> <ul> <li><strong>supervised distillation</strong> needs paired targets and usually short clips</li> <li><strong>adversarial training</strong> only needs “real vs generated” segments</li> </ul> <p>So LADD can train long-video generators from ordinary video data, even when very long continuous shots are rare.</p> <h3 id="523-training-pipeline-structure-same-spirit-as-aptaapt-but-latent-video-focused">5.2.3 Training pipeline structure (same spirit as APT/AAPT, but latent-video focused)</h3> <p>LADD-style training is best viewed as a staged pipeline:</p> <ol> <li><strong>Initialize from a pre-trained diffusion model</strong> <ul> <li>keeps strong prior / semantics / motion prior</li> </ul> </li> <li><strong>(Optional but common) distillation or adaptation warm-start</strong> <ul> <li>stabilize the student before adversarial training</li> <li>preserve diffusion behavior early</li> </ul> </li> <li><strong>Adversarial post-training in latent space</strong> <ul> <li>discriminator distinguishes real vs generated latent video segments</li> <li>generator is trained autoregressively</li> <li>generated outputs are recycled as future inputs (student-forcing behavior)</li> </ul> </li> <li><strong>Long-video segment training</strong> <ul> <li>generate long sequences</li> <li>split into shorter overlapping segments for discriminator evaluation</li> <li>accumulate gradients segment-wise</li> </ul> </li> </ol> <p>This is the same deep idea that appears in modern real-time video papers: the discriminator gives a scalable supervision signal for long-horizon rollout.</p> <h3 id="524-why-latent-adversarial-training-is-stronger-than-pixel-gan-in-this-setting">5.2.4 Why latent adversarial training is stronger than pixel GAN in this setting</h3> <p>Doing the adversarial game in latent/video representation space helps because:</p> <ul> <li>lower dimensionality → cheaper and more stable</li> <li>closer to the model’s native generation space</li> <li>easier to enforce temporal consistency than purely pixel GAN losses</li> </ul> <p>In practice, this makes LADD-style methods much more compatible with:</p> <ul> <li>one-step or few-step video generators</li> <li>autoregressive rollout</li> <li>KV-cache causal transformers</li> </ul> <h3 id="525-main-algorithmic-takeaway">5.2.5 Main algorithmic takeaway</h3> <p>LADD is not just “ADD but for video.” It is the transition from:</p> <ul> <li><strong>paired denoising distillation</strong> to</li> <li><strong>distribution-level adversarial alignment for long-horizon latent rollout</strong></li> </ul> <p>That shift is what makes minute-long streaming generation feasible.</p> <hr/> <h2 id="53-diffratio">5.3 DiffRatio</h2> <p>DiffRatio is one of the most interesting adversarial/distillation hybrids because it gives a <strong>theory-backed correction</strong> to standard diffusion distillation.</p> <h3 id="531-problem-teacher-forced-distillation-has-a-distribution-mismatch">5.3.1 Problem: teacher-forced distillation has a distribution mismatch</h3> <p>In teacher-forced distillation, the student is trained on teacher trajectories / noise states. But at inference, the student rolls out its <strong>own</strong> states.</p> <p>So the student is optimized under the wrong state distribution.</p> <p>This is the exact same failure mode as exposure bias in autoregressive models.</p> <h3 id="532-density-ratio-view-the-core-contribution">5.3.2 Density-ratio view (the core contribution)</h3> <p>DiffRatio frames this mismatch as a <strong>density ratio correction</strong> problem.</p> <p>They derive a correction factor that reweights the distillation objective by a ratio between:</p> <ul> <li>the student-induced trajectory distribution</li> <li>the teacher/reference trajectory distribution</li> </ul> <p>Conceptually:</p> \[\text{corrected objective} \;\sim\; \mathbb{E}_{\text{reference}} \left[ \frac{p_{\text{student}}}{p_{\text{reference}}} \cdot (\text{distillation error}) \right]\] <p>This is the main idea. The student should not just minimize teacher-forced error; it should minimize error under its own rollout distribution.</p> <h3 id="533-how-they-estimate-the-ratio-the-adversarialclassifier-trick">5.3.3 How they estimate the ratio (the adversarial/classifier trick)</h3> <p>The ratio is not known directly, so DiffRatio estimates it with a classifier (discriminator-like network).</p> <p>Train a binary classifier to distinguish samples from:</p> <ul> <li>teacher/reference distribution</li> <li>student rollout distribution</li> </ul> <p>Then convert the classifier logits into a density-ratio estimate. This is the same classic trick behind density-ratio estimation / f-GAN style estimation.</p> <p>So the adversarial component is not only for realism here. It is used as a <strong>distribution correction estimator</strong>.</p> <h3 id="534-final-training-recipe-algorithmically">5.3.4 Final training recipe (algorithmically)</h3> <p>DiffRatio training has two coupled updates:</p> <h4 id="a-ratio-estimator--classifier-update">(A) Ratio estimator / classifier update</h4> <p>Train a classifier to separate:</p> <ul> <li>reference (teacher) samples</li> <li>student-generated samples</li> </ul> <h4 id="b-student-update">(B) Student update</h4> <p>Train the student with:</p> <ul> <li>the original distillation loss</li> <li>reweighted by the estimated density ratio</li> </ul> <p>This directly targets the rollout mismatch that breaks one-step/few-step distillation.</p> <h3 id="535-why-this-is-important">5.3.5 Why this is important</h3> <p>DiffRatio is a more principled answer to the “teacher-student mismatch” than just adding more heuristics.</p> <p>It says:</p> <ul> <li>the issue is not only sharpness/blur</li> <li>the issue is <strong>wrong training distribution</strong></li> <li>adversarial estimation can be used to <strong>fix the measure</strong> the student is trained under</li> </ul> <p>That’s a very strong conceptual bridge between:</p> <ul> <li>diffusion distillation</li> <li>adversarial learning</li> <li>off-policy / covariate-shift correction ideas</li> </ul> <hr/> <h2 id="54-apt-and-the-modern-aapt-extension">5.4 APT (and the modern AAPT extension)</h2> <p>APT (Adversarial Post-Training) is the core one-step video idea: start from a diffusion model, distill/adapt it, then use adversarial training to recover visual quality and speed.</p> <p>AAPT extends APT to <strong>autoregressive real-time video generation</strong> with causal attention + KV cache.</p> <h3 id="541-the-3-stage-training-pipeline-the-important-part">5.4.1 The 3-stage training pipeline (the important part)</h3> <p>The modern APT/AAPT recipe is:</p> <ol> <li><strong>Diffusion adaptation</strong></li> <li><strong>Consistency distillation</strong></li> <li><strong>Adversarial training</strong></li> </ol> <p>This staged design is the key engineering insight. You do not jump directly from diffusion weights to GAN training.</p> <h4 id="stage-1-diffusion-adaptation">Stage 1: Diffusion adaptation</h4> <ul> <li>Convert the pretrained video diffusion transformer into a causal/autoregressive architecture</li> <li>Finetune with diffusion objective under teacher forcing</li> <li>Preserve the diffusion prior while adapting architecture and inputs</li> </ul> <h4 id="stage-2-consistency-distillation">Stage 2: Consistency distillation</h4> <ul> <li>Use consistency distillation as initialization before adversarial training</li> <li>Speeds convergence and stabilizes the later adversarial phase</li> <li>In AAPT, this is explicitly described as following APT</li> </ul> <h4 id="stage-3-adversarial-post-training">Stage 3: Adversarial post-training</h4> <ul> <li>Add a discriminator (initialized from diffusion weights in AAPT-style setups)</li> <li>Train generator + discriminator adversarially</li> <li>This improves frame quality and enables 1-step generation quality recovery</li> </ul> <h3 id="542-why-adversarial-post-training-is-necessary-after-distillation">5.4.2 Why adversarial post-training is necessary after distillation</h3> <p>Consistency / diffusion distillation gets you speed, but often:</p> <ul> <li>oversmooths details</li> <li>weakens texture realism</li> <li>accumulates errors in long rollout</li> </ul> <p>Adversarial post-training fixes exactly that:</p> <ul> <li>sharper details</li> <li>better perceptual realism</li> <li>stronger long-horizon behavior (especially with student-forcing)</li> </ul> <p>This is why APT-style methods matter in practice: they are the bridge from “distilled but soft” to “distilled and actually usable.”</p> <h3 id="543-aapts-autoregressive-extension-the-useful-algorithm-detail">5.4.3 AAPT’s autoregressive extension (the useful algorithm detail)</h3> <p>AAPT extends APT in a way that is super relevant for streaming / interactive video:</p> <h4 id="causal-generator--kv-cache">Causal generator + KV cache</h4> <ul> <li>autoregressive frame generation</li> <li>one latent frame per forward pass (1NFE)</li> <li>reuse KV cache for speed</li> </ul> <h4 id="student-forcing-adversarial-training">Student-forcing adversarial training</h4> <p>During adversarial training:</p> <ul> <li>only the first frame is ground truth</li> <li>afterward, the model feeds back its <strong>own generated frames</strong></li> <li>training behavior matches inference behavior</li> </ul> <p>This is a big deal. It directly attacks exposure error instead of hiding it with teacher forcing.</p> <h3 id="544-discriminator-design-and-loss-in-the-aptaapt-line">5.4.4 Discriminator design and loss in the APT/AAPT line</h3> <p>AAPT uses:</p> <ul> <li>a causal discriminator backbone (same family as the generator)</li> <li>per-frame logits (not only clip-level), enabling parallel multi-duration discrimination</li> <li>relativistic adversarial objective (R3GAN-style)</li> <li>approximated R1/R2 regularization (following APT)</li> </ul> <p>This is a very modern discriminator setup: it is designed for stability and long-video training, not old-school image GAN heuristics.</p> <h3 id="545-long-video-training-trick-important">5.4.5 Long-video training trick (important)</h3> <p>AAPT-style training solves the long-video data problem by:</p> <ul> <li>generating long videos</li> <li>splitting them into short overlapping segments for discriminator evaluation</li> <li>training the discriminator on real vs generated segments</li> </ul> <p>This avoids needing rare long paired ground-truth targets and lets the generator learn long-horizon rollout behavior from ordinary video datasets.</p> <p>That is the killer feature of adversarial post-training in this context: the discriminator supplies supervision at the <strong>distribution level</strong>, which scales to long sequences.</p> <hr/> <h2 id="55-comparisons">5.5 Comparisons</h2> <p>The adversarial line of diffusion distillation is not one thing; it has evolved through three distinct roles:</p> <ol> <li><strong>ADD:</strong> adversarial loss as a perceptual sharpness booster on top of diffusion distillation</li> <li><strong>APT / AAPT / LADD:</strong> adversarial post-training as the main way to make 1-step generators actually work for video and long-horizon rollout</li> <li><strong>DiffRatio:</strong> adversarial classifier as a density-ratio estimator for correcting teacher-student distribution mismatch</li> </ol> <p>That progression is the real story: adversarial methods moved from “make it sharper” to “fix the training distribution and rollout behavior.”</p> <hr/> <h1 id="6-video-generation">6. Video Generation</h1> <h2 id="61-causvid">6.1 CausVid</h2> <h4 id="611-core-idea-causal-ar-diffusion--distillation">6.1.1 Core idea (causal AR diffusion + distillation)</h4> <p>CausVid converts a pretrained <strong>bidirectional video diffusion/flow model</strong> into a <strong>causal autoregressive</strong> generator and then distills it into a <strong>few-step</strong> AR model.</p> <p>The key practical recipe is:</p> <ol> <li><strong>ODE trajectory initialization</strong> (teacher-generated supervision)</li> <li><strong>Asymmetric Distillation with DMD</strong> (teacher is strong multi-step, student is causal few-step)</li> </ol> <p>This is the right framing because causal masking alone is not enough; you first need to preserve the teacher’s dynamics under causal attention, then compress inference.</p> <h4 id="612-stage-0-ode-trajectory-initialization-important">6.1.2 Stage 0: ODE trajectory initialization (important)</h4> <p>CausVid first constructs ODE solution pairs from the pretrained teacher and uses them to initialize the causal student. This is basically a <strong>trajectory-preserving warm start</strong> before adversarial/distribution matching distillation.</p> <p>Why this matters:</p> <ul> <li>Directly jumping to DMD with a randomly causalized student is unstable.</li> <li>ODE-pair initialization makes the student already “look like” the teacher’s denoising trajectory under causal constraints.</li> </ul> <h4 id="613-asymmetric-distillation-with-dmd-main-algorithmic-part">6.1.3 Asymmetric Distillation with DMD (main algorithmic part)</h4> <p>CausVid uses <strong>asymmetric distillation</strong>:</p> <ul> <li><strong>Teacher</strong>: high-quality, many-step, bidirectional model</li> <li><strong>Student</strong>: causal, few-step AR model</li> </ul> <p>Then it applies a <strong>DMD-style distribution matching objective</strong> to train the student generator.</p> <p>A useful mental model:</p> <ul> <li>The teacher provides a high-quality target distribution (and score-like signal / critic guidance).</li> <li>The student is optimized to match that target with drastically fewer denoising steps.</li> </ul> <h4 id="614-causvid-training-algorithm-high-level">6.1.4 CausVid training algorithm (high-level)</h4> <p><strong>Algorithm sketch (CausVid-style):</strong></p> <ol> <li><strong>Initialize student</strong> with causal attention masking.</li> <li><strong>Warm start</strong> using ODE trajectory pairs sampled from the teacher.</li> <li><strong>Train with asymmetric DMD</strong>: <ul> <li>Sample prompts / conditions</li> <li>Generate teacher trajectories / target samples</li> <li>Generate student AR samples with few denoising steps</li> <li>Compute DMD-style generator loss (distribution matching)</li> <li>Update student (and discriminator / critic if using adversarial DMD variant)</li> </ul> </li> </ol> <h4 id="615-why-causvid-matters-in-the-progression">6.1.5 Why CausVid matters in the progression</h4> <p>CausVid is the clean bridge from:</p> <ul> <li>“fast image distillation” ideas (DMD / consistency / one-step)</li> <li>to <strong>causal autoregressive video diffusion</strong></li> </ul> <p>But it still has a train-test mismatch issue if training rollouts are not aligned with the true inference distribution (this is exactly what Self Forcing attacks next).</p> <hr/> <h2 id="62-self-forcing">6.2 Self-Forcing</h2> <h4 id="621-main-idea-fix-the-train-test-gap-the-real-problem">6.2.1 Main idea: fix the train-test gap (the real problem)</h4> <p>Self-Forcing’s central claim is dead-on:</p> <ul> <li><strong>Teacher Forcing (TF)</strong> and <strong>Diffusion Forcing (DF)</strong> train on context distributions that do <strong>not</strong> match the model’s actual inference-time autoregressive rollout.</li> <li>So even if you use a strong distillation loss (DMD / SiD / GAN), you may be matching the <strong>wrong generated distribution</strong>.</li> </ul> <p>Self-Forcing fixes this by doing <strong>autoregressive self-rollout during training</strong> and applying a <strong>holistic distribution matching loss</strong> on the final generated video.</p> <h4 id="622-holistic-post-training-objective-core-formulation">6.2.2 Holistic post-training objective (core formulation)</h4> <p>Instead of local frame-wise supervision, Self-Forcing trains on full autoregressive rollouts.</p> <p>Conceptually, the model defines an autoregressive distribution over video chunks/frames: \(p_\theta(X) = \prod_i p_\theta(x_i \mid x_{&lt;i})\)</p> <p>Then during training:</p> <ol> <li>Roll out the model autoregressively using its <strong>own generated context</strong></li> <li>Get a full generated video $\hat{X}$</li> <li>Apply a <strong>distribution matching loss</strong> on $\hat{X}$ vs target/teacher distribution</li> </ol> <p>This is the key upgrade:</p> <ul> <li>training process now mirrors inference</li> <li>exposure bias is handled directly, not indirectly</li> </ul> <h4 id="623-distribution-matching-losses-used-in-self-forcing">6.2.3 Distribution matching losses used in Self-Forcing</h4> <p>Self-Forcing is not tied to one distillation loss. It supports multiple post-training objectives:</p> <h3 id="a-dmd-style-loss">(a) DMD-style loss</h3> <p>A DMD objective can be applied to the <strong>final self-rolled-out video</strong>:</p> <ul> <li>generator gets a score/critic-driven gradient to move toward target distribution</li> <li>plus an auxiliary regression term for stability (same spirit as DMD2/DMD2-v style recipes)</li> </ul> <p>This gives a <strong>data-free</strong> route when using teacher-generated supervision/signals.</p> <h3 id="b-sid-style-loss">(b) SiD-style loss</h3> <p>Self-Forcing can also use <strong>Score identity Distillation (SiD)</strong> style updates:</p> <ul> <li>estimate/approximate the score mismatch</li> <li>optimize the student rollout distribution accordingly</li> </ul> <p>Again, the important part is not the exact score estimator; it’s that the loss is computed on <strong>true AR self-rollouts</strong>.</p> <h3 id="c-gan-loss-r3gan-style-in-their-implementation">(c) GAN loss (R3GAN-style in their implementation)</h3> <p>They also instantiate Self-Forcing with a GAN objective (R3GAN variant):</p> <ul> <li>discriminator sees real videos vs self-forced generated videos</li> <li>generator learns to produce realistic rollouts</li> </ul> <p>This is actually very natural for Self-Forcing because GANs already train on samples from the generator’s own distribution.</p> <h4 id="624-efficient-training-trick-stochastic-gradient-truncation">6.2.4 Efficient training trick: stochastic gradient truncation</h4> <p>Self-Forcing sounds expensive because training is sequential, but they make it tractable with <strong>gradient truncation</strong>:</p> <ul> <li>Roll out multiple AR steps</li> <li>Backprop only through the <strong>last $k$ rollout steps</strong> (or a truncated subset)</li> <li>Treat earlier generated context as stop-gradient / detached</li> </ul> <p>This drastically cuts memory and compute while preserving the important signal: the model still trains on its own generated context distribution.</p> <p>This is the algorithmic reason Self-Forcing is practical.</p> <h4 id="625-self-forcing-training-algorithm-practical">6.2.5 Self-Forcing training algorithm (practical)</h4> <p><strong>Algorithm sketch (Self-Forcing):</strong></p> <ol> <li>Start from a pretrained causal AR diffusion model (often CausVid-style initialization)</li> <li>For each training iteration: <ul> <li>autoregressively roll out the model to generate a video/chunk sequence</li> <li>optionally use few-step denoising per chunk/frame</li> <li>apply <strong>holistic</strong> loss on final rollout (DMD / SiD / GAN)</li> <li>use <strong>stochastic gradient truncation</strong> for efficiency</li> </ul> </li> <li>Update model</li> <li>(Optional) use rolling KV cache for efficient inference/extrapolation</li> </ol> <h4 id="626-why-self-forcing-is-important">6.2.6 Why Self-Forcing is important</h4> <p>This is the first really strong “RL-like” move in video diffusion post-training:</p> <ul> <li><strong>Pretrain in parallel</strong></li> <li><strong>Post-train sequentially on your own rollout distribution</strong></li> </ul> <p>That shift is the main conceptual contribution, not just the specific loss choice.</p> <hr/> <h2 id="63-transition-matching-distillation">6.3 Transition Matching Distillation</h2> <p>Transition Matching Distillation (TMD) bridges engineering and theory based adaptation of MeanFlow to <strong>video distillation</strong>.</p> <h4 id="631-core-problem-setup">6.3.1 Core problem setup</h4> <p>They want to distill a pretrained video diffusion teacher into a faster student. Direct one-stage distillation is hard in video because:</p> <ul> <li>the space is huge,</li> <li>temporal consistency matters,</li> <li>transformer-based video models make JVP painful (esp. attention kernels/FSDP/context parallelism).</li> </ul> <p>So TMD uses <strong>two stages</strong>.</p> <h4 id="632-stage-1-transition-matching-meanflow-tm-mf">6.3.2 Stage 1: Transition Matching MeanFlow (TM-MF)</h4> <p>This is the key new idea.</p> <p>Instead of applying MeanFlow directly in the original latent/data space, they define an <strong>inner transition</strong> problem and parameterize a conditional inner flow map via average velocity:</p> \[f_\theta(y_s,s,r;m) = y_s + (s-r)u_\theta(y_s,s,r;m)\] <p>where $m$ is a feature extracted from the main backbone.</p> <p>Then they use a MeanFlow-style objective to train this transition head.</p> <p>A very practical (and nontrivial) design choice:</p> <ul> <li>they <strong>reparameterize</strong> the average velocity to stay aligned with the teacher head:</li> </ul> \[u_\theta(y_s,s,r;m) = y_1 - \text{head}_\theta(y_s,s,r;m)\] <p>This is not cosmetic. It keeps the new head close to teacher semantics, which improves stability.</p> <h4 id="633-jvp-issue-and-finite-difference-approximation">6.3.3 JVP issue and finite-difference approximation</h4> <p>This paper is very realistic about systems constraints:</p> <ul> <li>exact JVP is annoying with large-scale video transformer stacks (FlashAttention, FSDP, context parallelism),</li> <li>so they use a <strong>finite-difference approximation</strong> of the JVP.</li> </ul> <p>That’s a practical compromise:</p> <ul> <li>theoretically less clean than exact JVP,</li> <li>but massively easier to integrate into production-grade training code.</li> </ul> <h4 id="634-stage-2-distributional-distillation-objective">6.3.4 Stage 2: Distributional distillation objective</h4> <p>After TM-MF pretraining, they switch to a stronger distillation stage using a VSD/discriminator-style objective (their simplified algorithm shows):</p> \[\mathcal{L} = \text{VSD}(\hat{x}) + \lambda \cdot \text{Discriminator}(\hat{x})\] <p>So the conceptual split is:</p> <ul> <li><strong>Stage 1 (TM-MF):</strong> learn a good transition-aware student parameterization, bootstrap geometry/dynamics</li> <li><strong>Stage 2:</strong> sharpen sample quality and distribution match</li> </ul> <p>This is a strong template for hard domains (video, 3D, multimodal) where pure one-shot distillation is brittle.</p> <h4 id="635-why-tmd-is-strong">6.3.5 Why TMD is strong</h4> <p>TMD wins because it combines both worlds:</p> <ul> <li><strong>trajectory-based structure</strong> (TM-MF / flow-map adaptation)</li> <li><strong>distribution-based distillation</strong> (DMD2-v)</li> </ul> <p>And it does so with an architecture that respects the hierarchical structure of big video diffusion Transformers.</p> <p>That’s why it gets a better speed-quality tradeoff than plain one-step/few-step distillation baselines in video.</p> <hr/> <h1 id="7-new-domains">7. New Domains</h1> <h2 id="71-jit-just-image-transformers">7.1 JiT (Just image Transformers)</h2> <h4 id="711-core-idea-x-prediction-in-pixel-space-transformers">7.1.1 Core idea: x-prediction in pixel-space Transformers</h4> <p>JiT’s key move is to make the Transformer directly predict the <strong>denoised image</strong> (an $x$-like target) instead of predicting noise/velocity in a high-dimensional noisy pixel patch space. The motivation is a <strong>manifold hypothesis</strong> argument: denoised images are lower-dimensional / easier targets for a ViT than noisy velocity fields. This is exactly the ingredient later reused by pMF.</p> <h4 id="712-algorithmic-form-x-pred-with-v-loss">7.1.2 Algorithmic form (x-pred with v-loss)</h4> <p>In the FM-style parameterization used in later papers when discussing JiT, the network outputs $x_\theta(z_t,t)$ and converts it to a velocity prediction: \(v_\theta(z_t,t)=\frac{1}{t}\big(z_t-x_\theta(z_t,t)\big)\) and training still uses the <strong>velocity-space regression loss</strong> (v-loss). This “prediction space vs loss space” decoupling is the important algorithmic pattern that keeps showing up in later distillation papers.</p> <h4 id="713-why-this-matters-for-distillation">7.1.3 Why this matters for distillation</h4> <p>The high-signal point is not “pixel-space diffusion” by itself, but that JiT establishes a recipe:</p> <ul> <li>choose an <strong>easier output space</strong> for the network (denoised/image-like),</li> <li>keep a <strong>stable/known loss space</strong> (velocity/noise/FM target),</li> <li>use a <strong>conversion map</strong> between them.</li> </ul> <p>That recipe is basically the blueprint for pMF and several later “decouple output-space from loss-space” methods. pMF explicitly cites JiT as the x-pred ingredient used to make one-step latent-free generation work.</p> <h4 id="714-practical-note-lossprediction-mismatch">7.1.4 Practical note (loss/prediction mismatch)</h4> <p>The JiT line also motivated later empirical work on <strong>output-space vs loss-space mismatch</strong>: x-pred can work best when paired with a velocity-style loss (or a reweighted variant), while naive direct x-loss can underperform. That design lesson reappears in pMF and MeanFlow-family variants.</p> <hr/> <h2 id="72-drifting">7.2 Drifting</h2> <h4 id="721-core-idea-replace-scorevelocity-field-with-a-drifting-field">7.2.1 Core idea: replace score/velocity field with a drifting field</h4> <p>Drifting proposes a different one-step generative formulation: instead of learning a diffusion/flow velocity or score, it learns an <strong>anti-symmetric kernelized field</strong> (the “drifting field”) and then trains a generator to align with that field. The paper frames this as a new way to get one-step generation with strong quality while keeping a mathematically structured training target.</p> <h4 id="722-algorithmic-object-empirical-drifting-field">7.2.2 Algorithmic object: empirical drifting field</h4> <p>The method builds an empirical field from batch-level statistics (mean-field style / kernelized interactions). In the paper’s notation, the drifting field $V_{\mathrm{drf}}$ is formed from weighted source/target terms, with weights computed from a softmax over kernel similarities. This is the main algorithmic primitive replacing the usual score/velocity target.</p> <h4 id="723-training-loop-distillation-lens">7.2.3 Training loop (distillation lens)</h4> <p>From a diffusion-distillation perspective, the important interpretation is:</p> <ul> <li>the model is trained to match a <strong>teacher-like vector field target</strong>,</li> <li>but the target is <strong>not</strong> a diffusion teacher score/velocity,</li> <li>it is a <strong>kernelized drifting field</strong> built from data/generator samples.</li> </ul> <p>So Drifting is part of the broader one-step trend, but it moves outside standard FM/score-distillation geometry and uses a different transport signal.</p> <hr/> <h2 id="73-pixel-meanflow-pmf">7.3 Pixel MeanFlow (pMF)</h2> <h4 id="731-why-pmf-is-in-this-chapter">7.3.1 Why pMF is in this chapter</h4> <p>pMF is a clean “new-domain” extension because it combines:</p> <ul> <li><strong>one-step MeanFlow-style distillation logic</strong> (JVP / MeanFlow identity),</li> <li><strong>JiT-style x-prediction</strong>,</li> <li>and does it in <strong>raw pixel space</strong> (latent-free).</li> </ul> <p>That makes it a direct example of diffusion/flow distillation ideas being adapted to a harder domain (high-dimensional pixel space).</p> <h4 id="732-core-conversion-from-average-velocity-to-image-like-target">7.3.2 Core conversion: from average velocity to image-like target</h4> <p>pMF defines an image-like field \(x(z_t,r,t)\equiv z_t - t\,u(z_t,r,t),\) where $u(z_t,r,t)$ is the MeanFlow average velocity. This is the key trick: make the network output something denoised/image-like, but still train via MeanFlow’s velocity-space machinery.</p> <p>The paper explicitly motivates this with a generalized manifold argument:</p> <ul> <li>$u$ looks noisy/high-dimensional,</li> <li>$x$ looks denoised/lower-dimensional,</li> <li>so $x$ is easier for the network to model.</li> </ul> <h4 id="733-the-algorithm-this-is-the-important-part">7.3.3 The algorithm (this is the important part)</h4> <p>pMF reparameterizes the network output as: \(u_\theta(z_t,r,t)=\frac{1}{t}\big(z_t-x_\theta(z_t,r,t)\big),\) then plugs that into the improved MeanFlow/iMF JVP compound target: \(V_\theta = u_\theta + (t-r)\cdot \mathrm{JVP}_{\mathrm{sg}},\) and trains with a standard velocity regression: \(\mathcal{L}_{\mathrm{pMF}}=\mathbb{E}\|V_\theta-v\|_2^2.\) So pMF is <strong>x-prediction + MeanFlow JVP distillation + v-loss</strong>. That’s the whole algorithmic identity.</p> <h4 id="734-pseudocode-structure-practical">7.3.4 Pseudocode structure (practical)</h4> <p>The training pseudocode is very explicit:</p> <ol> <li>sample $(t,r)$ and noise,</li> <li>form $z_t=(1-t)x+t\epsilon$,</li> <li>compute $u$ from the x-pred network output,</li> <li>set instantaneous velocity via the $r=t$ case,</li> <li>run a JVP to get $\frac{d}{dt}u$,</li> <li>build $V=u+(t-r)\,\text{stopgrad}(du/dt)$,</li> <li>regress to $(\epsilon-x)$.</li> </ol> <p>This is basically iMF with the network living in image space instead of velocity space.</p> <h4 id="735-distillation-takeaway">7.3.5 Distillation takeaway</h4> <p>pMF is one of the cleanest examples of the modern pattern:</p> <ul> <li><strong>teacher target space</strong> stays physically meaningful (velocity/FM),</li> <li><strong>student output space</strong> becomes easier (image-like/manifold-aligned),</li> <li>a <strong>conversion + JVP identity</strong> bridges the two.</li> </ul> <p>That pattern is exactly what makes these one-step methods actually trainable at high resolution.</p> <hr/> <h2 id="74-repa-representation-alignment-for-generation">7.4 REPA (Representation Alignment for Generation)</h2> <h4 id="741-core-idea-distill-semantics-into-dit-hidden-states">7.4.1 Core idea: distill semantics into DiT hidden states</h4> <p>REPA is not a one-step sampler by itself. It is a <strong>training-time acceleration / regularization</strong> method for diffusion transformers: align early hidden states of the noisy-input diffusion model with representations from a strong pretrained encoder (e.g., DINO/SigLIP/MAE), so the diffusion model learns semantic structure faster. The paper reports substantially faster convergence and better FID.</p> <h4 id="742-algorithmic-pattern-representation-distillation">7.4.2 Algorithmic pattern (representation distillation)</h4> <p>The method is basically:</p> <ul> <li>run the diffusion transformer on noisy input,</li> <li>take hidden states from an early block,</li> <li>project them to a feature space,</li> <li>align them to frozen pretrained visual representations of the clean image.</li> </ul> <p>This is a <strong>teacher-student distillation signal in feature space</strong>, not in score/velocity space. It complements the usual denoising loss rather than replacing it.</p> <h4 id="743-why-it-matters-in-a-distillation-chapter">7.4.3 Why it matters in a distillation chapter</h4> <p>REPA expands “distillation” beyond sampling-step distillation:</p> <ul> <li>classic diffusion distillation compresses <strong>inference trajectories</strong>,</li> <li>REPA distills <strong>representation priors</strong> into the denoiser backbone.</li> </ul> <p>That’s a different axis of acceleration: faster training / better sample efficiency, not just fewer NFEs. It also composes well with other diffusion recipes, which is why it shows up as a practical building block.</p> <hr/> <h2 id="75-latent-forcing">7.5 Latent Forcing</h2> <h4 id="751-core-idea-distill-a-latent-planner-into-a-pixel-refiner">7.5.1 Core idea: distill a latent planner into a pixel refiner</h4> <p>Latent Forcing is a two-stage autoregressive video generation setup that explicitly splits generation into:</p> <ol> <li>a <strong>base latent diffusion</strong> model (semantic / long-horizon structure),</li> <li>a <strong>pixel-space refiner diffusion</strong> model (visual details),</li> </ol> <p>with a distillation-like forcing mechanism that conditions the refiner on latent predictions. This is basically “semantic planning in latent space, rendering in pixel space.”</p> <h4 id="752-key-algorithm-trick-two-time-conditioning">7.5.2 Key algorithm trick: two-time conditioning</h4> <p>The refiner needs to know <strong>both</strong>:</p> <ul> <li>how noisy the current pixel sample is,</li> <li>and how much to trust the latent prediction.</li> </ul> <p>So they add a <strong>second time/noise embedding</strong> (for latent-conditioning time) into the U-Net conditioning stack. This is the core algorithmic adaptation and is the reason the method works across different latent/pixel noise levels.</p> <h4 id="753-training-objective">7.5.3 Training objective</h4> <p>Training optimizes a weighted sum of:</p> <ul> <li>the base latent diffusion loss,</li> <li>the pixel refiner denoising loss.</li> </ul> <p>The paper writes this as a joint objective (Eq. 1), with a scalar weight balancing the base and refiner parts. This is the cleanest way to read it: <strong>joint distillation/training across two domains (latent + pixel)</strong>.</p> <h4 id="754-timestep-schedule-is-the-secret-sauce">7.5.4 Timestep schedule is the secret sauce</h4> <p>They do not use identical timesteps for base and refiner. Instead, they derive/schedule a latent-conditioning timestep as a function of the global/pixel timestep (their Eq. 4-style schedule, with clipping / constraints in later equations). This prevents the refiner from being conditioned on latent predictions that are unrealistically clean/noisy relative to the current pixel denoising stage.</p> <h4 id="755-distillation-takeaway">7.5.5 Distillation takeaway</h4> <p>Latent Forcing is a domain-transfer distillation recipe:</p> <ul> <li>distill <strong>global structure</strong> into a compact latent process,</li> <li>force a pixel model to consume that structure reliably,</li> <li>synchronize the two with explicit noise-time coupling.</li> </ul> <p>This is exactly the kind of “new domain” extension diffusion distillation needed for long-horizon video.</p> <hr/> <h2 id="76-unified-latents-ul">7.6 Unified Latents (UL)</h2> <h4 id="761-core-idea-train-the-latent-space-for-diffusion-not-before-diffusion">7.6.1 Core idea: train the latent space <em>for</em> diffusion, not before diffusion</h4> <p>UL reframes latent learning as a <strong>jointly trained system</strong>:</p> <ul> <li>encoder produces latents,</li> <li>a diffusion prior regularizes/model these latents,</li> <li>a diffusion decoder reconstructs the data.</li> </ul> <p>The punchline is that they explicitly link encoder noise to the prior’s minimum noise level, which gives a principled handle on latent information capacity (bitrate). This is a big deal because previous latent pipelines often tuned KL strength heuristically.</p> <h4 id="762-algorithm-1-joint-training">7.6.2 Algorithm 1 (joint training)</h4> <p>UL’s training algorithm is clean and practical:</p> <ol> <li>encode $x$ to a clean latent $z_{\text{clean}}$,</li> <li>add diffusion noise in latent space and train a <strong>latent prior diffusion loss</strong> $L_z$,</li> <li>sample a slightly noisy latent $z_0$ and noisy image $x_t$,</li> <li>train a <strong>decoder diffusion loss</strong> $L_x$ conditioned on $z_0$,</li> <li>optimize the combined objective $L = L_z + L_x$.</li> </ol> <p>This is not post-hoc distillation; it is <strong>co-training</strong> the latent representation and the diffusion models so the latent becomes diffusion-friendly by construction.</p> <h4 id="763-sampling-algorithm-factorized-generation">7.6.3 Sampling algorithm (factorized generation)</h4> <p>Sampling is also explicitly two-stage:</p> <ol> <li>sample latent noise $z_1$,</li> <li>denoise to a latent $z_0$ with the latent prior,</li> <li>sample image noise $x_1$,</li> <li>denoise with the decoder conditioned on $z_0$.</li> </ol> <p>This factorization is the deployment-side analog of the training split above.</p> <h4 id="764-why-ul-belongs-in-distillationnew-domains">7.6.4 Why UL belongs in “distillation/new domains”</h4> <p>UL is a latent-learning framework, but it matters for diffusion distillation because it changes the upstream problem:</p> <ul> <li>if the latent is easier to model, <strong>few-step / distilled samplers become easier downstream</strong>;</li> <li>UL gives a principled way to control this via latent noise/bitrate rather than ad hoc KL tuning.</li> </ul> <p>So UL is not trajectory distillation, but it is absolutely part of the modern diffusion efficiency stack.</p> <hr/> <h2 id="77-unifying-pattern-across-these-new-domain-methods">7.7 Unifying pattern across these “new-domain” methods</h2> <p>The common pattern across JiT, pMF, Latent Forcing, REPA, and UL is:</p> <ol> <li><strong>Choose an easier target/domain</strong> <ul> <li>image-like target (JiT, pMF),</li> <li>latent structure target (Latent Forcing, UL),</li> <li>pretrained semantic features (REPA).</li> </ul> </li> <li><strong>Keep a stable training signal</strong> <ul> <li>velocity/FM loss (JiT, pMF),</li> <li>standard denoising losses (Latent Forcing, UL),</li> <li>feature alignment regularization (REPA).</li> </ul> </li> <li><strong>Bridge them with an explicit conversion/conditioning mechanism</strong> <ul> <li>$x \to v$ (JiT),</li> <li>$x \to u \to V$ + JVP (pMF),</li> <li>coupled latent/pixel timesteps (Latent Forcing),</li> <li>projection heads for feature alignment (REPA),</li> <li>encoder-noise/prior-noise linkage (UL).</li> </ul> </li> </ol> <p>That’s the real story of diffusion distillation in new domains: not just “fewer NFEs,” but <strong>engineering the target space so distillation/training is actually learnable</strong>.</p> <hr/> <h1 id="8-manifold">8. Manifold</h1> <h2 id="81-riemannian-manifold">8.1 Riemannian Manifold</h2> <h3 id="811-why-manifold-aware-distillation-is-different">8.1.1 Why manifold-aware distillation is different</h3> <p>In Euclidean flow matching / flow-map distillation, we can interpolate with linear paths and add vectors freely. On a manifold $\mathcal{M}$, both assumptions break:</p> <ul> <li>the state must stay on $\mathcal{M}$,</li> <li>velocities must live in the correct tangent space $T_x\mathcal{M}$,</li> <li>distances / residuals should be measured with the Riemannian metric $g$ (or geodesic distance).</li> </ul> <p>So the whole distillation stack (interpolant, velocity parameterization, self-distillation loss) has to be geometry-aware.</p> <hr/> <h3 id="812-riemannian-flow--geodesic-interpolant-the-core-setup">8.1.2 Riemannian flow + geodesic interpolant (the core setup)</h3> <p>The probability flow ODE / continuity equation becomes the Riemannian version:</p> \[\partial_t x_t = v_t(x_t), \qquad \partial_t \rho_t(x) = -\operatorname{div}_g\!\big(\rho_t(x)\, v_t(x)\big).\] <p>The key change is the interpolant. Instead of a linear interpolant, use the geodesic interpolant:</p> \[I_t(x_0,x_1) = \exp_{x_0}\!\big(\alpha_t \log_{x_0}(x_1)\big), \qquad \alpha_0=0,\; \alpha_1=1.\] <p>This keeps the path on the manifold by construction.</p> <hr/> <h3 id="813-generalized-flow-map-gfm-parameterization">8.1.3 Generalized Flow Map (GFM) parameterization</h3> <p>The generalized flow map is a time-to-time map on the manifold:</p> \[X^\theta_{s,t} : \mathcal{M} \to \mathcal{M}, \qquad X^\theta_{s,t}(x_s) \approx x_t.\] <p>To make the learned vector field valid on a manifold, the model output is projected onto the tangent space:</p> \[v^\theta_{s,t}(p) = \operatorname{proj}_{T_p\mathcal{M}} \, f_\theta(s,t,p).\] <p>This is the important implementation detail. Without tangent projection, the model can produce off-manifold directions and the losses become geometrically invalid.</p> <hr/> <h3 id="814-the-manifold-distillation-objective-algorithmic-form">8.1.4 The manifold distillation objective (algorithmic form)</h3> <p>The GFM paper defines a combined objective:</p> \[\mathcal{L}(\theta) = \mathcal{L}_{\mathrm{RFM}}(\theta) + \mathcal{L}_{\mathrm{GFM\text{-}SD}}(\theta).\] <ul> <li>$\mathcal{L}_{\mathrm{RFM}}$: the Riemannian flow-matching term (diagonal / teacher-like velocity supervision)</li> <li>$\mathcal{L}_{\mathrm{GFM\text{-}SD}}$: a self-distillation term that teaches the off-diagonal flow map behavior (few-step jump structure)</li> </ul> <p>This is exactly the same high-level recipe as Euclidean flow-map training, but all operators are ported to manifold geometry.</p> <hr/> <h3 id="815-three-manifold-self-distillation-losses-the-actual-algorithm-targets">8.1.5 Three manifold self-distillation losses (the actual algorithm targets)</h3> <h4 id="a-g-lsd-generalized-lagrangian-self-distillation">(A) G-LSD: Generalized Lagrangian Self-Distillation</h4> <p>This enforces consistency between the time derivative of the learned flow map and the learned diagonal velocity, measured in the Riemannian norm:</p> \[\mathcal{L}_{\mathrm{G\text{-}LSD}}(\theta) = \mathbb{E}\Big[ \big\| \partial_t X^\theta_{s,t}(I_s) - v^\theta_{t,t}\!\big(X^\theta_{s,t}(I_s)\big) \big\|_g^2 \Big].\] <p>This is the cleanest manifold analogue of the Euclidean Lagrangian self-distillation loss.</p> <hr/> <h4 id="b-g-esd-generalized-eulerian-self-distillation">(B) G-ESD: Generalized Eulerian Self-Distillation</h4> <p>This is the manifold version of the Eulerian PDE residual, using the differential (Jacobian pushforward) of the flow map:</p> \[\mathcal{L}_{\mathrm{G\text{-}ESD}}(\theta) = \mathbb{E}\Big[ \big\| \partial_s X^\theta_{s,t}(x_s) + d(X^\theta_{s,t})_{I_s}\!\left[v^\theta_{s,s}(I_s)\right] \big\|_g^2 \Big].\] <p>This is more PDE-like and elegant, but in practice it can be trickier numerically (more derivative structure).</p> <hr/> <h4 id="c-g-psd-generalized-progressive-self-distillation">(C) G-PSD: Generalized Progressive Self-Distillation</h4> <p>This enforces the semigroup property directly using geodesic distance:</p> \[\mathcal{L}_{\mathrm{G\text{-}PSD}}(\theta) = \mathbb{E}\Big[ d_g^2\!\big( X^\theta_{s,t}(I_s), X^\theta_{u,t}(X^\theta_{s,u}(I_s)) \big) \Big].\] <p>This one is derivative-free (no time derivative or spatial Jacobian of the model in the loss), which makes it the simplest to implement.</p> <hr/> <h3 id="816-manifold-gfm-training-algorithm-practical-loop">8.1.6 Manifold GFM training algorithm (practical loop)</h3> <p><strong>Training loop (same structure as your earlier chapters, but manifold-aware):</strong></p> <ol> <li>Sample a batch of: <ul> <li>times $t_i \sim \mathcal{T}$,</li> <li>earlier times $s_i \sim \mathcal{S}\mid t_i$,</li> <li>endpoint pairs $(x_0^i, x_1^i) \sim \rho$ (a coupling).</li> </ul> </li> <li>Build manifold interpolant points: \(x_s^i = I(x_0^i, x_1^i, s_i)\) and interpolant tangent targets: \(u_s^i = \partial_s I(x_0^i, x_1^i, s_i).\)</li> <li>Compute the Riemannian flow-matching loss $\widehat{\mathcal{L}}_{\mathrm{RFM}}$.</li> <li>Compute one self-distillation term $\widehat{\mathcal{L}}_{\mathrm{GFM\text{-}SD}}$: <ul> <li>G-LSD, or</li> <li>G-ESD, or</li> <li>G-PSD.</li> </ul> </li> <li>Optimize: \(\widehat{\mathcal{L}}_{\mathrm{RFM}} + \widehat{\mathcal{L}}_{\mathrm{GFM\text{-}SD}}.\)</li> <li>Repeat until convergence.</li> </ol> <p>Output is the learned flow map $X^\theta_{s,t}$, which supports few-step (including 1-step) generation on the manifold.</p> <hr/> <h3 id="817-why-this-matters-for-distillation-the-high-signal-takeaway">8.1.7 Why this matters for distillation (the high-signal takeaway)</h3> <p>This is the geometric version of the same big idea behind modern diffusion/flow distillation:</p> <ul> <li><strong>Teacher-like local supervision</strong> on the diagonal ($s=t$),</li> <li>plus <strong>self-distilled jump constraints</strong> off-diagonal ($s\neq t$),</li> <li>so you can do <strong>few-step transport</strong> instead of expensive ODE integration.</li> </ul> <p>The manifold paper shows this is not just a Euclidean trick: it works on tori, spheres, $SO(3)$, and hyperbolic geometry, with strong 1-NFE gains (especially G-LSD).</p> <hr/> <h2 id="82-optimal-transport">8.2 Optimal Transport</h2> <h3 id="821-why-ot-belongs-in-a-distillation-chapter">8.2.1 Why OT belongs in a distillation chapter</h3> <p>Optimal transport (OT) is the geometry-first version of distribution transport. It matters here because diffusion/flow distillation quality is heavily affected by the <strong>interpolation path</strong> and <strong>coupling</strong> between source and target samples.</p> <p>OT gives a principled way to choose both:</p> <ul> <li>a coupling (who should be paired with whom),</li> <li>and a path (how mass should move).</li> </ul> <p>That usually means straighter, lower-action transports, which is exactly what you want for few-step generation.</p> <hr/> <h3 id="822-dynamic-ot-benamou-brenier-as-a-velocity-learning-objective">8.2.2 Dynamic OT (Benamou-Brenier) as a velocity-learning objective</h3> <p>The dynamic OT formulation says the squared Wasserstein-2 distance is the minimum kinetic energy over all density/velocity paths connecting $\alpha_0$ and $\alpha_1$:</p> \[W_2^2(\alpha_0,\alpha_1) = \inf_{(\alpha_t,v_t)} \int_0^1 \int_{\mathbb{R}^d} \|v_t(x)\|^2 \, d\alpha_t(x)\, dt\] <p>subject to the continuity equation</p> \[\partial_t \alpha_t + \operatorname{div}(\alpha_t v_t)=0, \qquad \alpha_{t=0}=\alpha_0,\;\alpha_{t=1}=\alpha_1.\] <p>This is the cleanest algorithmic bridge to flow models: it is literally a constrained velocity-learning problem.</p> <hr/> <h3 id="823-the-ot-interpolation-path-what-changes-vs-standard-fm">8.2.3 The OT interpolation path (what changes vs standard FM)</h3> <p>If $T$ is the optimal Monge map, the OT geodesic is</p> \[\alpha_t = \big((1-t)\operatorname{Id}+tT\big)_\# \alpha_0,\] <p>with particle motion along straight lines from $x$ to $T(x)$ and velocity</p> \[v_t\big((1-t)x+tT(x)\big)=T(x)-x.\] <p>This is <em>not</em> the same as standard flow-matching interpolation, which uses independent pairs $(X_0,X_1)\sim \alpha_0\otimes\alpha_1$. That distinction is huge:</p> <ul> <li><strong>Flow matching</strong>: couples independent endpoints (convolution-like path)</li> <li><strong>OT</strong>: couples each point to its optimal destination (geodesic path)</li> </ul> <p>For distillation, OT-style couplings tend to produce a cleaner transport target, which generally helps few-step / one-step objectives.</p> <hr/> <h3 id="824-flow-matching-vs-ot-the-exact-algorithmic-difference">8.2.4 Flow Matching vs OT (the exact algorithmic difference)</h3> <p>Standard FM solves an unconstrained least-squares regression for the velocity on a <em>prescribed</em> interpolation:</p> \[\min_{(v_t)_t} \int_0^1 \mathbb{E}\Big[ \|v_t((1-t)X_0+tX_1)-(X_1-X_0)\|^2 \Big]dt.\] <p>OT instead optimizes over the path itself (and coupling), with a continuity constraint and kinetic-energy objective.</p> <p>That means:</p> <ul> <li>FM is easier to train directly (plain regression),</li> <li>OT gives better geometry (but harder optimization),</li> <li>modern distillation methods often live in between by using OT-informed couplings/interpolants and FM-style regression.</li> </ul> <hr/> <h3 id="825-ot-aware-distillation-recipe-practical-synthesis">8.2.5 OT-aware distillation recipe (practical synthesis)</h3> <p>This is the useful way to think about OT in a distillation pipeline:</p> <h4 id="option-a-ot-informed-pairing--fm--flow-map-distillation">Option A: OT-informed pairing + FM / flow-map distillation</h4> <ol> <li>Build a better coupling $\rho(x_0,x_1)$ (ideally OT-like).</li> <li>Use that coupling in the interpolant / training pairs.</li> <li>Train your velocity or flow map with standard FM + self-distillation losses.</li> </ol> <p>This is already enough to make the distillation target easier.</p> <h4 id="option-b-ot-geodesic-path-as-the-teacher-path">Option B: OT geodesic path as the teacher path</h4> <ol> <li>Use OT geodesic interpolation (or an approximation to it).</li> <li>Train a velocity field on that path.</li> <li>Distill into a flow map / one-step map (Lagrangian / Eulerian / progressive style).</li> </ol> <p>This is conceptually the cleanest “OT + distillation” stack.</p> <hr/> <h3 id="826-why-ot-is-especially-relevant-for-one-step--few-step-generation">8.2.6 Why OT is especially relevant for one-step / few-step generation</h3> <p>Few-step generation hates curved, high-variance trajectories. OT minimizes transport action, so it tends to produce shorter, more coherent paths.</p> <p>That directly helps distillation because self-distillation losses are trying to compress a continuous transport process into a small number of jumps. If the teacher path is already close to a geodesic, compression is easier.</p> <p>This is the same reason OT-inspired couplings keep showing up across modern diffusion/flow papers.</p> <hr/> <h3 id="827-manifold--ot-the-next-frontier">8.2.7 Manifold + OT (the next frontier)</h3> <p>The strongest long-term direction is combining:</p> <ul> <li><strong>manifold-aware geometry</strong> (Riemannian metrics, geodesics, tangent constraints),</li> <li>with <strong>OT-aware couplings / geodesic transport</strong>,</li> <li>and <strong>flow-map distillation</strong> for few-step inference.</li> </ul> <p>That gives a unified view:</p> <ul> <li>Geometry decides the valid space and metric.</li> <li>OT decides the “best” transport path/coupling.</li> <li>Distillation compresses that path into fast inference.</li> </ul> <p>That is basically the right conceptual stack for generative modeling on structured domains (rotations, spheres, articulated states, meshes, trajectories, etc.).</p>]]></content><author><name></name></author><category term="research-survey"/><category term="diffusion"/><category term="survey"/><summary type="html"><![CDATA[Recent developments of diffusion distillation techniques]]></summary></entry><entry><title type="html">Recent Methods Utilizing Diffusion in Graphics</title><link href="https://abecid.github.io/blog/2026/graphics-diffusion/" rel="alternate" type="text/html" title="Recent Methods Utilizing Diffusion in Graphics"/><published>2026-02-22T14:24:00+00:00</published><updated>2026-02-22T14:24:00+00:00</updated><id>https://abecid.github.io/blog/2026/graphics-diffusion</id><content type="html" xml:base="https://abecid.github.io/blog/2026/graphics-diffusion/"><![CDATA[]]></content><author><name></name></author><category term="research-survey"/><category term="diffusion"/><category term="graphics"/><category term="survey"/><summary type="html"><![CDATA[Recent developments of diffusion in graphics methods]]></summary></entry><entry><title type="html">Spatial Memory in Video Diffusion Survey</title><link href="https://abecid.github.io/blog/2026/spatial-video/" rel="alternate" type="text/html" title="Spatial Memory in Video Diffusion Survey"/><published>2026-02-21T14:24:00+00:00</published><updated>2026-02-21T14:24:00+00:00</updated><id>https://abecid.github.io/blog/2026/spatial-video</id><content type="html" xml:base="https://abecid.github.io/blog/2026/spatial-video/"><![CDATA[]]></content><author><name></name></author><category term="research-survey"/><category term="3d"/><category term="diffusion"/><category term="survey"/><category term="video_generation"/><summary type="html"><![CDATA[Recent developments of using spatial memory in video diffusion models]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://abecid.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://abecid.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://abecid.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>Learn more:Learn more:Learn more:Learn more:Learn more:Learn more:May 14, 2024 We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><category term="external-posts"/><category term="google"/><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://abecid.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://abecid.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://abecid.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author><category term="external-posts"/><category term="medium"/></entry></feed>