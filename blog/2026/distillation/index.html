<!DOCTYPE html> <html lang=""> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Recent Flow-Map Distillation Methods | Adam Abecid </title> <meta name="author" content="Adam Abecid"> <meta name="description" content="Recent developments of diffusion distillation techniques"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://abecid.github.io/blog/2026/distillation/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Adam Abecid </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Recent Flow-Map Distillation Methods</h1> <p class="post-meta"> Created on February 20, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/diffusion"> <i class="fa-solid fa-hashtag fa-sm"></i> diffusion</a>   <a href="/blog/tag/survey"> <i class="fa-solid fa-hashtag fa-sm"></i> survey</a>   ·   <a href="/blog/category/research-survey"> <i class="fa-solid fa-tag fa-sm"></i> research-survey</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Meta FlowMap</p> <h1 id="flow-matching-to-flow-maps-to-distillation-a-deep-dive">Flow Matching to Flow Maps to Distillation: A Deep Dive</h1> <p>(MeanFlow, Flow Map Self-Distillation, Stochastic/Meta Flow Maps, TMD, TVM)</p> <h2 id="table-of-content">Table of Content</h2> <ol> <li><a href="#1-foundations-flow-matching-vs-flow-maps">Foundations: Flow Matching and Flow Maps</a></li> <li><a href="#2-meanflow">MeanFlow</a></li> <li><a href="#3-freeflowmap">FreeFlowMap</a></li> <li><a href="#4-meta-flow-map">Meta FlowMap</a></li> <li><a href="#5-transition-matching-distillation">Transition Matching Distillation</a></li> <li><a href="#6-terminal-velocity-matching">Terminal Velocity Matching</a></li> </ol> <h2 id="overview">Overview</h2> <p>Recent generative modeling utilize and develop upon flow maps and jvp based distillation techniques to reduce the number of function evaluations during inference.</p> <h2 id="summary">Summary</h2> <p>The modern progression is:</p> <ol> <li> <strong>Flow Matching (FM)</strong> learns the <strong>instantaneous velocity field</strong>.</li> <li> <strong>Flow-map methods</strong> learn <strong>time-to-time transport maps</strong> (or average velocities), which are much more compatible with <strong>1-step / few-step generation</strong>.</li> <li> <strong>MeanFlow</strong> gives a clean, derivation-first way to train a two-time flow-map model via a <strong>JVP-based identity</strong> (no extra consistency axiom).</li> <li> <strong>Flow Map Distillation without Data / self-distillation</strong> (the “FreeFlowMap / How to build a consistency model” line) points out <strong>teacher-data mismatch</strong> and replaces data-dependent distillation with <strong>prior-only self-generated supervision</strong>, then adds a <strong>correction objective</strong> to fix distribution drift.</li> <li> <strong>Stochastic Flow Maps / Meta Flow Maps</strong> generalize flow maps to <strong>stochastic transitions</strong> and derive <strong>diagonal + consistency</strong> objectives from conditional posterior structure; this is the right lens when deterministic flow maps are too restrictive.</li> <li> <strong>TMD</strong> ports MeanFlow-style ideas into <strong>video distillation</strong>, using a <strong>transition-matching MeanFlow pretraining stage</strong> + a second-stage distributional distillation objective.</li> <li> <strong>TVM</strong> shifts the target from “match initial/local velocity” to <strong>match terminal velocity</strong>, and gives a more principled guarantee (explicit <strong>2-Wasserstein upper bound</strong>), while also exposing a key practical systems issue: <strong>JVP through transformer attention</strong>.</li> </ol> <hr> <h2 id="1-foundations-flow-matching-and-flow-maps">1) Foundations: Flow Matching and Flow Maps</h2> <h3 id="11-flow-matching-instantaneous-field">1.1 Flow Matching (instantaneous field)</h3> <p>Generative models learn to transport probability ditribution from prior, $p_{0}$, to a data distribution, $p_{1}$.</p> <p>Classic flow matching learns a vector field \(u(x,t)\) that defines an ODE trajectory. Sampling is ODE integration from noise to data.</p> <p>There are several bottlenecks to this approach: if the learned trajectory is curved, a decent solver (Euler, Heun) is required and many NFEs.</p> <p>The flow map perspective directly targets: \(\phi_u(x_t, t, s)\) which maps a state at time (t) to time (s), instead of learning only the local tangent.</p> <p>Recent methods have emerged developing upon this flow map formulation for fewer step, student-teacher, data-free distillation families.</p> <hr> <h2 id="2-meanflow">2) MeanFlow</h2> <h3 id="21-average-velocity-instead-of-instantaneous-velocity">2.1 Average velocity instead of instantaneous velocity</h3> <p>MeanFlow defines an <strong>average velocity</strong> \(u(z_t, r, t)\) over the interval ([r,t]), so that the displacement is \((t-r)u(z_t,r,t).\)</p> <h3 id="22-the-meanflow-identity-the-central-derivation">2.2 The MeanFlow Identity (the central derivation)</h3> <p>Start from the definition: \((t-r)u(z_t,r,t) = \int_r^t v(z_\tau,\tau)\, d\tau\)</p> <p>Differentiate both sides with respect to (t) (holding (r) fixed). By product rule + FTC: \(u(z_t,r,t) + (t-r)\frac{d}{dt}u(z_t,r,t) = v(z_t,t)\)</p> <p>Rearrange: \(u(z_t,r,t) = v(z_t,t) - (t-r)\frac{d}{dt}u(z_t,r,t)\)</p> <p>This is the <strong>MeanFlow Identity</strong>.</p> <h3 id="23-why-this-matters">2.3 Why this matters</h3> <ul> <li>It rewrites an intractable target (the average velocity integral) into a trainable target using: <ul> <li>instantaneous velocity $v$ (available from FM-style interpolation),</li> <li>a total derivative term computed via <strong>JVP</strong>.</li> </ul> </li> <li>No explicit consistency regularizer is imposed by fiat.</li> <li>The consistency-like structure falls out from the definition of average velocity.</li> </ul> <h3 id="24-meanflow-loss-core-training-objective">2.4 MeanFlow loss (core training objective)</h3> <p>Parameterize $u_\theta(z_t,r,t)$, and regress to the identity-induced target: \(\mathcal{L}_{\text{MeanFlow}}(\theta) = \mathbb{E}\left[ \left\| u_\theta(z_t,r,t) - \operatorname{sg}(u_{\text{tgt}}) \right\|_2^2 \right]\)</p> <p>with \(u_{\text{tgt}} = v_t - (t-r)\left(v_t \,\partial_z u_\theta + \partial_t u_\theta\right)\)</p> <p>where the total derivative is implemented through a JVP along tangent ((v_t, 0, 1)).</p> <p>The stop-gradient is not just a random hack:</p> <ul> <li>it avoids double backprop through the JVP term,</li> <li>but zero loss still implies the identity is satisfied.</li> </ul> <h3 id="25-reduction-to-standard-fm">2.5 Reduction to standard FM</h3> <p>If you set (r=t), the correction term vanishes: \(u_{\text{tgt}} = v_t\) so MeanFlow collapses to standard FM. This is the cleanest way to think about MeanFlow:</p> <ul> <li><strong>FM = the degenerate diagonal case</strong></li> <li><strong>MeanFlow = FM + propagation off the diagonal in two-time space</strong></li> </ul> <h3 id="26-sampling">2.6 Sampling</h3> <p>Once you learn the average velocity, sampling is just: \(z_r = z_t - (t-r)u(z_t,r,t)\)</p> <p>For 1-step: \(z_0 = z_1 - u(z_1,0,1),\quad z_1 \sim p_{\text{prior}}\)</p> <p>That’s the whole point: replace time integration with a learned displacement.</p> <hr> <h2 id="3-freeflowmap">3) FreeFlowMap</h2> <p>Flow Map Distillation Without Data</p> <h2 id="31-teacher-data-mismatch-the-hidden-bug-in-many-distillation-pipelines">3.1 Teacher-data mismatch (the hidden bug in many distillation pipelines)</h2> <p>Traditional flow-map distillation often samples intermediate states (x_t) from an <strong>external dataset distribution</strong> and supervises the student using teacher velocities at those states.</p> <p>But the student is supposed to reproduce the teacher’s <strong>sampling process</strong>, i.e. the trajectory distribution induced by the teacher from the prior.</p> <p>Supervision states coming from a mismatched distribution results in a <strong>teacher-data mismatch</strong>:</p> <ul> <li>the student is trained on states that are not on the teacher’s true rollout distribution,</li> <li>more augmentation can worsen it,</li> <li>student quality degrades.</li> </ul> <p>This is a deep point because it says the standard “distill on data” recipe is fundamentally misaligned with the actual objective (imitate the teacher sampler).</p> <h2 id="32-prior-only--self-generated-flow-map-objective">3.2 Prior-only / self-generated flow-map objective</h2> <p>Supervise entirely from the prior and the student’s own generated states.</p> <p>They derive a sufficient optimality condition leading to a loss of the form \(\mathcal{L}_{\text{pred}} = \mathbb{E}_{z,\delta} \left[ \|F_\theta(z,\delta)-\operatorname{sg}(u_{\text{target}})\|^2 \right]\)</p> <p>with \(u_{\text{target}} = u(f_\theta(z,\delta),1-\delta) - \delta \,\partial_\delta F_\theta(z,\delta)\)</p> <p>The key interpretation:</p> <ul> <li>(f_\theta(z,\delta)) defines the student’s current trajectory,</li> <li>(\partial_\delta f_\theta) is the <strong>student’s generating velocity</strong>,</li> <li>the loss is equivalent to aligning the student generating velocity with the teacher field: \(\partial_\delta f_\theta \approx u(f_\theta(z,\delta),1-\delta)\)</li> </ul> <p>So the student learns to <strong>ride the teacher vector field along its own generated path</strong>, starting from pure noise, no dataset needed.</p> <p>This is the right fix for teacher-data mismatch.</p> <h2 id="33-gradient-view">3.3 Gradient view</h2> <p>They explicitly write the optimization gradient in terms of a velocity mismatch \(\Delta v_{G,u} = v_G - u\) which is great because it makes gradient weighting / normalization tricks easier to reason about.</p> <p>This is one of those “small” presentation choices that actually matters in practice.</p> <h2 id="34-correction-objective-fixing-distribution-drift-not-just-local-velocity">3.4 Correction objective (fixing distribution drift, not just local velocity)</h2> <p>Here’s the catch: aligning generating velocity locally is necessary, but in finite-capacity/discrete training the generated distribution can still drift.</p> <p>So they add a <strong>correction objective</strong> motivated from minimizing a KL term over intermediate marginals and then translating score mismatch into <strong>velocity mismatch</strong> (via the score–velocity equivalence for linear interpolants).</p> <p>This yields a gradient proportional to \(\nabla_\theta \; \mathbb{E}_{z,n,r} \left[ F_\theta(z,1)^\top \operatorname{sg} \left( v_N(I_r(f_\theta(z,1),n),r) - u(I_r(f_\theta(z,1),n),r) \right) \right]\)</p> <p>where:</p> <ul> <li>(u) is the teacher marginal velocity,</li> <li>(v_N) is the student-induced <strong>noising</strong> marginal velocity,</li> <li>(I_r(\cdot,\cdot)) is the interpolation to intermediate time (r).</li> </ul> <p>Intuition:</p> <ul> <li>the prediction loss aligns the <strong>student’s forward/generating flow</strong> </li> <li>the correction loss aligns the <strong>student-induced noising marginals</strong> with the teacher’s marginals</li> </ul> <p>That is a nice bidirectional correction mechanism.</p> <hr> <h2 id="4-meta-flow-maps">4) Meta Flow Maps</h2> <p>Meta flow maps correspond to a <strong>stochastic flow map</strong>, which is important because deterministic flow maps are too rigid.</p> <h3 id="41-why-stochastic-flow-maps">4.1 Why stochastic flow maps?</h3> <p>Deterministic flow-map learning works if the transport map is the right object. But for many diffusion-like processes, especially when you want richer uncertainty handling, a <strong>stochastic transition kernel</strong> \(\kappa_{t,s}(z_t, z_s)\) is the right object.</p> <p>The paper frames this using:</p> <ul> <li><strong>marginal consistency</strong></li> <li><strong>conditional consistency</strong></li> <li> <table> <tbody> <tr> <td>a family of posterior conditionals (p_{1</td> <td>t})</td> </tr> </tbody> </table> </li> <li>and a diagonal supervision view</li> </ul> <p>The important conceptual upgrade is:</p> <ul> <li>instead of only learning deterministic trajectories,</li> <li>learn a transition operator consistent with the stochastic process structure.</li> </ul> <h3 id="42-the-diagonal-condition-same-role-as-fm-diagonal-supervision">4.2 The diagonal condition (same role as FM diagonal supervision)</h3> <p>They derive that on the diagonal: \(\kappa_{t,t}(z_t, z_1) = p_{1|t}(z_1 \mid z_t)\)</p> <p>This is the stochastic analogue of “when (r=t), your two-time object must match the one-time target.”</p> <p>So the diagonal again plays the role of anchor supervision.</p> <h3 id="43-pathwiseconsistency-relation-for-stochastic-flow-maps">4.3 Pathwise/consistency relation for stochastic flow maps</h3> <p>They also derive a consistency/composition condition (their Eq. 23 in the snippet): \(\kappa_{t,s}(z_t,z_s) = \mathbb{E}_{z_1 \sim p_{1|t}(\cdot|z_t)} \big[ \kappa_{u,s}(z_u,z_s) \big]\) (with the appropriate latent dependence through (z_1)/paths)</p> <p>The exact notation is heavier, but the key idea is the same as flow-map composition:</p> <ul> <li> <strong>two-time transitions must compose correctly through intermediate times</strong>, but now in distributional form.</li> </ul> <h3 id="44-their-training-objective-mfm-loss">4.4 Their training objective (MFM loss)</h3> <p>They build:</p> <ol> <li>a <strong>diagonal supervision loss</strong> (fit the posterior on diagonal time pairs),</li> <li>a <strong>consistency loss</strong> (enforce off-diagonal composition consistency),</li> <li>and combine them into an MFM objective: \(\mathcal{L}_{\text{MFM}} = \mathcal{L}_{\text{diag}} + \lambda \mathcal{L}_{\text{cons}}\)</li> </ol> <p>This is the stochastic counterpart of the deterministic progression:</p> <ul> <li>diagonal target = “FM-like” anchor</li> <li>off-diagonal consistency = “flow-map-like” propagation</li> </ul> <h3 id="45-why-this-matters-for-the-broader-field">4.5 Why this matters for the broader field</h3> <p>This paper gives a more general lens:</p> <ul> <li>MeanFlow / deterministic flow maps are one branch</li> <li>stochastic transition learning is the broader object when uncertainty matters</li> <li>the “diagonal + consistency” decomposition is the unifying pattern</li> </ul> <p>This is exactly the kind of conceptual bridge diffusion researchers should care about.</p> <hr> <h2 id="5-transition-matching-distillation">5) Transition Matching Distillation</h2> <p>Transition Matching Distillation (TMD) bridges engineering and theory based adaptation of MeanFlow to <strong>video distillation</strong>.</p> <h3 id="51-core-problem-setup">5.1 Core problem setup</h3> <p>They want to distill a pretrained video diffusion teacher into a faster student. Direct one-stage distillation is hard in video because:</p> <ul> <li>the space is huge,</li> <li>temporal consistency matters,</li> <li>transformer-based video models make JVP painful (esp. attention kernels/FSDP/context parallelism).</li> </ul> <p>So TMD uses <strong>two stages</strong>.</p> <h3 id="52-stage-1-transition-matching-meanflow-tm-mf">5.2 Stage 1: Transition Matching MeanFlow (TM-MF)</h3> <p>This is the key new idea.</p> <p>Instead of applying MeanFlow directly in the original latent/data space, they define an <strong>inner transition</strong> problem and parameterize a conditional inner flow map via average velocity: \(f_\theta(y_s,s,r;m) = y_s + (s-r)u_\theta(y_s,s,r;m)\)</p> <p>where (m) is a feature extracted from the main backbone.</p> <p>Then they use a MeanFlow-style objective to train this transition head.</p> <p>A very practical (and nontrivial) design choice:</p> <ul> <li>they <strong>reparameterize</strong> the average velocity to stay aligned with the teacher head: \(u_\theta(y_s,s,r;m) = y_1 - \text{head}_\theta(y_s,s,r;m)\)</li> </ul> <p>This is not cosmetic. It keeps the new head close to teacher semantics, which improves stability.</p> <h3 id="53-jvp-issue-and-finite-difference-approximation">5.3 JVP issue and finite-difference approximation</h3> <p>This paper is very realistic about systems constraints:</p> <ul> <li>exact JVP is annoying with large-scale video transformer stacks (FlashAttention, FSDP, context parallelism),</li> <li>so they use a <strong>finite-difference approximation</strong> of the JVP.</li> </ul> <p>That’s a practical compromise:</p> <ul> <li>theoretically less clean than exact JVP,</li> <li>but massively easier to integrate into production-grade training code.</li> </ul> <h3 id="54-stage-2-distributional-distillation-objective">5.4 Stage 2: Distributional distillation objective</h3> <p>After TM-MF pretraining, they switch to a stronger distillation stage using a VSD/discriminator-style objective (their simplified algorithm shows): \(\mathcal{L} = \text{VSD}(\hat{x}) + \lambda \cdot \text{Discriminator}(\hat{x})\)</p> <p>So the conceptual split is:</p> <ul> <li> <strong>Stage 1 (TM-MF):</strong> learn a good transition-aware student parameterization, bootstrap geometry/dynamics</li> <li> <strong>Stage 2:</strong> sharpen sample quality and distribution match</li> </ul> <p>This is a strong template for hard domains (video, 3D, multimodal) where pure one-shot distillation is brittle.</p> <hr> <h2 id="6-terminal-velocity-matching">6) Terminal Velocity Matching</h2> <p>Terminal Velocity Matching (TVM): A More Principled Objective for One/Few-Step Models</p> <h3 id="61-what-it-changes">6.1 What it changes</h3> <p>Prior methods (FM/MeanFlow/FMM-style) mostly match local or initial-time velocity constraints.</p> <p>TVM says: match the <strong>terminal velocity</strong> of the flow trajectory instead.</p> <p>That sounds minor, but it changes the theory:</p> <ul> <li>they derive an explicit <strong>2-Wasserstein upper bound</strong> </li> <li>and motivate a more stable training target for one/few-step generation</li> </ul> <h3 id="62-core-theorem-and-loss-structure">6.2 Core theorem and loss structure</h3> <p>TVM introduces a terminal velocity target \(u^*(x_t,t,s) = \mathbb{E}[v_t \mid x_t]\) (at terminal pairing (s), with the paper’s precise conditioning)</p> <p>Then they define a target involving a time derivative of the learned map: \(u^*_\theta(x_t,t,s) = u^*(x_t,t,s) - (t-s)\partial_s F_\theta(x_t,t,s)\)</p> <p>and train with a matching loss of the form \(\mathbb{E}\|F_\theta - u^*_\theta\|^2\)</p> <p>The crucial thing is <strong>not</strong> just the formula; it’s the theorem: they show this objective upper-bounds the 2-Wasserstein distance (up to constants / residual terms in their theorem statement).</p> <p>That gives TVM a stronger distributional interpretation than “just match a derivative identity.”</p> <h3 id="63-significance">6.3 Significance</h3> <p>This is the first really clean signal that the field is maturing beyond:</p> <ul> <li>local consistency heuristics,</li> <li>empirical JVP identities,</li> <li>“it works in 1 step”</li> </ul> <p>into:</p> <ul> <li><strong>distributional guarantees</strong></li> <li>explicit control over terminal behavior</li> <li>better theory-practice alignment</li> </ul> <h3 id="64-the-systems-contribution">6.4 The systems contribution</h3> <p>TVM also points out a major implementation pain:</p> <ul> <li>JVP of scaled dot-product attention is poorly supported / inefficient in standard autograd stacks.</li> <li>Unlike prior works, TVM also propagates gradient through the JVP term (not just stop-grad around it), which is even harder.</li> </ul> <p>They propose a FlashAttention kernel that fuses JVP with forward pass and supports backward through the JVP result.</p> <p>That matters a lot if you care about scaling this family to modern DiT/transformer stacks.</p> <hr> <h2 id="7-unifying-view-the-fields-progression-in-one-picture">7) Unifying View: The Field’s Progression in One Picture</h2> <p>Almost every method in this area can be seen as:</p> <ol> <li> <strong>Diagonal anchor</strong> <ul> <li>when time-pair collapses, match a standard object (FM velocity / posterior / terminal target)</li> </ul> </li> <li> <strong>Off-diagonal propagation</strong> <ul> <li>via a differential identity (MeanFlow / FMM),</li> <li>or consistency/composition (stochastic/meta flow maps),</li> <li>or terminal-time control (TVM)</li> </ul> </li> </ol> <p>This “diagonal + propagation” lens is the best mental model for the literature.</p> <hr> <h2 id="8-practical-research-takeaways-for-top-lab-diffusion-folks">8) Practical Research Takeaways (for top-lab diffusion folks)</h2> <h3 id="81-if-youre-doing-one-stepfew-step-image-generation">8.1 If you’re doing one-step/few-step image generation</h3> <p>Start by deciding which failure mode you care about:</p> <ul> <li> <strong>trajectory geometry / displacement quality</strong> → MeanFlow-style</li> <li> <strong>distributional mismatch under self-generated rollouts</strong> → add FreeFlowMap-style correction</li> <li> <strong>theory / Wasserstein control</strong> → TVM-style objective</li> </ul> <h3 id="82-if-youre-doing-video--transformers">8.2 If you’re doing video / transformers</h3> <p>The math is not the main bottleneck; <strong>JVP implementation is</strong>. TMD and TVM both basically scream this:</p> <ul> <li>attention kernels + JVP + distributed training are the real constraint</li> <li>finite-difference JVP is often the “actually trains” solution</li> <li>custom kernels become a differentiator</li> </ul> <h3 id="83-if-youre-doing-3d--world-models--stochastic-simulators">8.3 If you’re doing 3D / world models / stochastic simulators</h3> <p>The stochastic flow-map lens is probably the most future-proof:</p> <ul> <li>deterministic flow maps are great for one-step generation</li> <li>but world models usually need stochastic transitions</li> <li>the <strong>diagonal posterior + consistency</strong> formulation is much closer to what you actually want</li> </ul> <hr> <h2 id="9-a-compact-theory-stack-to-remember">9) A compact “theory stack” to remember</h2> <p>The whole area can be compressed into this stack:</p> <ol> <li> <strong>FM</strong>: learn local tangent</li> <li> <strong>MeanFlow/FMM</strong>: convert local tangent into a trainable two-time displacement rule (via differential identity/JVP)</li> <li> <strong>Self-distilled flow maps</strong>: train on the student’s own rollout distribution (fix teacher-data mismatch)</li> <li> <strong>Correction terms</strong>: explicitly align marginals / noising velocities</li> <li> <strong>Stochastic flow maps</strong>: move from deterministic maps to transition kernels (posterior-diagonal + consistency)</li> <li> <strong>TVM</strong>: choose a target (terminal velocity) that gives stronger distributional guarantees</li> </ol> <p>That’s the real progression.</p> <hr> <h2 id="10-where-the-field-is-likely-going-next">10) Where the field is likely going next</h2> <p>The next wave is probably a merge of these threads:</p> <ul> <li><strong>TVM-style distributional guarantees</strong></li> <li><strong>self-distilled prior-only training</strong></li> <li><strong>stochastic transition operators</strong></li> <li><strong>kernel-aware JVP training for transformers/video/world models</strong></li> </ul> <p>In other words: the future is not just “better one-step image generation,” it’s <strong>learned transition operators</strong> for high-dimensional structured dynamics (video, 3D, simulators, world models), with both:</p> <ul> <li>strong numerical behavior (few NFEs)</li> <li>and actual distributional control (Wasserstein/KL-style guarantees)</li> </ul> <p>That’s exactly where flow maps stop being a distillation trick and become a real modeling primitive.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adam Abecid. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>