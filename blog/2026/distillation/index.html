<!DOCTYPE html> <html lang=""> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Modern Diffusion Distillation Overview | Adam Abecid </title> <meta name="author" content="Adam Abecid"> <meta name="description" content="Recent developments of diffusion distillation techniques"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://abecid.github.io/blog/2026/distillation/"> <script src="/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Adam Abecid </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Modern Diffusion Distillation Overview</h1> <p class="post-meta"> Created on February 20, 2026 </p> <p class="post-tags"> <a href="/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/tag/diffusion"> <i class="fa-solid fa-hashtag fa-sm"></i> diffusion</a>   <a href="/blog/tag/survey"> <i class="fa-solid fa-hashtag fa-sm"></i> survey</a>   ·   <a href="/blog/category/research-survey"> <i class="fa-solid fa-tag fa-sm"></i> research-survey</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="flow-matching-to-flow-maps-to-distillation-a-deep-dive">Flow Matching to Flow Maps to Distillation: A Deep Dive</h1> <h1 id="table-of-content">Table of Content</h1> <ol> <li> <a href="#1-foundations-flow-matching-vs-flow-maps">Foundations</a> 1-1. Diffusion 1-2. Flow Matching 1-3. Rectified Flow 1-4. FlowMap 1-5. Consistency Model</li> <li> <a href="#2-meanflow">MeanFlow Family</a> 2-1. MeanFlow 2-2. iMeanFlow 2-3. AlphaFlow 2-4. Improved MeanFlow 2-5. Decoupled MeanFlow</li> <li> <a href="#3-freeflowmap">Flow Map</a> 3-1. Data Free: FreeFlowMap 3-2. Meta FlowMap 3-3. TVM</li> <li> <a href="">Score Distillation</a> 4-1. VSD 4-2. DMD 4-3. Adaptive Matching Distillation</li> <li> <a href="">Adversarial</a> 5-1. DiffRatio 5-2. APT</li> <li> <a href="">Video Generation</a> 6-1. CausVid 6-2. Self-forcing 6-3. TMD</li> <li> <a href="">New Domains</a> 7-1. Drifting 7-2. JIT 7-3. PixelFlow 7-4. LatentForcing</li> <li> <a href="">Manifold</a> 8-1. Riemmian Manifold 8-2. Optimal Transport</li> </ol> <h1 id="overview">Overview</h1> <p>Recent generative modeling utilize and develop upon flow maps and jvp based distillation techniques to reduce the number of function evaluations during inference. We focus on the Meanflow family, score distillation methods, and its applications in video generation.</p> <hr> <h1 id="1-foundations">1. Foundations</h1> <h2 id="11-diffusion">1.1 Diffusion</h2> <p>Diffusion models define a <strong>forward noising process</strong> that gradually corrupts data into noise, and a <strong>reverse process</strong> that learns to reconstruct data from noise. The main reason diffusion models became dominant is that they are stable and high-quality, but the tradeoff is <strong>slow iterative sampling</strong>.</p> <h3 id="111-forward-process-discrete-ddpm-view">1.1.1 Forward process (discrete DDPM view)</h3> <p>In DDPM, the forward process is a Markov chain:</p> \[q(x_t \mid x_{t-1}) = \mathcal{N}\!\left(\sqrt{1-\beta_t}\,x_{t-1}, \beta_t I\right),\] <p>where $\beta_t \in (0,1)$ is a variance schedule.</p> <p>A key closed form is:</p> \[q(x_t \mid x_0) = \mathcal{N}\!\left(\sqrt{\bar\alpha_t}\,x_0,\,(1-\bar\alpha_t)I\right),\] <p>with</p> \[\alpha_t = 1-\beta_t,\qquad \bar\alpha_t=\prod_{s=1}^t \alpha_s.\] <p>So we can sample $x_t$ directly as:</p> \[x_t = \sqrt{\bar\alpha_t}\,x_0 + \sqrt{1-\bar\alpha_t}\,\epsilon,\qquad \epsilon \sim \mathcal{N}(0,I).\] <p>This is the standard “data + Gaussian noise” interpolation used in many diffusion derivations.</p> <h3 id="112-reverse-process-and-denoising-objective">1.1.2 Reverse process and denoising objective</h3> <p>The generative model learns the reverse transitions</p> \[p_\theta(x_{t-1}\mid x_t),\] <p>which are parameterized via a neural network (predicting noise, $x_0$, or velocity depending on parameterization).</p> <p>The most common training objective is noise prediction:</p> \[\mathcal{L}_{\text{simple}}(\theta) = \mathbb{E}_{t,x_0,\epsilon} \left[ \left\|\epsilon - \epsilon_\theta(x_t,t)\right\|^2 \right].\] <p>This objective is simple and works extremely well, but inference still requires many reverse denoising steps.</p> <h3 id="113-continuous-time-diffusion-sde-view">1.1.3 Continuous-time diffusion (SDE view)</h3> <p>A continuous-time diffusion can be written as an SDE:</p> \[dx = f(x,t)\,dt + g(t)\,dW_t,\] <p>where $f$ is drift, $g$ is diffusion scale, and $W_t$ is a Wiener process.</p> <p>The reverse-time generative dynamics also form an SDE involving the score:</p> \[\nabla_x \log p_t(x).\] <p>This is the bridge to <strong>score-based generative modeling</strong> and continuous-time transport formulations.</p> <h3 id="114-probability-flow-ode-deterministic-counterpart">1.1.4 Probability flow ODE (deterministic counterpart)</h3> <p>Every diffusion SDE has an associated deterministic <strong>probability flow ODE</strong> that shares the same marginals $p_t$:</p> \[\frac{dx}{dt} = f(x,t) - \frac{1}{2}g(t)^2 \nabla_x \log p_t(x).\] <p>This is huge conceptually because it turns diffusion sampling into solving an ODE, which directly connects to:</p> <ul> <li>continuous normalizing flows,</li> <li>flow matching,</li> <li>rectified flow,</li> <li>and later flow-map distillation methods.</li> </ul> <p>So diffusion is not just “denoising noise,” it is also a <strong>continuous transport process</strong> in disguise.</p> <h3 id="115-why-diffusion-motivates-distillation">1.1.5 Why diffusion motivates distillation</h3> <p>Diffusion teachers are strong but slow because generation requires many function evaluations (NFEs). Distillation methods aim to compress this long trajectory into:</p> <ul> <li> <strong>few-step samplers</strong> (e.g. 2–8 steps),</li> <li>or even <strong>one-step generators</strong>,</li> </ul> <p>while preserving the teacher’s learned transport geometry.</p> <p>This is exactly why Chapter 1 naturally progresses from <strong>Diffusion $\to$ Flow Matching $\to$ Flow Maps / Consistency / Distillation</strong>.</p> <h2 id="12-flow-matching">1.2 Flow Matching</h2> <p><img src="/assets/img/blogs/1_distillation/flowmatching.png" alt="Flow Matching"></p> <p><em>Figure 1. Flow Matching. from Sabour, Fidler, and Kreis (2025),</em> Align Your Flow: Scaling Continuous-Time Flow Map Distillation <em>(arXiv:2506.14603).</em></p> <p>Flow Matching (FM) reframes generative modeling as directly learning a <strong>time-dependent velocity field</strong> that transports a simple source distribution (usually Gaussian noise) to the data distribution.</p> <h3 id="121-core-idea-learn-the-instantaneous-velocity">1.2.1 Core idea: learn the instantaneous velocity</h3> <p>Instead of learning reverse denoising conditionals, FM learns a vector field</p> \[u_\theta(x,t)\] <p>such that samples evolve via the ODE</p> \[\frac{dx}{dt} = u_\theta(x,t),\qquad t\in[0,1].\] <p>If this ODE is integrated from source noise at $t=0$ to $t=1$, the final samples should follow the data distribution.</p> <h3 id="122-conditional-path-and-target-velocity">1.2.2 Conditional path and target velocity</h3> <p>FM is usually trained by defining a conditional interpolation path between paired endpoints $(x_0,x_1)$:</p> \[x_t = \psi_t(x_0,x_1).\] <p>For simple linear interpolation:</p> \[x_t = (1-t)x_0 + tx_1.\] <p>The target velocity along this path is:</p> \[\dot{x}_t = \frac{d}{dt}\psi_t(x_0,x_1).\] <p>For the linear path, this becomes:</p> \[\dot{x}_t = x_1 - x_0.\] <p>The model is trained to match this conditional velocity in expectation:</p> \[\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t,x_0,x_1} \left[ \left\|u_\theta(x_t,t)-\dot{x}_t\right\|^2 \right].\] <p>So FM is “supervised vector field learning” on a chosen path family.</p> <h3 id="123-why-fm-is-attractive">1.2.3 Why FM is attractive</h3> <p>Compared to score/diffusion training, FM gives a very clean ODE-learning objective and avoids explicit score estimation. It is especially natural when you want to reason about:</p> <ul> <li>transport geometry,</li> <li>ODE trajectories,</li> <li>and later finite-time maps (flow maps).</li> </ul> <h3 id="124-limitation-local-field-expensive-sampling">1.2.4 Limitation: local field, expensive sampling</h3> <p>FM learns a <strong>local tangent</strong> $u_\theta(x,t)$, not a finite-time jump. That means sampling still requires numerical integration:</p> \[x_{t+\Delta t} \approx x_t + \Delta t\,u_\theta(x_t,t)\] <p>(or higher-order solvers like Heun).</p> <p>If trajectories are curved, discretization error accumulates, so many NFEs are needed. This is the main bottleneck that motivates:</p> <ul> <li>Rectified Flow (straighter trajectories),</li> <li>Flow Maps (direct time-to-time transport),</li> <li>and distillation methods (few-step or one-step generation).</li> </ul> <h2 id="13-rectified-flow">1.3 Rectified Flow</h2> <p>Rectified Flow (RF) keeps the ODE/transport framing of FM, but explicitly pushes the learned trajectories to become <strong>straighter</strong>, which makes them much easier to sample with few steps.</p> <h3 id="131-motivation-straight-trajectories-are-cheap">1.3.1 Motivation: straight trajectories are cheap</h3> <p>If a trajectory is highly curved, Euler updates need many small steps. If a trajectory is nearly straight, even a coarse solver can track it accurately.</p> <p>So RF is basically a geometry-aware fix to the FM sampling bottleneck.</p> <h3 id="132-linear-interpolation-path-and-velocity-target">1.3.2 Linear interpolation path and velocity target</h3> <p>A standard RF path is the same linear interpolation:</p> \[x_t = (1-t)x_0 + tx_1,\] <p>with instantaneous derivative</p> \[\frac{dx_t}{dt} = x_1 - x_0.\] <p>RF trains a velocity field to match this transport direction along the path:</p> \[v_\theta(x_t,t) \approx x_1 - x_0.\] <p>A common training objective is:</p> \[\mathcal{L}_{\text{RF}}(\theta) = \mathbb{E}_{t,x_0,x_1} \left[ \left\|v_\theta(x_t,t)-(x_1-x_0)\right\|^2 \right].\] <p>This looks similar to FM, but the interpretation is sharper: RF cares about learning a transport field whose induced trajectories are easy to discretize.</p> <h3 id="133-reflow-iterative-rectification">1.3.3 Reflow (iterative rectification)</h3> <p>A major practical idea in RF is <strong>reflow</strong>:</p> <ol> <li>Train an initial transport field.</li> <li>Sample trajectories from the model.</li> <li>Use those trajectories (or endpoint couplings) to retrain a straighter field.</li> <li>Repeat.</li> </ol> <p>Each round reduces curvature and improves few-step generation. This is why RF is often viewed as a bridge between classical diffusion/FM and modern one-step distillation.</p> <h3 id="134-why-rf-matters-for-the-rest-of-this-blog">1.3.4 Why RF matters for the rest of this blog</h3> <p>RF is the clean conceptual bridge to flow-map methods because it shifts the focus from “match local field” to “shape trajectories for fast transport.” Once you think this way, the next obvious step is:</p> <blockquote> <p>Why learn only the local tangent at all?<br> Why not learn the <strong>finite-time map</strong> directly?</p> </blockquote> <p>That is exactly the flow-map perspective.</p> <h2 id="14-flow-map">1.4 Flow Map</h2> <p><img src="/assets/img/blogs/1_distillation/flowmap.png" alt="Flow Map"></p> <p><em>Figure 2. Flow Map. from Sabour, Fidler, and Kreis (2025),</em> Align Your Flow: Scaling Continuous-Time Flow Map Distillation <em>(arXiv:2506.14603).</em></p> <p>Flow-map methods move beyond local vector fields and directly learn a <strong>time-to-time transport operator</strong>.</p> <h3 id="141-from-vector-field-to-finite-time-map">1.4.1 From vector field to finite-time map</h3> <p>Given a velocity field $u(x,t)$ and its ODE</p> \[\frac{dx}{dt}=u(x,t),\] <p>the associated flow map $\phi_u$ sends a state from time $t$ to time $s$:</p> \[\phi_u(x_t,t,s)=x_s.\] <p>So instead of learning only the local tangent $u(x,t)$, we learn the finite-time update:</p> \[x_s \approx \phi_\theta(x_t,t,s).\] <p>This is much more aligned with few-step sampling, because a single model evaluation can move across a large time interval.</p> <h3 id="142-why-flow-maps-help-distillation">1.4.2 Why flow maps help distillation</h3> <p>A flow field gives infinitesimal updates; a flow map gives finite jumps.</p> <p>That means flow maps are naturally suited for:</p> <ul> <li> <strong>few-step sampling</strong> (large $t\to s$ jumps),</li> <li> <strong>teacher-student distillation</strong> (student imitates teacher transitions),</li> <li> <strong>self-distillation</strong> (model supervises its own multistep consistency),</li> <li>and <strong>data-free distillation</strong> variants (matching dynamics without original data).</li> </ul> <p>This is the key conceptual move behind MeanFlow-family and FreeFlowMap-family methods.</p> <h3 id="143-semigroup--composition-structure">1.4.3 Semigroup / composition structure</h3> <p>Exact flow maps satisfy a composition rule (semigroup property):</p> \[\phi(x_t,t,r) = \phi\!\left(\phi(x_t,t,s),\,s,\,r\right) \qquad \text{for } t \le s \le r.\] <p>This property is incredibly important because it gives a built-in consistency constraint across time triples. Many modern distillation methods exploit some version of this:</p> <ul> <li>explicit composition matching,</li> <li>consistency losses,</li> <li>JVP-based local constraints that imply finite-time consistency.</li> </ul> <h3 id="144-relation-to-fm-and-rf">1.4.4 Relation to FM and RF</h3> <ul> <li> <strong>FM</strong> learns $u(x,t)$ (local instantaneous velocity)</li> <li> <strong>RF</strong> improves trajectory geometry (straighter ODE paths)</li> <li> <strong>Flow Map</strong> learns $\phi(x_t,t,s)$ (finite-time transport)</li> </ul> <p>So flow maps are not a totally different universe; they are the natural next abstraction after FM/RF if your goal is <strong>fast generation</strong>.</p> <h2 id="15-consistency-models">1.5 Consistency Models</h2> <p>Consistency Models (CMs) attack the same bottleneck from another angle: instead of learning a vector field or even an explicit flow map, they learn a <strong>cross-time consistent predictor</strong> that maps noisy states to a shared target representation (often an estimate of clean data).</p> <p>This makes them one of the foundational one-step / few-step distillation paradigms.</p> <h3 id="151-core-consistency-idea">1.5.1 Core consistency idea</h3> <p>Suppose $x_t$ and $x_s$ lie on the same teacher trajectory (or same underlying denoising path). A consistency model $f_\theta$ is trained so that: \(f_\theta(x_t,t) \approx f_\theta(x_s,s),\) after the appropriate scaling/parameterization.</p> <p>In words: different noise levels along the same trajectory should produce the same final prediction.</p> <p>This is a <strong>cross-time agreement constraint</strong>, not just a local derivative-matching objective.</p> <h3 id="152-why-this-enables-one-step-generation">1.5.2 Why this enables one-step generation</h3> <p>Because the model is trained to collapse trajectory points to a common target, we can often sample by evaluating the model once (or very few times) from a noisy input.</p> <p>This directly targets inference speed, unlike standard diffusion training which optimizes denoising accuracy at every step but does not inherently optimize for low-NFE sampling.</p> <h3 id="153-teacher-student-consistency-distillation">1.5.3 Teacher-student consistency distillation</h3> <p>A common setup is:</p> <ol> <li>Start with a strong diffusion/score teacher.</li> <li>Generate paired states $(x_t, x_s)$ on teacher trajectories.</li> <li>Train the student consistency model to agree across those states.</li> </ol> <p>This makes consistency models a very important predecessor to later:</p> <ul> <li>flow-map distillation,</li> <li>self-distillation,</li> <li>and JVP-based transport-map objectives.</li> </ul> <h3 id="154-conceptual-relation-to-flow-maps">1.5.4 Conceptual relation to flow maps</h3> <p>Consistency models and flow-map methods are closely related in spirit:</p> <ul> <li> <strong>Flow map view:</strong> learn explicit transport $\phi(x_t,t,s)$</li> <li> <strong>Consistency view:</strong> learn a representation/prediction that is invariant (or aligned) across times on the same trajectory</li> </ul> <p>Both replace purely local supervision with <strong>cross-time structure</strong>, which is exactly what you need for few-step and one-step generation.</p> <h3 id="155-scm">1.5.5 sCM</h3> <h3 id="156-rcm">1.5.6 rCM</h3> <hr> <h1 id="2-meanflow-family">2. MeanFlow Family</h1> <h2 id="2-1-meanflow">2-1. MeanFlow</h2> <h4 id="21-average-velocity-instead-of-instantaneous-velocity">2.1 Average velocity instead of instantaneous velocity</h4> <p><img src="/assets/img/blogs/1_distillation/meanflow.png" alt="Mean Flow"></p> <p><em>Figure 3. Mean Flow, with different target timestep $t$. from Geng et al. (2025),</em> Mean Flows for One-step Generative Modeling <em>(arXiv:2505.13447).</em></p> <p>MeanFlow defines an <strong>average velocity</strong></p> \[u(z_t, r, t)\] <p>over the interval $[r,t]$, so the displacement is:</p> \[(t-r)u(z_t,r,t).\] <h4 id="22-the-meanflow-identity-the-central-derivation">2.2 The MeanFlow Identity (the central derivation)</h4> <p>Start from the definition:</p> \[(t-r)u(z_t,r,t) = \int_r^t v(z_\tau,\tau)\, d\tau\] <p>Differentiate both sides with respect to (t) (holding (r) fixed). By product rule + FTC:</p> \[u(z_t,r,t) + (t-r)\frac{d}{dt}u(z_t,r,t) = v(z_t,t)\] <p>Rearranging leads to the <strong>MeanFlow Identity</strong>:</p> \[u(z_t,r,t) = v(z_t,t) - (t-r)\frac{d}{dt}u(z_t,r,t)\] <h4 id="23-main-points">2.3 Main Points</h4> <ul> <li>The authors rewrite an intractable target (the average velocity integral) into a trainable target: <ul> <li>By first taking the derivative of both sides.</li> <li>Instantaneous velocity $v$ is available from FM-style interpolation.</li> <li>Total derivative term computed via <strong>JVP</strong>.</li> </ul> </li> <li>No explicit consistency regularizer is imposed.</li> <li>The consistency-like structure falls out from the definition of average velocity.</li> </ul> <h4 id="24-meanflow-loss">2.4 MeanFlow loss</h4> <p>Parameterize $u_\theta(z_t,r,t)$, and regress to the identity-induced target:</p> \[\mathcal{L}_{\text{MeanFlow}}(\theta) = \mathbb{E}\left[ \left\| u_\theta(z_t,r,t) - \operatorname{sg}(u_{\text{tgt}}) \right\|_2^2 \right]\] <p>with</p> \[u_{\text{tgt}} = v_t - (t-r)\left(v_t \,\partial_z u_\theta + \partial_t u_\theta\right)\] <p>where the total derivative is implemented through a JVP along tangent $(v_t, 0, 1)$.</p> <h4 id="25-sampling">2.5 Sampling</h4> <p>Once you learn the average velocity, sampling is:</p> \[z_r = z_t - (t-r)u(z_t,r,t)\] <p>For 1-step:</p> \[z_0 = z_1 - u(z_1,0,1),\quad z_1 \sim p_{\text{prior}}\] <hr> <h2 id="22-improved-meanflow-imf">2.2 Improved MeanFlow (iMF)</h2> <p>iMF addresses two practical issues in MeanFlow:</p> <ol> <li>the self-referential target,</li> <li>fixed-CFG training (bad for inference-time flexibility).</li> </ol> <h4 id="221-meanflow-as-a-v-loss">2.2.1 MeanFlow as a v-loss</h4> <p>iMF rewrites the MeanFlow identity into a <strong>velocity regression form</strong>:</p> \[v(z_t) = u(z_t,r,t) + (t-r)\frac{d}{dt}u(z_t,r,t).\] <p>Then parameterize the RHS with $u_\theta$:</p> \[V_\theta = u_\theta(z_t,r,t) + (t-r)\,\mathrm{JVP}_{\mathrm{sg}}(u_\theta; v_\theta),\] <p>and train with a Flow-Matching-like loss</p> \[\mathcal{L}_{\mathrm{iMF}} = \mathbb{E}\left[\left\|V_\theta - (\epsilon-x)\right\|_2^2\right].\] <p>This is cleaner because the regression target is now the standard FM target $(\epsilon-x)$ rather than an apparent target constructed from $u_\theta$.</p> <h3 id="222-flexible-cfg-as-conditioning">2.2.2 Flexible CFG as conditioning</h3> <p>Original MeanFlow supports CFG in 1-NFE, but with a <strong>fixed guidance scale</strong> chosen at training time.</p> <p>iMF fixes this by making the guidance scale part of the conditioning:</p> <ul> <li>guidance scale $\omega$ becomes an input condition,</li> <li>the same model can be sampled with different CFG scales at inference.</li> </ul> <p>That is a big deal because the optimal CFG scale shifts with model size, training progress, and NFE.</p> <h3 id="223-in-context-conditioning">2.2.3 In-context conditioning</h3> <p>iMF also upgrades conditioning architecture:</p> <ul> <li>conditions include $(r,t)$, class label $c$, and guidance-related variables $\Omega$,</li> <li>each condition is represented with multiple learnable tokens,</li> <li>all condition tokens are concatenated with image latent tokens and processed by the Transformer.</li> </ul> <p>This allows:</p> <ul> <li>support richer heterogeneous conditioning more naturally,</li> <li>remove <strong>adaLN-zero</strong>,</li> <li>cut params significantly (they report about <strong>1/3 reduction</strong> in a base model setting).</li> </ul> <h2 id="23-alphaflow">2.3 AlphaFlow</h2> <p>AlphaFlow is the paper that gives the most useful conceptual interpretation of MeanFlow training.</p> <h4 id="231-core-insight-meanflow-decomposes-into-two-losses">2.3.1 Core insight: MeanFlow decomposes into two losses</h4> <p>AlphaFlow shows the MeanFlow objective can be algebraically decomposed into:</p> <ol> <li><strong>Trajectory Flow Matching (TFM)</strong></li> <li><strong>Trajectory Consistency (TC)</strong></li> </ol> <p>The decomposition (up to a constant) is:</p> \[\mathcal{L}_{\mathrm{MF}} = \underbrace{\mathbb{E}\left[\|u_\theta(z_t,r,t)-v_t\|_2^2\right]}_{\mathcal{L}_{\mathrm{TFM}}} + \underbrace{\mathbb{E}\left[2(t-r)\,u_\theta^\top \frac{d u_\theta^-}{dt}\right]}_{\mathcal{L}_{\mathrm{TC}}} + C.\] <p>Interpretation:</p> <ul> <li> <strong>TFM</strong> says “fit the trajectory-local velocity target.”</li> <li> <strong>TC</strong> says “be self-consistent along the trajectory.”</li> <li>MeanFlow is effectively a <strong>consistency-like model with extra trajectory FM supervision</strong>.</li> </ul> <h4 id="232-why-meanflow-often-needs-lots-of-border-case-fm-samples">2.3.2 Why MeanFlow often needs lots of border-case FM samples</h4> <p>AlphaFlow also explains a weird empirical fact from MeanFlow:</p> <ul> <li>MeanFlow works best when many samples use the border case $r=t$ (which looks like vanilla FM).</li> </ul> <p>Their analysis shows this is not just a hack:</p> <ul> <li>the gradients of TFM and trajectory consistency are often <strong>negatively correlated</strong>,</li> <li>so the extra FM-style supervision helps stabilize and speed up training.</li> </ul> <h4 id="233-α-flow-loss-one-objective-that-unifies-tfm-shortcut-meanflow">2.3.3 α-Flow loss: one objective that unifies TFM, Shortcut, MeanFlow</h4> <p>AlphaFlow defines a family of losses parameterized by $\alpha$:</p> \[\mathcal{L}_\alpha(\theta) = \mathbb{E}_{t,r,z_t} \left[ \alpha^{-1} \left\| u_\theta(z_t,r,t) - \left(\alpha \,\tilde v_{s,t} + (1-\alpha)\,u_{\theta^-}(z_s,r,s)\right) \right\|_2^2 \right],\] <p>where</p> \[s = \alpha r + (1-\alpha)t\] <p>is an intermediate time.</p> <p>This unifies several training objectives:</p> <ul> <li>$\alpha=1$ gives <strong>trajectory flow matching</strong> (with suitable $\tilde v_{s,t}$),</li> <li>$\alpha=\tfrac{1}{2}$ recovers a <strong>Shortcut-style</strong> objective,</li> <li>$\alpha \to 0$ recovers the <strong>MeanFlow gradient</strong>.</li> </ul> <p>That is the key conceptual win: AlphaFlow puts FM, Shortcut, and MeanFlow on one continuum.</p> <h4 id="234-curriculum">2.3.4 Curriculum</h4> <p>Because TFM and TC conflict early in training, AlphaFlow uses a curriculum:</p> <ul> <li>start more FM-like (larger $\alpha$),</li> <li>gradually anneal toward MeanFlow-like behavior (smaller $\alpha$).</li> </ul> <p>This disentangles optimization and improves convergence.</p> <h4 id="235-practical-takeaway">2.3.5 Practical takeaway</h4> <p>AlphaFlow is best understood as:</p> <ul> <li>a <strong>theory paper for MeanFlow optimization</strong> (decomposition + gradient conflict),</li> <li>plus a <strong>practical training recipe</strong> (curriculum over $\alpha$) that improves one-step/few-step quality.</li> </ul> <h2 id="24-accelerating-and-meanflow">2.4 Accelerating and MeanFlow</h2> <h2 id="25-decoupled-meanflow">2.5 Decoupled MeanFlow</h2> <hr> <h1 id="3-flowmap">3. FlowMap</h1> <h2 id="31-free-flowmap">3.1 Free FlowMap</h2> <p>Flow Map Distillation Without Data bias</p> <h4 id="311-teacher-data-mismatch-the-hidden-bug-in-many-distillation-pipelines">3.1.1 Teacher-data mismatch (the hidden bug in many distillation pipelines)</h4> <p>Traditional flow-map distillation often samples intermediate states $x_t$ from an <strong>external dataset distribution</strong> and supervises the student using teacher velocities at those states.</p> <p>But the student is supposed to reproduce the teacher’s <strong>sampling process</strong>, i.e. the trajectory distribution induced by the teacher from the prior.</p> <p>Supervision states coming from a mismatched distribution results in a <strong>teacher-data mismatch</strong>:</p> <ul> <li>the student is trained on states that are not on the teacher’s true rollout distribution,</li> <li>more augmentation can worsen it,</li> <li>student quality degrades.</li> </ul> <p>This is a deep point because it says the standard “distill on data” recipe is fundamentally misaligned with the actual objective (imitate the teacher sampler).</p> <h4 id="312-prior-only--self-generated-flow-map-objective">3.1.2 Prior-only / self-generated flow-map objective</h4> <p>Supervise entirely from the prior and the student’s own generated states.</p> <p>They derive a sufficient optimality condition leading to a loss of the form</p> \[\mathcal{L}_{\text{pred}} = \mathbb{E}_{z,\delta} \left[ \|F_\theta(z,\delta)-\operatorname{sg}(u_{\text{target}})\|^2 \right]\] <p>with</p> \[u_{\text{target}} = u(f_\theta(z,\delta),1-\delta) - \delta \,\partial_\delta F_\theta(z,\delta)\] <p>The key interpretation:</p> <ul> <li>$f_\theta(z,\delta)$ defines the student’s current trajectory,</li> <li>$\partial_\delta f_\theta$ is the <strong>student’s generating velocity</strong>,</li> <li>the loss is equivalent to aligning the student generating velocity with the teacher field:</li> </ul> \[\partial_\delta f_\theta \approx u(f_\theta(z,\delta),1-\delta)\] <p>So the student learns to <strong>ride the teacher vector field along its own generated path</strong>, starting from pure noise, no dataset needed.</p> <p>This is the right fix for teacher-data mismatch.</p> <h4 id="313-gradient-view">3.1.3 Gradient view</h4> <p>They explicitly write the optimization gradient in terms of a velocity mismatch</p> \[\Delta v_{G,u} = v_G - u\] <p>which is great because it makes gradient weighting / normalization tricks easier to reason about.</p> <p>This is one of those “small” presentation choices that actually matters in practice.</p> <h4 id="314-correction-objective-fixing-distribution-drift-not-just-local-velocity">3.1.4 Correction objective (fixing distribution drift, not just local velocity)</h4> <p>Here’s the catch: aligning generating velocity locally is necessary, but in finite-capacity/discrete training the generated distribution can still drift.</p> <p>So they add a <strong>correction objective</strong> motivated from minimizing a KL term over intermediate marginals and then translating score mismatch into <strong>velocity mismatch</strong> (via the score–velocity equivalence for linear interpolants).</p> <p>This yields a gradient proportional to</p> \[\nabla_\theta \; \mathbb{E}_{z,n,r} \left[ F_\theta(z,1)^\top \operatorname{sg} \left( v_N(I_r(f_\theta(z,1),n),r) - u(I_r(f_\theta(z,1),n),r) \right) \right]\] <p>where:</p> <ul> <li>$u$ is the teacher marginal velocity,</li> <li>$v_N$ is the student-induced <strong>noising</strong> marginal velocity,</li> <li>$I_r(\cdot,\cdot)$ is the interpolation to intermediate time $r$.</li> </ul> <p>Intuition:</p> <ul> <li>the prediction loss aligns the <strong>student’s forward/generating flow</strong> </li> <li>the correction loss aligns the <strong>student-induced noising marginals</strong> with the teacher’s marginals</li> </ul> <p>That is a nice bidirectional correction mechanism.</p> <hr> <h1 id="32-meta-flow-maps">3.2 Meta Flow Maps</h1> <p>Meta flow maps correspond to a <strong>stochastic flow map</strong>, which is important because deterministic flow maps are too rigid.</p> <h4 id="321-why-stochastic-flow-maps">3.2.1 Why stochastic flow maps?</h4> <p>Deterministic flow-map learning works if the transport map is the right object. But for many diffusion-like processes, especially when you want richer uncertainty handling, a <strong>stochastic transition kernel</strong></p> <p>\(\kappa_{t,s}(z_t, z_s)\) is the right object.</p> <p>The paper frames this using:</p> <ul> <li><strong>marginal consistency</strong></li> <li><strong>conditional consistency</strong></li> <li> <table> <tbody> <tr> <td>a family of posterior conditionals $p_{1</td> <td>t}$</td> </tr> </tbody> </table> </li> <li>and a diagonal supervision view</li> </ul> <p>The important conceptual upgrade is:</p> <ul> <li>instead of only learning deterministic trajectories,</li> <li>learn a transition operator consistent with the stochastic process structure.</li> </ul> <h4 id="322-the-diagonal-condition-same-role-as-fm-diagonal-supervision">3.2.2 The diagonal condition (same role as FM diagonal supervision)</h4> <p>They derive that on the diagonal:</p> \[\kappa_{t,t}(z_t, z_1) = p_{1|t}(z_1 \mid z_t)\] <p>This is the stochastic analogue of “when $r=t$, your two-time object must match the one-time target.”</p> <p>So the diagonal again plays the role of anchor supervision.</p> <h4 id="323-pathwiseconsistency-relation-for-stochastic-flow-maps">3.2.3 Pathwise/consistency relation for stochastic flow maps</h4> <p>They also derive a consistency/composition condition (their Eq. 23 in the snippet):</p> \[\kappa_{t,s}(z_t,z_s) = \mathbb{E}_{z_1 \sim p_{1|t}(\cdot|z_t)} \big[ \kappa_{u,s}(z_u,z_s) \big]\] <p>(with the appropriate latent dependence through $z_1$/paths)</p> <p>The exact notation is heavier, but the key idea is the same as flow-map composition:</p> <ul> <li> <strong>two-time transitions must compose correctly through intermediate times</strong>, but now in distributional form.</li> </ul> <h4 id="324-their-training-objective-mfm-loss">3.2.4 Their training objective (MFM loss)</h4> <p>They build:</p> <ol> <li>a <strong>diagonal supervision loss</strong> (fit the posterior on diagonal time pairs),</li> <li>a <strong>consistency loss</strong> (enforce off-diagonal composition consistency),</li> <li>and combine them into an MFM objective:</li> </ol> \[\mathcal{L}_{\text{MFM}} = \mathcal{L}_{\text{diag}} + \lambda \mathcal{L}_{\text{cons}}\] <p>This is the stochastic counterpart of the deterministic progression:</p> <ul> <li>diagonal target = “FM-like” anchor</li> <li>off-diagonal consistency = “flow-map-like” propagation</li> </ul> <h4 id="325-takeaways">3.2.5 Takeaways</h4> <p>This paper gives a more general lens:</p> <ul> <li>MeanFlow / deterministic flow maps are one branch</li> <li>stochastic transition learning is the broader object when uncertainty matters</li> <li>the “diagonal + consistency” decomposition is the unifying pattern</li> </ul> <p>This is exactly the kind of conceptual bridge diffusion researchers should care about.</p> <hr> <h1 id="33-transition-matching-distillation">3.3 Transition Matching Distillation</h1> <p>Transition Matching Distillation (TMD) bridges engineering and theory based adaptation of MeanFlow to <strong>video distillation</strong>.</p> <h4 id="331-core-problem-setup">3.3.1 Core problem setup</h4> <p>They want to distill a pretrained video diffusion teacher into a faster student. Direct one-stage distillation is hard in video because:</p> <ul> <li>the space is huge,</li> <li>temporal consistency matters,</li> <li>transformer-based video models make JVP painful (esp. attention kernels/FSDP/context parallelism).</li> </ul> <p>So TMD uses <strong>two stages</strong>.</p> <h4 id="332-stage-1-transition-matching-meanflow-tm-mf">3.3.2 Stage 1: Transition Matching MeanFlow (TM-MF)</h4> <p>This is the key new idea.</p> <p>Instead of applying MeanFlow directly in the original latent/data space, they define an <strong>inner transition</strong> problem and parameterize a conditional inner flow map via average velocity:</p> \[f_\theta(y_s,s,r;m) = y_s + (s-r)u_\theta(y_s,s,r;m)\] <p>where $m$ is a feature extracted from the main backbone.</p> <p>Then they use a MeanFlow-style objective to train this transition head.</p> <p>A very practical (and nontrivial) design choice:</p> <ul> <li>they <strong>reparameterize</strong> the average velocity to stay aligned with the teacher head:</li> </ul> \[u_\theta(y_s,s,r;m) = y_1 - \text{head}_\theta(y_s,s,r;m)\] <p>This is not cosmetic. It keeps the new head close to teacher semantics, which improves stability.</p> <h4 id="333-jvp-issue-and-finite-difference-approximation">3.3.3 JVP issue and finite-difference approximation</h4> <p>This paper is very realistic about systems constraints:</p> <ul> <li>exact JVP is annoying with large-scale video transformer stacks (FlashAttention, FSDP, context parallelism),</li> <li>so they use a <strong>finite-difference approximation</strong> of the JVP.</li> </ul> <p>That’s a practical compromise:</p> <ul> <li>theoretically less clean than exact JVP,</li> <li>but massively easier to integrate into production-grade training code.</li> </ul> <h4 id="334-stage-2-distributional-distillation-objective">3.3.4 Stage 2: Distributional distillation objective</h4> <p>After TM-MF pretraining, they switch to a stronger distillation stage using a VSD/discriminator-style objective (their simplified algorithm shows):</p> \[\mathcal{L} = \text{VSD}(\hat{x}) + \lambda \cdot \text{Discriminator}(\hat{x})\] <p>So the conceptual split is:</p> <ul> <li> <strong>Stage 1 (TM-MF):</strong> learn a good transition-aware student parameterization, bootstrap geometry/dynamics</li> <li> <strong>Stage 2:</strong> sharpen sample quality and distribution match</li> </ul> <p>This is a strong template for hard domains (video, 3D, multimodal) where pure one-shot distillation is brittle.</p> <hr> <h1 id="6-terminal-velocity-matching">6. Terminal Velocity Matching</h1> <p>Terminal Velocity Matching (TVM): A More Principled Objective for One/Few-Step Models</p> <h3 id="61-what-it-changes">6.1 What it changes</h3> <p>Prior methods (FM/MeanFlow/FMM-style) mostly match local or initial-time velocity constraints.</p> <p>TVM says: match the <strong>terminal velocity</strong> of the flow trajectory instead.</p> <p>That sounds minor, but it changes the theory:</p> <ul> <li>they derive an explicit <strong>2-Wasserstein upper bound</strong> </li> <li>and motivate a more stable training target for one/few-step generation</li> </ul> <h3 id="62-core-theorem-and-loss-structure">6.2 Core theorem and loss structure</h3> <p>TVM introduces a terminal velocity target \(u^*(x_t,t,s) = \mathbb{E}[v_t \mid x_t]\) (at terminal pairing (s), with the paper’s precise conditioning)</p> <p>Then they define a target involving a time derivative of the learned map: \(u^*_\theta(x_t,t,s) = u^*(x_t,t,s) - (t-s)\partial_s F_\theta(x_t,t,s)\)</p> <p>and train with a matching loss of the form \(\mathbb{E}\|F_\theta - u^*_\theta\|^2\)</p> <p>The crucial thing is <strong>not</strong> just the formula; it’s the theorem: they show this objective upper-bounds the 2-Wasserstein distance (up to constants / residual terms in their theorem statement).</p> <p>That gives TVM a stronger distributional interpretation than “just match a derivative identity.”</p> <h3 id="63-significance">6.3 Significance</h3> <p>This is the first really clean signal that the field is maturing beyond:</p> <ul> <li>local consistency heuristics,</li> <li>empirical JVP identities,</li> <li>“it works in 1 step”</li> </ul> <p>into:</p> <ul> <li><strong>distributional guarantees</strong></li> <li>explicit control over terminal behavior</li> <li>better theory-practice alignment</li> </ul> <h3 id="64-the-systems-contribution">6.4 The systems contribution</h3> <p>TVM also points out a major implementation pain:</p> <ul> <li>JVP of scaled dot-product attention is poorly supported / inefficient in standard autograd stacks.</li> <li>Unlike prior works, TVM also propagates gradient through the JVP term (not just stop-grad around it), which is even harder.</li> </ul> <p>They propose a FlashAttention kernel that fuses JVP with forward pass and supports backward through the JVP result.</p> <p>That matters a lot if you care about scaling this family to modern DiT/transformer stacks.</p> <hr> <h1 id="7-unifying-view-the-fields-progression-in-one-picture">7. Unifying View: The Field’s Progression in One Picture</h1> <p>Almost every method in this area can be seen as:</p> <ol> <li> <strong>Diagonal anchor</strong> <ul> <li>when time-pair collapses, match a standard object (FM velocity / posterior / terminal target)</li> </ul> </li> <li> <strong>Off-diagonal propagation</strong> <ul> <li>via a differential identity (MeanFlow / FMM),</li> <li>or consistency/composition (stochastic/meta flow maps),</li> <li>or terminal-time control (TVM)</li> </ul> </li> </ol> <p>This “diagonal + propagation” lens is the best mental model for the literature.</p> <hr> <h1 id="8-practical-research-takeaways-for-top-lab-diffusion-folks">8. Practical Research Takeaways (for top-lab diffusion folks)</h1> <h3 id="81-if-youre-doing-one-stepfew-step-image-generation">8.1 If you’re doing one-step/few-step image generation</h3> <p>Start by deciding which failure mode you care about:</p> <ul> <li> <strong>trajectory geometry / displacement quality</strong> → MeanFlow-style</li> <li> <strong>distributional mismatch under self-generated rollouts</strong> → add FreeFlowMap-style correction</li> <li> <strong>theory / Wasserstein control</strong> → TVM-style objective</li> </ul> <h3 id="82-if-youre-doing-video--transformers">8.2 If you’re doing video / transformers</h3> <p>The math is not the main bottleneck; <strong>JVP implementation is</strong>. TMD and TVM both basically scream this:</p> <ul> <li>attention kernels + JVP + distributed training are the real constraint</li> <li>finite-difference JVP is often the “actually trains” solution</li> <li>custom kernels become a differentiator</li> </ul> <h3 id="83-if-youre-doing-3d--world-models--stochastic-simulators">8.3 If you’re doing 3D / world models / stochastic simulators</h3> <p>The stochastic flow-map lens is probably the most future-proof:</p> <ul> <li>deterministic flow maps are great for one-step generation</li> <li>but world models usually need stochastic transitions</li> <li>the <strong>diagonal posterior + consistency</strong> formulation is much closer to what you actually want</li> </ul> <hr> <h1 id="9-a-compact-theory-stack-to-remember">9. A compact “theory stack” to remember</h1> <p>The whole area can be compressed into this stack:</p> <ol> <li> <strong>FM</strong>: learn local tangent</li> <li> <strong>MeanFlow/FMM</strong>: convert local tangent into a trainable two-time displacement rule (via differential identity/JVP)</li> <li> <strong>Self-distilled flow maps</strong>: train on the student’s own rollout distribution (fix teacher-data mismatch)</li> <li> <strong>Correction terms</strong>: explicitly align marginals / noising velocities</li> <li> <strong>Stochastic flow maps</strong>: move from deterministic maps to transition kernels (posterior-diagonal + consistency)</li> <li> <strong>TVM</strong>: choose a target (terminal velocity) that gives stronger distributional guarantees</li> </ol> <p>That’s the real progression.</p> <hr> <h1 id="10-where-the-field-is-likely-going-next">10. Where the field is likely going next</h1> <p>The next wave is probably a merge of these threads:</p> <ul> <li><strong>TVM-style distributional guarantees</strong></li> <li><strong>self-distilled prior-only training</strong></li> <li><strong>stochastic transition operators</strong></li> <li><strong>kernel-aware JVP training for transformers/video/world models</strong></li> </ul> <p>In other words: the future is not just “better one-step image generation,” it’s <strong>learned transition operators</strong> for high-dimensional structured dynamics (video, 3D, simulators, world models), with both:</p> <ul> <li>strong numerical behavior (few NFEs)</li> <li>and actual distributional control (Wasserstein/KL-style guarantees)</li> </ul> <p>That’s exactly where flow maps stop being a distillation trick and become a real modeling primitive.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/graphics-diffusion/">Recent Methods Utilizing Diffusion in Graphics</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2026/spatial-video/">Spatial Memory in Video Diffusion Survey</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adam Abecid. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>